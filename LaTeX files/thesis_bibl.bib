
@inproceedings{abbasiFairClusteringEquitable2021,
  title = {Fair {{Clustering}} via {{Equitable Group Representations}}},
  booktitle = {Proceedings of the 2021 {{ACM Conference}} on {{Fairness}}, {{Accountability}}, and {{Transparency}}},
  author = {Abbasi, Mohsen and Bhaskara, Aditya and Venkatasubramanian, Suresh},
  date = {2021-03-03},
  pages = {504--514},
  publisher = {{ACM}},
  location = {{Virtual Event Canada}},
  doi = {10.1145/3442188.3445913},
  url = {https://dl.acm.org/doi/10.1145/3442188.3445913},
  urldate = {2021-07-29},
  abstract = {What does it mean for a clustering to be fair? One popular approach seeks to ensure that each cluster contains groups in (roughly) the same proportion in which they exist in the population. The normative principle at play is balance: any cluster might act as a representative of the data, and thus should reflect its diversity.},
  eventtitle = {{{FAccT}} '21: 2021 {{ACM Conference}} on {{Fairness}}, {{Accountability}}, and {{Transparency}}},
  isbn = {978-1-4503-8309-7},
  langid = {english},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\4VS4B5UQ\\Abbasi et al. - 2021 - Fair Clustering via Equitable Group Representation.pdf}
}

@inproceedings{abdollahpouriConnectionPopularityBias2020,
  title = {The {{Connection Between Popularity Bias}}, {{Calibration}}, and {{Fairness}} in {{Recommendation}}},
  booktitle = {Fourteenth {{ACM Conference}} on {{Recommender Systems}}},
  author = {Abdollahpouri, Himan and Mansoury, Masoud and Burke, Robin and Mobasher, Bamshad},
  date = {2020-09-22},
  pages = {726--731},
  publisher = {{ACM}},
  location = {{Virtual Event Brazil}},
  doi = {10.1145/3383313.3418487},
  url = {https://dl.acm.org/doi/10.1145/3383313.3418487},
  urldate = {2021-07-29},
  abstract = {Recently there has been a growing interest in fairness-aware recommender systems including fairness in providing consistent performance across different users or groups of users. A recommender system could be considered unfair if the recommendations do not fairly represent the tastes of a certain group of users while other groups receive recommendations that are consistent with their preferences. In this paper, we use a metric called miscalibration for measuring how a recommendation algorithm is responsive to users’ true preferences and we consider how various algorithms may result in different degrees of miscalibration for different users. In particular, we conjecture that popularity bias which is a wellknown phenomenon in recommendation is one important factor leading to miscalibration in recommendation. Our experimental results using two real-world datasets show that there is a connection between how different user groups are affected by algorithmic popularity bias and their level of interest in popular items. Moreover, we show that the more a group is affected by the algorithmic popularity bias, the more their recommendations are miscalibrated.},
  eventtitle = {{{RecSys}} '20: {{Fourteenth ACM Conference}} on {{Recommender Systems}}},
  isbn = {978-1-4503-7583-2},
  langid = {english},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\ES4LNWJZ\\Abdollahpouri et al. - 2020 - The Connection Between Popularity Bias, Calibratio.pdf}
}

@inproceedings{abdollahpouriControllingPopularityBias2017,
  title = {Controlling {{Popularity Bias}} in {{Learning-to-Rank Recommendation}}},
  booktitle = {Proceedings of the {{Eleventh ACM Conference}} on {{Recommender Systems}}},
  author = {Abdollahpouri, Himan and Burke, Robin and Mobasher, Bamshad},
  date = {2017-08-27},
  pages = {42--46},
  publisher = {{ACM}},
  location = {{Como Italy}},
  doi = {10.1145/3109859.3109912},
  url = {https://dl.acm.org/doi/10.1145/3109859.3109912},
  urldate = {2021-07-29},
  abstract = {Many recommendation algorithms su er from popularity bias in their output: popular items are recommended frequently and less popular ones rarely, if at all. However, less popular, long-tail items are precisely those that are o en desirable recommendations. In this paper, we introduce a exible regularization-based framework to enhance the long-tail coverage of recommendation lists in a learning-to-rank algorithm. We show that regularization provides a tunable mechanism for controlling the trade-o between accuracy and coverage. Moreover, the experimental results using two data sets show that it is possible to improve coverage of long tail items without substantial loss of ranking performance.},
  eventtitle = {{{RecSys}} '17: {{Eleventh ACM Conference}} on {{Recommender Systems}}},
  isbn = {978-1-4503-4652-8},
  langid = {english},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\2GWFA8PZ\\Abdollahpouri et al. - 2017 - Controlling Popularity Bias in Learning-to-Rank Re.pdf}
}

@online{abdollahpouriImpactPopularityBias2019,
  title = {The {{Impact}} of {{Popularity Bias}} on {{Fairness}} and {{Calibration}} in {{Recommendation}}},
  author = {Abdollahpouri, Himan and Mansoury, Masoud and Burke, Robin and Mobasher, Bamshad},
  date = {2019-10-15},
  eprint = {1910.05755},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1910.05755},
  urldate = {2021-07-29},
  abstract = {Recently there has been a growing interest in fairness-aware recommender systems, including fairness in providing consistent performance across di erent users or groups of users. A recommender system could be considered unfair if the recommendations do not fairly represent the tastes of a certain group of users while other groups receive recommendations that are consistent with their preferences. In this paper, we use a metric called miscalibration for measuring how a recommendation algorithm is responsive to users’ true preferences and we consider how various algorithms may result in di erent degrees of miscalibration. A well-known type of bias in recommendation is popularity bias where few popular items are over-represented in recommendations, while the majority of other items do not get signi cant exposure. We conjecture that popularity bias is one important factor leading to miscalibration in recommendation. Our experimental results using two real-world datasets show that there is a strong correlation between how different user groups are a ected by algorithmic popularity bias and their level of interest in popular items. Moreover, we show that the more a group is a ected by the algorithmic popularity bias, the more their recommendations are miscalibrated. Finally, we show that the algorithms with greater popularity bias ampli cation tend to have greater overall miscalibration.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Information Retrieval},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\M8DYDK3M\\Abdollahpouri et al. - 2019 - The Impact of Popularity Bias on Fairness and Cali.pdf}
}

@online{abdollahpouriManagingPopularityBias2019,
  title = {Managing {{Popularity Bias}} in {{Recommender Systems}} with {{Personalized Re-ranking}}},
  author = {Abdollahpouri, Himan and Burke, Robin and Mobasher, Bamshad},
  date = {2019-08-12},
  eprint = {1901.07555},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1901.07555},
  urldate = {2021-07-29},
  abstract = {Many recommender systems suffer from popularity bias: popular items are recommended frequently while less popular, niche products, are recommended rarely or not at all. However, recommending the ignored products in the `long tail' is critical for businesses as they are less likely to be discovered. In this paper, we introduce a personalized diversification re-ranking approach to increase the representation of less popular items in recommendations while maintaining acceptable recommendation accuracy. Our approach is a post-processing step that can be applied to the output of any recommender system. We show that our approach is capable of managing popularity bias more effectively, compared with an existing method based on regularization. We also examine both new and existing metrics to measure the coverage of long-tail items in the recommendation.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Information Retrieval},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\YV8Y4L25\\Abdollahpouri et al. - 2019 - Managing Popularity Bias in Recommender Systems wi.pdf}
}

@inproceedings{abdollahpouriManagingPopularityBias2019a,
  title = {Managing {{Popularity Bias}} in {{Recommender Systems}} with {{Personalized Re-Ranking}}},
  booktitle = {The {{Thirty-Second International Flairs Conference}}},
  author = {Abdollahpouri, Himan and Burke, Robin and Mobasher, Bamshad},
  date = {2019-05-19},
  url = {https://www.aaai.org/ocs/index.php/FLAIRS/FLAIRS19/paper/view/18199},
  urldate = {2022-02-22},
  abstract = {Many recommender systems suffer from popularity bias: popular items are recommended frequently while less popular, niche products, are recommended rarely or not at all. However, recommending the ignored products in the ``long tail'' is critical for businesses as they are less likely to be discovered. In this paper, we introduce a personalized diversification re-ranking approach to increase the representation of less popular items in recommendations while maintaining acceptable recommendation accuracy. Our approach is a post-processing step that can be applied to the output of any recommender system. We show that our approach is capable of managing popularity bias more effectively, compared with an existing method based on regularization. We also examine both new and existing metrics to measure the coverage of long-tail items in the recommendation.},
  eventtitle = {The {{Thirty-Second International Flairs Conference}}},
  langid = {english},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\M6QUDVXG\\Abdollahpouri et al. - 2019 - Managing Popularity Bias in Recommender Systems wi.pdf;C\:\\Users\\Romanos\\Zotero\\storage\\QKVH28FX\\18199.html}
}

@online{abdollahpouriMultisidedExposureBias2020,
  title = {Multi-Sided {{Exposure Bias}} in {{Recommendation}}},
  author = {Abdollahpouri, Himan and Mansoury, Masoud},
  date = {2020-07-01},
  eprint = {2006.15772},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2006.15772},
  urldate = {2021-07-28},
  abstract = {Academic research in recommender systems has been greatly focusing on the accuracy-related measures of recommendations. Even when non-accuracy measures such as popularity bias, diversity, and novelty are studied, it is often solely from the users’ perspective. However, many real-world recommenders are often multistakeholder environments in which the needs and interests of several stakeholders should be addressed in the recommendation process. In this paper, we focus on the popularity bias problem which is a well-known property of many recommendation algorithms where few popular items are over-recommended while the majority of other items do not get proportional attention and address its impact on different stakeholders. Using several recommendation algorithms and two publicly available datasets in music and movie domains, we empirically show the inherent popularity bias of the algorithms and how this bias impacts different stakeholders such as users and suppliers of the items. We also propose metrics to measure the exposure bias of recommendation algorithms from the perspective of different stakeholders.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Information Retrieval},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\9R6UXQZS\\Abdollahpouri and Mansoury - 2020 - Multi-sided Exposure Bias in Recommendation.pdf}
}

@article{abdollahpouriMultistakeholderRecommendationSurvey2020,
  title = {Multistakeholder Recommendation: {{Survey}} and Research Directions},
  shorttitle = {Multistakeholder Recommendation},
  author = {Abdollahpouri, Himan and Adomavicius, Gediminas and Burke, Robin and Guy, Ido and Jannach, Dietmar and Kamishima, Toshihiro and Krasnodebski, Jan and Pizzato, Luiz},
  date = {2020-03},
  journaltitle = {User Model User-Adap Inter},
  volume = {30},
  number = {1},
  pages = {127--158},
  issn = {0924-1868, 1573-1391},
  doi = {10.1007/s11257-019-09256-1},
  url = {http://link.springer.com/10.1007/s11257-019-09256-1},
  urldate = {2021-07-28},
  langid = {english},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\GPC8D9QY\\Abdollahpouri et al. - 2020 - Multistakeholder recommendation Survey and resear.pdf}
}

@online{abdollahpouriPopularityBiasRecommendation2020,
  title = {Popularity {{Bias}} in {{Recommendation}}: {{A Multi-stakeholder Perspective}}},
  shorttitle = {Popularity {{Bias}} in {{Recommendation}}},
  author = {Abdollahpouri, Himan},
  date = {2020-08-19},
  eprint = {2008.08551},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2008.08551},
  urldate = {2021-07-28},
  abstract = {Traditionally, especially in academic research in recommender systems, the focus has been solely on the satisfaction of the end-user. While user satisfaction has, indeed, been associated with the success of the business, it is not the only factor. In many recommendation domains, there are other stakeholders whose needs should be taken into account in the recommendation generation and evaluation. In this dissertation, I describe the notion of multi-stakeholder recommendation. In particular, I study one of the most important challenges in recommendation research, popularity bias, from a multi-stakeholder perspective since, as I show later in this dissertation, it impacts different stakeholders in a recommender system. Popularity bias is a well-known phenomenon in recommender systems where popular items are recommended even more frequently than their popularity would warrant, amplifying long-tail effects already present in many recommendation domains. Prior research has examined various approaches for mitigating popularity bias and enhancing the recommendation of long-tail items overall. The effectiveness of these approaches, however, has not been assessed in multi-stakeholder environments. In this dissertation, I study the impact of popularity bias in recommender systems from a multi-stakeholder perspective. In addition, I propose several algorithms each approaching the popularity bias mitigation from a different angle and compare their performances using several metrics with some other state-of-the-art approaches in the literature. I show that, often, the standard evaluation measures of popularity bias mitigation in the literature do not reflect the real picture of an algorithm's performance when it is evaluated from a multi-stakeholder point of view.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Information Retrieval},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\E6RCWSMM\\Abdollahpouri - 2020 - Popularity Bias in Recommendation A Multi-stakeho.pdf}
}

@online{abdollahpouriUnfairnessPopularityBias2019,
  title = {The {{Unfairness}} of {{Popularity Bias}} in {{Recommendation}}},
  author = {Abdollahpouri, Himan and Mansoury, Masoud and Burke, Robin and Mobasher, Bamshad},
  date = {2019-09-19},
  eprint = {1907.13286},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1907.13286},
  urldate = {2021-07-29},
  abstract = {Recommender systems are known to suffer from the popularity bias problem: popular (i.e. frequently rated) items get a lot of exposure while less popular ones are under-represented in the recommendations. Research in this area has been mainly focusing on finding ways to tackle this issue by increasing the number of recommended long-tail items or otherwise the overall catalog coverage. In this paper, however, we look at this problem from the users’ perspective: we want to see how popularity bias causes the recommendations to deviate from what the user expects to get from the recommender system. We define three different groups of users according to their interest in popular items (Niche, Diverse and Blockbuster-focused) and show the impact of popularity bias on the users in each group. Our experimental results on a movie dataset show that in many recommendation algorithms the recommendations the users get are extremely concentrated on popular items even if a user is interested in long-tail and non-popular items showing an extreme bias disparity.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Information Retrieval},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\J4XY7VQ8\\Abdollahpouri et al. - 2019 - The Unfairness of Popularity Bias in Recommendatio.pdf}
}

@article{abdollahpouriUsercenteredEvaluationPopularity2021,
  title = {User-Centered {{Evaluation}} of {{Popularity Bias}} in {{Recommender Systems}}},
  author = {Abdollahpouri, Himan and Mansoury, Masoud and Burke, Robin and Mobasher, Bamshad and Malthouse, Edward},
  date = {2021-06-21},
  journaltitle = {Proceedings of the 29th ACM Conference on User Modeling, Adaptation and Personalization},
  eprint = {2103.06364},
  eprinttype = {arxiv},
  pages = {119--129},
  doi = {10.1145/3450613.3456821},
  url = {http://arxiv.org/abs/2103.06364},
  urldate = {2021-07-28},
  abstract = {Recommendation and ranking systems are known to suffer from popularity bias; the tendency of the algorithm to favor a few popular items while under-representing the majority of other items. Prior research has examined various approaches for mitigating popularity bias and enhancing the recommendation of long-tail, less popular, items. The effectiveness of these approaches is often assessed using different metrics to evaluate the extent to which over-concentration on popular items is reduced. However, not much attention has been given to the user-centered evaluation of this bias; how different users with different levels of interest towards popular items are affected by such algorithms. In this paper, we show the limitations of the existing metrics to evaluate popularity bias mitigation when we want to assess these algorithms from the users’ perspective and we propose a new metric that can address these limitations. In addition, we present an effective approach that mitigates popularity bias from the user-centered point of view. Finally, we investigate several state-of-the-art approaches proposed in recent years to mitigate popularity bias and evaluate their performances using the existing metrics and also from the users’ perspective. Our experimental results using two publicly-available datasets show that existing popularity bias mitigation techniques ignore the users’ tolerance towards popular items. Our proposed user-centered method can tackle popularity bias effectively for different users while also improving the existing metrics.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Information Retrieval},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\UZCLE686\\Abdollahpouri et al. - 2021 - User-centered Evaluation of Popularity Bias in Rec.pdf}
}

@article{adamsAlevelResultsAlmost2020,
  title = {A-Level Results: Almost 40\% of Teacher Assessments in {{England}} Downgraded},
  shorttitle = {A-Level Results},
  author = {Adams, Richard and Weale, Sally and Barr, Caelainn},
  date = {2020-08-13T10:39:48},
  journaltitle = {The Guardian},
  issn = {0261-3077},
  url = {https://www.theguardian.com/education/2020/aug/13/almost-40-of-english-students-have-a-level-results-downgraded},
  urldate = {2022-01-30},
  abstract = {Ofqual figures show 39.1\% of 700,000 teacher assessments were lowered by at least one grade},
  entrysubtype = {newspaper},
  journalsubtitle = {Education},
  langid = {british},
  keywords = {A-levels,Education,Exams,Ofqual,Schools,Sixth form,UK news},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\R33PJUE6\\almost-40-of-english-students-have-a-level-results-downgraded.html}
}

@article{adomaviciusImpactDataCharacteristics2012,
  title = {Impact of Data Characteristics on Recommender Systems Performance},
  author = {Adomavicius, Gediminas and Zhang, Jingjing},
  date = {2012-04},
  journaltitle = {ACM Trans. Manage. Inf. Syst.},
  volume = {3},
  number = {1},
  pages = {1--17},
  issn = {2158-656X, 2158-6578},
  doi = {10.1145/2151163.2151166},
  url = {https://dl.acm.org/doi/10.1145/2151163.2151166},
  urldate = {2022-02-05},
  abstract = {This article investigates the impact of rating data characteristics on the performance of several popular recommendation algorithms, including user-based and item-based collaborative filtering, as well as matrix factorization. We focus on three groups of data characteristics: rating space, rating frequency distribution, and rating value distribution. A sampling procedure was employed to obtain different rating data subsamples with varying characteristics; recommendation algorithms were used to estimate the predictive accuracy for each sample; and linear regression-based models were used to uncover the relationships between data characteristics and recommendation accuracy. Experimental results on multiple rating datasets show the consistent and significant effects of several data characteristics on recommendation accuracy.},
  langid = {english},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\22ZZKQPA\\Adomavicius and Zhang - 2012 - Impact of data characteristics on recommender syst.pdf}
}

@article{adomaviciusImprovingAggregateRecommendation2012,
  title = {Improving {{Aggregate Recommendation Diversity Using Ranking-Based Techniques}}},
  author = {Adomavicius, G. and {YoungOk Kwon}},
  date = {2012-05},
  journaltitle = {IEEE Trans. Knowl. Data Eng.},
  volume = {24},
  number = {5},
  pages = {896--911},
  issn = {1041-4347},
  doi = {10.1109/TKDE.2011.15},
  url = {http://ieeexplore.ieee.org/document/5680904/},
  urldate = {2022-01-12},
  abstract = {Recommender systems are becoming increasingly important to individual users and businesses for providing personalized recommendations. However, while the majority of algorithms proposed in recommender systems literature have focused on improving recommendation accuracy (as exemplified by the recent Netflix Prize competition), other important aspects of recommendation quality, such as the diversity of recommendations, have often been overlooked. In this paper, we introduce and explore a number of item ranking techniques that can generate recommendations that have substantially higher aggregate diversity across all users while maintaining comparable levels of recommendation accuracy. Comprehensive empirical evaluation consistently shows the diversity gains of the proposed techniques using several real-world rating datasets and different rating prediction algorithms.},
  langid = {english},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\UKW22XUX\\Adomavicius and YoungOk Kwon - 2012 - Improving Aggregate Recommendation Diversity Using.pdf}
}

@article{adomaviciusReducingRecommenderSystems,
  title = {Reducing {{Recommender Systems Biases}}: {{An Investigation}} of {{Rating Display Designs}}},
  author = {Adomavicius, Gediminas and Bockstedt, Jesse C and Curley, Shawn P and Zhang, Jingjing},
  pages = {55},
  abstract = {Prior research has shown that online recommendations have a significant influence on consumers’ preference ratings and economic behavior. Specifically, biases induced by observing personalized system recommendations can lead to distortions in users’ self-reported preference ratings after consumption of an item, thus contaminating the users’ subsequent inputs to the recommender system. This, in turn, provides the system with an inaccurate view of user preferences and opens up possibilities of rating manipulation. As recommender systems continue to become increasingly popular in today’s online environments, preventing or reducing such system-induced biases constitutes a highly important and practical research problem. In this paper, we address this problem via the analysis of different rating display designs for the purpose of proactively preventing biases before they occur, i.e., at rating collection time. We use randomized laboratory experimentation to test how the presentation format of personalized recommendations affects the biases generated in post-consumption preference ratings. We demonstrate that graphical rating display designs of recommender systems are more advantageous than numerical designs in reducing the biases, although none are able to remove biases completely. We also show that scale compatibility is a contributing mechanism operating to create these biases, although not the only one. Together, the results have practical implications for the design and implementation of recommender systems as well as theoretical implications for the study of recommendation biases.},
  langid = {english},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\ACE673IG\\Adomavicius et al. - Reducing Recommender Systems Biases An Investigat.pdf}
}

@book{aggarwalRecommenderSystems2016,
  title = {Recommender {{Systems}}},
  author = {Aggarwal, Charu C.},
  date = {2016},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-29659-3},
  url = {http://link.springer.com/10.1007/978-3-319-29659-3},
  urldate = {2021-07-28},
  isbn = {978-3-319-29657-9 978-3-319-29659-3},
  langid = {english},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\5QN9FG5M\\Aggarwal - 2016 - Recommender Systems.pdf}
}

@inproceedings{aiolliEfficientTopnRecommendation2013,
  title = {Efficient Top-n Recommendation for Very Large Scale Binary Rated Datasets},
  booktitle = {Proceedings of the 7th {{ACM}} Conference on {{Recommender}} Systems},
  author = {Aiolli, Fabio},
  date = {2013-10-12},
  pages = {273--280},
  publisher = {{ACM}},
  location = {{Hong Kong China}},
  doi = {10.1145/2507157.2507189},
  url = {https://dl.acm.org/doi/10.1145/2507157.2507189},
  urldate = {2021-11-24},
  abstract = {We present a simple and scalable algorithm for top-N recommendation able to deal with very large datasets and (binary rated) implicit feedback. We focus on memory-based collaborative filtering algorithms similar to the well known neighboor based technique for explicit feedback. The major difference, that makes the algorithm particularly scalable, is that it uses positive feedback only and no explicit computation of the complete (user-by-user or itemby-item) similarity matrix needs to be performed.},
  eventtitle = {{{RecSys}} '13: {{Seventh ACM Conference}} on {{Recommender Systems}}},
  isbn = {978-1-4503-2409-0},
  langid = {english},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\Z7EUDX83\\Aiolli - 2013 - Efficient top-n recommendation for very large scal.pdf}
}

@book{andersonLongTailWhy2006,
  title = {The {{Long Tail}}: {{Why}} the {{Future}} of {{Business Is Selling Less}} of {{More}}},
  shorttitle = {The {{Long Tail}}},
  author = {Anderson, Chris},
  date = {2006},
  publisher = {{Hyperion}},
  isbn = {978-1-4013-0237-5}
}

@inproceedings{anelliElliotComprehensiveRigorous2021,
  title = {Elliot: {{A Comprehensive}} and {{Rigorous Framework}} for {{Reproducible Recommender Systems Evaluation}}},
  shorttitle = {Elliot},
  booktitle = {Proceedings of the 44th {{International ACM SIGIR Conference}} on {{Research}} and {{Development}} in {{Information Retrieval}}},
  author = {Anelli, Vito Walter and Bellogin, Alejandro and Ferrara, Antonio and Malitesta, Daniele and Merra, Felice Antonio and Pomo, Claudio and Donini, Francesco Maria and Di Noia, Tommaso},
  date = {2021-07-11},
  pages = {2405--2414},
  publisher = {{ACM}},
  location = {{Virtual Event Canada}},
  doi = {10.1145/3404835.3463245},
  url = {https://dl.acm.org/doi/10.1145/3404835.3463245},
  urldate = {2021-07-29},
  eventtitle = {{{SIGIR}} '21: {{The}} 44th {{International ACM SIGIR Conference}} on {{Research}} and {{Development}} in {{Information Retrieval}}},
  isbn = {978-1-4503-8037-9},
  langid = {english},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\NXYAPXDL\\Anelli et al. - 2021 - Elliot A Comprehensive and Rigorous Framework for.pdf}
}

@inproceedings{antikaciogluPostProcessingRecommender2017,
  title = {Post {{Processing Recommender Systems}} for {{Diversity}}},
  booktitle = {Proceedings of the 23rd {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Antikacioglu, Arda and Ravi, R.},
  date = {2017-08-13},
  pages = {707--716},
  publisher = {{ACM}},
  location = {{Halifax NS Canada}},
  doi = {10.1145/3097983.3098173},
  url = {https://dl.acm.org/doi/10.1145/3097983.3098173},
  urldate = {2021-07-28},
  abstract = {Collaborative filtering is a broad and powerful framework for building recommendation systems that has seen widespread adoption. Over the past decade, the propensity of such systems for favoring popular products and thus creating echo chambers have been observed. This has given rise to an active area of research that seeks to diversify recommendations generated by such algorithms.[2, 11, 37]. We address the problem of increasing diversity in recommendation systems that are based on collaborative filtering that use past ratings to predict a rating quality for potential recommendations. Following our earlier work, [7], we formulate recommendation system design as a subgraph selection problem from a candidate super-graph of potential recommendations where both diversity and rating quality are explicitly optimized: (1) On the modeling side, we define a new flexible notion of diversity that allows a system designer to prescribe the number of recommendations each item should receive, and smoothly penalizes deviations from this distribution. (2) On the algorithmic side, we show that minimum-cost network flow methods yield fast algorithms in theory and practice for designing recommendation subgraphs that optimize this notion of diversity. (3) On the empirical side, we show the effectiveness of our new model and method to increase diversity while maintaining high rating quality in standard rating data sets from Netflix and MovieLens.},
  eventtitle = {{{KDD}} '17: {{The}} 23rd {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  isbn = {978-1-4503-4887-4},
  langid = {english},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\WAFZZGS7\\Antikacioglu and Ravi - 2017 - Post Processing Recommender Systems for Diversity.pdf}
}

@inproceedings{antikaciogluPostProcessingRecommender2017a,
  title = {Post {{Processing Recommender Systems}} for {{Diversity}}},
  booktitle = {Proceedings of the 23rd {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Antikacioglu, Arda and Ravi, R.},
  date = {2017-08-13},
  pages = {707--716},
  publisher = {{ACM}},
  location = {{Halifax NS Canada}},
  doi = {10.1145/3097983.3098173},
  url = {https://dl.acm.org/doi/10.1145/3097983.3098173},
  urldate = {2021-07-28},
  abstract = {Collaborative filtering is a broad and powerful framework for building recommendation systems that has seen widespread adoption. Over the past decade, the propensity of such systems for favoring popular products and thus creating echo chambers have been observed. This has given rise to an active area of research that seeks to diversify recommendations generated by such algorithms.[2, 11, 37]. We address the problem of increasing diversity in recommendation systems that are based on collaborative filtering that use past ratings to predict a rating quality for potential recommendations. Following our earlier work, [7], we formulate recommendation system design as a subgraph selection problem from a candidate super-graph of potential recommendations where both diversity and rating quality are explicitly optimized: (1) On the modeling side, we define a new flexible notion of diversity that allows a system designer to prescribe the number of recommendations each item should receive, and smoothly penalizes deviations from this distribution. (2) On the algorithmic side, we show that minimum-cost network flow methods yield fast algorithms in theory and practice for designing recommendation subgraphs that optimize this notion of diversity. (3) On the empirical side, we show the effectiveness of our new model and method to increase diversity while maintaining high rating quality in standard rating data sets from Netflix and MovieLens.},
  eventtitle = {{{KDD}} '17: {{The}} 23rd {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  isbn = {978-1-4503-4887-4},
  langid = {english},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\BFHJFWX9\\Antikacioglu and Ravi - 2017 - Post Processing Recommender Systems for Diversity.pdf}
}

@book{beauregardFirstCourseLinear1973,
  title = {A First Course in Linear Algebra; with Optional Introduction to Groups, Rings, and Fields},
  author = {Beauregard, Raymond A.},
  date = {1973},
  publisher = {{Boston, Houghton Mifflin}},
  url = {http://archive.org/details/firstcourseinlin0000beau},
  urldate = {2022-01-22},
  abstract = {xiii, 410 pages 25 cm},
  editora = {{Internet Archive}},
  editoratype = {collaborator},
  isbn = {978-0-395-14017-8},
  langid = {english},
  pagetotal = {442},
  keywords = {Algebras; Linear}
}

@online{bellamyAIFairness3602018,
  title = {{{AI Fairness}} 360: {{An Extensible Toolkit}} for {{Detecting}}, {{Understanding}}, and {{Mitigating Unwanted Algorithmic Bias}}},
  shorttitle = {{{AI Fairness}} 360},
  author = {Bellamy, Rachel K. E. and Dey, Kuntal and Hind, Michael and Hoffman, Samuel C. and Houde, Stephanie and Kannan, Kalapriya and Lohia, Pranay and Martino, Jacquelyn and Mehta, Sameep and Mojsilovic, Aleksandra and Nagar, Seema and Ramamurthy, Karthikeyan Natesan and Richards, John and Saha, Diptikalyan and Sattigeri, Prasanna and Singh, Moninder and Varshney, Kush R. and Zhang, Yunfeng},
  date = {2018-10-03},
  eprint = {1810.01943},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1810.01943},
  urldate = {2021-08-11},
  abstract = {Fairness is an increasingly important concern as machine learning models are used to support decision making in high-stakes applications such as mortgage lending, hiring, and prison sentencing. This paper introduces a new open source Python toolkit for algorithmic fairness, AI Fairness 360 (AIF360), released under an Apache v2.0 license (https://github.com/ibm/aif360). The main objectives of this toolkit are to help facilitate the transition of fairness research algorithms to use in an industrial setting and to provide a common framework for fairness researchers to share and evaluate algorithms.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\9H4GZU2T\\Bellamy et al. - 2018 - AI Fairness 360 An Extensible Toolkit for Detecti.pdf}
}

@book{bellman1957dynamic,
  title = {Dynamic Programming},
  author = {Bellman, R. and Bellman, R.E. and Corporation, Rand},
  date = {1957},
  series = {Rand Corporation Research Study},
  publisher = {{Princeton University Press}},
  url = {https://books.google.gr/books?id=rZW4ugAACAAJ},
  lccn = {lc57005444},
  pagetotal = {339}
}

@book{bellmanDynamicProgramming1984,
  title = {Dynamic Programming},
  author = {Bellman, Richard},
  date = {1984},
  publisher = {{Princeton Univ. Pr}},
  location = {{Princeton, NJ}},
  isbn = {978-0-691-07951-6},
  langid = {english},
  pagetotal = {339},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\FXL9Y7D2\\Bellman - 1984 - Dynamic programming.pdf}
}

@report{bird2020fairlearn,
  title = {Fairlearn: {{A}} Toolkit for Assessing and Improving Fairness in {{AI}}},
  author = {Bird, Sarah and Dudík, Miro and Edgar, Richard and Horn, Brandon and Lutz, Roman and Milan, Vanessa and Sameki, Mehrnoosh and Wallach, Hanna and Walker, Kathleen},
  date = {2020-05},
  number = {MSR-TR-2020-32},
  institution = {{Microsoft}},
  url = {https://www.microsoft.com/en-us/research/publication/fairlearn-a-toolkit-for-assessing-and-improving-fairness-in-ai/},
  abstract = {We introduce Fairlearn, an open source toolkit that empowers data scientists and developers to assess and improve the fairness of their AI systems. Fairlearn has two components: an interactive visualization dashboard and unfairness mitigation algorithms. These components are designed to help with navigating trade-offs between fairness and model performance. We emphasize that prioritizing fairness in AI systems is a sociotechnical challenge. Because there are many complex sources of unfairness—some societal and some technical—it is not possible to fully “debias” a system or to guarantee fairness; the goal is to mitigate fairness-related harms as much as possible. As Fairlearn grows to include additional fairness metrics, unfairness mitigation algorithms, and visualization capabilities, we hope that it will be shaped by a diverse community of stakeholders, ranging from data scientists, developers, and business decision makers to the people whose lives may be affected by the predictions of AI systems.}
}

@article{birdFairlearnToolkitAssessing2020,
  title = {Fairlearn: {{A}} Toolkit for Assessing and Improving Fairness in {{AI}}},
  shorttitle = {Fairlearn},
  author = {Bird, Sarah and Dudík, Miro and Edgar, Richard and Horn, Brandon and Lutz, Roman and Milan, Vanessa and Sameki, Mehrnoosh and Wallach, Hanna and Walker, Kathleen},
  date = {2020-05-18},
  url = {https://www.microsoft.com/en-us/research/publication/fairlearn-a-toolkit-for-assessing-and-improving-fairness-in-ai/},
  urldate = {2021-08-16},
  abstract = {We introduce Fairlearn, an open source toolkit that empowers data scientists and developers to assess and improve the fairness of their AI systems. Fairlearn has two components: an interactive visualization dashboard and unfairness mitigation algorithms. These components are designed to help with navigating trade-offs between fairness and model performance. We emphasize that prioritizing fairness in […]},
  langid = {american},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\FU9Q5J7H\\Bird et al. - 2020 - Fairlearn A toolkit for assessing and improving f.pdf;C\:\\Users\\Romanos\\Zotero\\storage\\85LKCZFU\\fairlearn-a-toolkit-for-assessing-and-improving-fairness-in-ai.html}
}

@article{bobadillaDeepFairDeepLearning2021,
  title = {{{DeepFair}}: {{Deep Learning}} for {{Improving Fairness}} in {{Recommender Systems}}},
  shorttitle = {{{DeepFair}}},
  author = {Bobadilla, Jesús and Lara-Cabrera, Raúl and González-Prieto, Ángel and Ortega, Fernando},
  date = {2021},
  journaltitle = {IJIMAI},
  volume = {6},
  number = {6},
  eprint = {2006.05255},
  eprinttype = {arxiv},
  pages = {86},
  issn = {1989-1660},
  doi = {10.9781/ijimai.2020.11.001},
  url = {http://arxiv.org/abs/2006.05255},
  urldate = {2021-07-28},
  abstract = {The lack of bias management in Recommender Systems leads to minority groups receiving unfair recommendations. Moreover, the trade-off between equity and precision makes it difficult to obtain recommendations that meet both criteria. Here we propose a Deep Learning (DL) based Collaborative Filtering (CF) algorithm that provides recommendations with an optimum balance between fairness and accuracy without knowing demographic information about the users. Experimental results show that it is possible to make fair recommendations without losing a significant proportion of accuracy.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,I.5.1,Statistics - Machine Learning},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\E3C4B3PU\\Bobadilla et al. - 2021 - DeepFair Deep Learning for Improving Fairness in .pdf}
}

@article{borattoConnectingUserItem2021,
  title = {Connecting User and Item Perspectives in Popularity Debiasing for Collaborative Recommendation},
  author = {Boratto, Ludovico and Fenu, Gianni and Marras, Mirko},
  date = {2021-01},
  journaltitle = {Information Processing \& Management},
  volume = {58},
  number = {1},
  pages = {102387},
  issn = {03064573},
  doi = {10.1016/j.ipm.2020.102387},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0306457320308827},
  urldate = {2021-07-29},
  abstract = {Recommender systems learn from historical users’ feedback that is often non-uniformly dis­ tributed across items. As a consequence, these systems may end up suggesting popular items more than niche items progressively, even when the latter would be of interest for users. This can hamper several core qualities of the recommended lists (e.g., novelty, coverage, diversity), im­ pacting on the future success of the underlying platform itself. In this paper, we formalize two novel metrics that quantify how much a recommender system equally treats items along the popularity tail. The first one encourages equal probability of being recommended across items, while the second one encourages true positive rates for items to be equal. We characterize the recommendations of representative algorithms by means of the proposed metrics, and we show that the item probability of being recommended and the item true positive rate are biased against the item popularity. To promote a more equal treatment of items along the popularity tail, we propose an in-processing approach aimed at minimizing the biased correlation between user-item relevance and item popularity. Extensive experiments show that, with small losses in accuracy, our popularity-mitigation approach leads to important gains in beyond-accuracy recommenda­ tion quality.},
  langid = {english},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\97894Y4L\\Boratto et al. - 2021 - Connecting user and item perspectives in popularit.pdf}
}

@incollection{borattoEffectAlgorithmicBias2019,
  title = {The {{Effect}} of {{Algorithmic Bias}} on {{Recommender Systems}} for {{Massive Open Online Courses}}},
  booktitle = {Advances in {{Information Retrieval}}},
  author = {Boratto, Ludovico and Fenu, Gianni and Marras, Mirko},
  editor = {Azzopardi, Leif and Stein, Benno and Fuhr, Norbert and Mayr, Philipp and Hauff, Claudia and Hiemstra, Djoerd},
  date = {2019},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {11437},
  pages = {457--472},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-15712-8_30},
  url = {http://link.springer.com/10.1007/978-3-030-15712-8_30},
  urldate = {2021-07-29},
  abstract = {Most recommender systems are evaluated on how they accurately predict user ratings. However, individuals use them for more than an anticipation of their preferences. The literature demonstrated that some recommendation algorithms achieve good prediction accuracy, but suffer from popularity bias. Other algorithms generate an item category bias due to unbalanced rating distributions across categories. These effects have been widely analyzed in the context of books, movies, music, and tourism, but contrasting conclusions have been reached so far. In this paper, we explore how recommender systems work in the context of massive open online courses, going beyond prediction accuracy. To this end, we compared existing algorithms and their recommended lists against biases related to course popularity, catalog coverage, and course category popularity. Our study remarks even more the need of better understanding how recommenders react against bias in diverse contexts.},
  isbn = {978-3-030-15711-1 978-3-030-15712-8},
  langid = {english},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\P4Z5Z6NV\\Boratto et al. - 2019 - The Effect of Algorithmic Bias on Recommender Syst.pdf}
}

@inproceedings{borgesEnhancingLongTerm2019,
  title = {Enhancing {{Long Term Fairness}} in {{Recommendations}} with {{Variational Autoencoders}}},
  booktitle = {Proceedings of the 11th {{International Conference}} on {{Management}} of {{Digital EcoSystems}}},
  author = {Borges, Rodrigo and Stefanidis, Kostas},
  date = {2019-11-12},
  pages = {95--102},
  publisher = {{ACM}},
  location = {{Limassol Cyprus}},
  doi = {10.1145/3297662.3365798},
  url = {https://dl.acm.org/doi/10.1145/3297662.3365798},
  urldate = {2022-01-22},
  abstract = {Recommender systems have become indispensable for several Web sites, helping users deal with big amounts of data. They are capable of analyzing user/item interactions taking place on-line, and provide each user with a list of suggestions sorted by relevance. Items with the same or very close relevance, however, may occupy different positions in the ranking and may be exposed to completely different levels of attention. This promotes unfair treatment and can only be addressed by a long term strategy.},
  eventtitle = {{{MEDES}} '19: 11th {{International Conference}} on {{Management}} of {{Digital EcoSystems}}},
  isbn = {978-1-4503-6238-2},
  langid = {english},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\K3VHLX9M\\Borges and Stefanidis - 2019 - Enhancing Long Term Fairness in Recommendations wi.pdf}
}

@book{brusilovskyAdaptiveWebMethods2007,
  title = {The {{Adaptive Web}}: {{Methods}} and {{Strategies}} of {{Web Personalization}}},
  shorttitle = {The {{Adaptive Web}}},
  editor = {Brusilovsky, Peter and Kobsa, Alfred and Nejdl, Wolfgang},
  date = {2007},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {4321},
  publisher = {{Springer Berlin Heidelberg}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-72079-9},
  url = {http://link.springer.com/10.1007/978-3-540-72079-9},
  urldate = {2022-01-23},
  editorb = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Rangan, C. Pandu and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard},
  editorbtype = {redactor},
  isbn = {978-3-540-72078-2 978-3-540-72079-9},
  langid = {english}
}

@article{brynjolfssonerikNichesRichesAnatomy2006,
  title = {From {{Niches}} to {{Riches}}: {{Anatomy}} of the {{Long Tail}}},
  author = {Brynjolfsson Erik and Yu “Jeffrey” Hu and Michael D. Smith},
  date = {2006-01-07},
  shortjournal = {Organizations and Markets eJournal},
  pages = {6},
  abstract = {Dozens of markets of all types are in the early stages of a revolution as the Internet and related technologies vastly expand the variety of products that can be produced, promoted and purchased. Although this revolution is based on a simple set of economic and technological drivers, the authors argue that its implications are far-reaching for managers, consumers and the economy as a whole. This article looks at what has been dubbed the "Long Tail" phenomenon, examining how customers derive value from an important characteristic of Internet markets: the ability of online merchants to help consumers locate, evaluate and purchase a far wider range of products than they can typically buy via the traditional brick-and-mortar channels. The article examines the Long Tail from both the supply side and the demand side and identifies several key drivers. On the supply side, the authors point out how e-tailers' expanded, centralized warehousing allows for more offerings, thus making it possible for them to cater to more varied tastes. On the demand side, tools such as search engines, recommender software and sampling tools are allowing customers to find products outside of their geographic area. The authors also look toward the future to discuss second order amplified effects of Long Tail, including the growth of markets serving smaller niches},
  langid = {english},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\247MF8N6\\Brynjolfsson - The Internet marketplace allows companies to produ.pdf}
}

@inproceedings{burkeExperimentationFairnessawareRecommendation2020,
  title = {Experimentation with Fairness-Aware Recommendation Using Librec-Auto: Hands-on Tutorial},
  shorttitle = {Experimentation with Fairness-Aware Recommendation Using Librec-Auto},
  booktitle = {Proceedings of the 2020 {{Conference}} on {{Fairness}}, {{Accountability}}, and {{Transparency}}},
  author = {Burke, Robin Douglas and Mansoury, Masoud and Sonboli, Nasim},
  date = {2020-01-27},
  pages = {700--700},
  publisher = {{ACM}},
  location = {{Barcelona Spain}},
  doi = {10.1145/3351095.3375670},
  url = {https://dl.acm.org/doi/10.1145/3351095.3375670},
  urldate = {2021-07-28},
  eventtitle = {{{FAT}}* '20: {{Conference}} on {{Fairness}}, {{Accountability}}, and {{Transparency}}},
  isbn = {978-1-4503-6936-7},
  langid = {english},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\NRKB97W9\\Burke et al. - 2020 - Experimentation with fairness-aware recommendation.pdf}
}

@online{burkeMultisidedFairnessRecommendation2017,
  title = {Multisided {{Fairness}} for {{Recommendation}}},
  author = {Burke, Robin},
  date = {2017-07-08},
  eprint = {1707.00093},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1707.00093},
  urldate = {2021-07-28},
  abstract = {Recent work on machine learning has begun to consider issues of fairness. In this paper, we extend the concept of fairness to recommendation. In particular, we show that in some recommendation contexts, fairness may be a multisided concept, in which fair outcomes for multiple individuals need to be considered. Based on these considerations, we present a taxonomy of classes of fairness-aware recommender systems and suggest possible fairness-aware recommendation architectures.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computers and Society,Computer Science - Information Retrieval},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\3DX9TLBP\\Burke - 2017 - Multisided Fairness for Recommendation.pdf}
}

@book{burkov2019hundred,
  title = {The Hundred-Page Machine Learning Book},
  author = {Burkov, A.},
  date = {2019},
  publisher = {{Andriy Burkov}},
  url = {https://books.google.gr/books?id=0jbxwQEACAAJ},
  isbn = {978-1-9995795-1-7}
}

@book{burkovAllModelsAre,
  title = {“{{All}} Models Are Wrong, but Some Are Useful.” — {{George Box}}},
  author = {Burkov, Andriy},
  langid = {english},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\G2UZMLZK\\Burkov - “All models are wrong, but some are useful.” — Geo.pdf}
}

@book{burkovHundredPageMachineLearning2019,
  title = {The {{Hundred-Page Machine Learning Book}}},
  author = {Burkov, Andriy},
  date = {2019},
  eprint = {0jbxwQEACAAJ},
  eprinttype = {googlebooks},
  publisher = {{Andriy Burkov}},
  abstract = {As its title says, it's the hundred-page machine learning book. It was written by an expert in machine learning holding a Ph.D. in Artificial Intelligence with almost two decades of industry experience in computer science and hands-on machine learning. This is a unique book in many aspects. It is the first successful attempt to write an easy to read book on machine learning that isn't afraid of using math. It's also the first attempt to squeeze a wide range of machine learning topics in a systematic way and without loss in quality. The book contains only those parts of the huge body of material on machine learning developed since the 1960s that have proven to have a significant practical value. A beginner in machine learning will find in this book just enough details to get a comfortable level of understanding of the field and start asking the right questions. Practitioners with experience will use this book as a collection of pointers to the directions of further self-improvement. The book also comes in handy when brainstorming at the beginning of a project, when you try to answer the question whether a given technical or business problem is "machine-learnable" and, if yes, which techniques you should try to solve it. The book comes with a wiki which contains pages that extend some book chapters with additional information: Q\&A, code snippets, further reading, tools, and other relevant resources. Thanks to the continuously updated wiki this book like a good wine keeps getting better after you buy it.},
  isbn = {978-1-9995795-1-7},
  langid = {english},
  pagetotal = {160}
}

@online{calmonOptimizedDataPreProcessing2017,
  title = {Optimized {{Data Pre-Processing}} for {{Discrimination Prevention}}},
  author = {Calmon, Flavio P. and Wei, Dennis and Ramamurthy, Karthikeyan Natesan and Varshney, Kush R.},
  date = {2017-04-11},
  eprint = {1704.03354},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  url = {http://arxiv.org/abs/1704.03354},
  urldate = {2021-07-29},
  abstract = {Non-discrimination is a recognized objective in algorithmic decision making. In this paper, we introduce a novel probabilistic formulation of data pre-processing for reducing discrimination. We propose a convex optimization for learning a data transformation with three goals: controlling discrimination, limiting distortion in individual data samples, and preserving utility. We characterize the impact of limited sample size in accomplishing this objective, and apply two instances of the proposed optimization to datasets, including one on real-world criminal recidivism. The results demonstrate that all three criteria can be simultaneously achieved and also reveal interesting patterns of bias in American society.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computers and Society,Computer Science - Information Theory,Statistics - Machine Learning},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\8AZCPAZM\\Calmon et al. - 2017 - Optimized Data Pre-Processing for Discrimination P.pdf}
}

@inproceedings{canamaresShouldFollowCrowd2018,
  title = {Should {{I Follow}} the {{Crowd}}?: {{A Probabilistic Analysis}} of the {{Effectiveness}} of {{Popularity}} in {{Recommender Systems}}},
  shorttitle = {Should {{I Follow}} the {{Crowd}}?},
  booktitle = {The 41st {{International ACM SIGIR Conference}} on {{Research}} \& {{Development}} in {{Information Retrieval}}},
  author = {Cañamares, Rocío and Castells, Pablo},
  date = {2018-06-27},
  pages = {415--424},
  publisher = {{ACM}},
  location = {{Ann Arbor MI USA}},
  doi = {10.1145/3209978.3210014},
  url = {https://dl.acm.org/doi/10.1145/3209978.3210014},
  urldate = {2021-07-29},
  abstract = {The use of IR methodology in the evaluation of recommender systems has become common practice in recent years. IR metrics have been found however to be strongly biased towards rewarding algorithms that recommend popular items –the same bias that state of the art recommendation algorithms display. Recent research has confirmed and measured such biases, and proposed methods to avoid them. The fundamental question remains open though whether popularity is really a bias we should avoid or not; whether it could be a useful and reliable signal in recommendation, or it may be unfairly rewarded by the experimental biases. We address this question at a formal level by identifying and modeling the conditions that can determine the answer, in terms of dependencies between key random variables, involving item rating, discovery and relevance. We find conditions that guarantee popularity to be effective or quite the opposite, and for the measured metric values to reflect a true effectiveness, or qualitatively deviate from it. We exemplify and confirm the theoretical findings with empirical results. We build a crowdsourced dataset devoid of the usual biases displayed by common publicly available data, in which we illustrate contradictions between the accuracy that would be measured in a common biased offline experimental setting, and the actual accuracy that can be measured with unbiased observations.},
  eventtitle = {{{SIGIR}} '18: {{The}} 41st {{International ACM SIGIR}} Conference on Research and Development in {{Information Retrieval}}},
  isbn = {978-1-4503-5657-2},
  langid = {english},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\ESD8QP3N\\Cañamares and Castells - 2018 - Should I Follow the Crowd A Probabilistic Analys.pdf}
}

@online{catonFairnessMachineLearning2020,
  title = {Fairness in {{Machine Learning}}: {{A Survey}}},
  shorttitle = {Fairness in {{Machine Learning}}},
  author = {Caton, Simon and Haas, Christian},
  date = {2020-10-04},
  eprint = {2010.04053},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/2010.04053},
  urldate = {2021-07-29},
  abstract = {As Machine Learning technologies become increasingly used in contexts that affect citizens, companies as well as researchers need to be confident that their application of these methods will not have unexpected social implications, such as bias towards gender, ethnicity, and/or people with disabilities. There is significant literature on approaches to mitigate bias and promote fairness, yet the area is complex and hard to penetrate for newcomers to the domain. This article seeks to provide an overview of the different schools of thought and approaches to mitigating (social) biases and increase fairness in the Machine Learning literature. It organises approaches into the widely accepted framework of pre-processing, in-processing, and post-processing methods, subcategorizing into a further 11 method areas. Although much of the literature emphasizes binary classification, a discussion of fairness in regression, recommender systems, unsupervised learning, and natural language processing is also provided along with a selection of currently available open source libraries. The article concludes by summarising open challenges articulated as four dilemmas for fairness research.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\4BDH5X2R\\Caton and Haas - 2020 - Fairness in Machine Learning A Survey.pdf}
}

@online{chenBiasDebiasRecommender2020,
  title = {Bias and {{Debias}} in {{Recommender System}}: {{A Survey}} and {{Future Directions}}},
  shorttitle = {Bias and {{Debias}} in {{Recommender System}}},
  author = {Chen, Jiawei and Dong, Hande and Wang, Xiang and Feng, Fuli and Wang, Meng and He, Xiangnan},
  date = {2020-10-07},
  eprint = {2010.03240},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2010.03240},
  urldate = {2021-07-28},
  abstract = {While recent years have witnessed a rapid growth of research papers on recommender system (RS), most of the papers focus on inventing machine learning models to better fit user behavior data. However, user behavior data is observational rather than experimental. This makes various biases widely exist in the data, including but not limited to selection bias, position bias, exposure bias, and popularity bias. Blindly fitting the data without considering the inherent biases will result in many serious issues, e.g., the discrepancy between offline evaluation and online metrics, hurting user satisfaction and trust on the recommendation service, etc. To transform the large volume of research models into practical improvements, it is highly urgent to explore the impacts of the biases and perform debiasing when necessary. When reviewing the papers that consider biases in RS, we find that, to our surprise, the studies are rather fragmented and lack a systematic organization. The terminology “bias” is widely used in the literature, but its definition is usually vague and even inconsistent across papers. This motivates us to provide a systematic survey of existing work on RS biases. In this paper, we first summarize seven types of biases in recommendation, along with their definitions and characteristics. We then provide a taxonomy to position and organize the existing work on recommendation debiasing. Finally, we identify some open challenges and envision some future directions, with the hope of inspiring more research work on this important yet less investigated topic.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Information Retrieval},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\QJUFTWNK\\Chen et al. - 2020 - Bias and Debias in Recommender System A Survey an.pdf}
}

@article{chenESAMDiscriminativeDomain2020,
  title = {{{ESAM}}: {{Discriminative Domain Adaptation}} with {{Non-Displayed Items}} to {{Improve Long-Tail Performance}}},
  shorttitle = {{{ESAM}}},
  author = {Chen, Zhihong and Xiao, Rong and Li, Chenliang and Ye, Gangfeng and Sun, Haochuan and Deng, Hongbo},
  date = {2020-07-25},
  journaltitle = {Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
  eprint = {2005.10545},
  eprinttype = {arxiv},
  pages = {579--588},
  doi = {10.1145/3397271.3401043},
  url = {http://arxiv.org/abs/2005.10545},
  urldate = {2021-07-28},
  abstract = {Most of ranking models are trained only with displayed items (most are hot items), but they are utilized to retrieve items in the entire space which consists of both displayed and non-displayed items (most are long-tail items). Due to the sample selection bias, the long-tail items lack sufficient records to learn good feature representations, i.e. data sparsity and cold start problems. The resultant distribution discrepancy between displayed and non-displayed items would cause poor long-tail performance. To this end, we propose an entire space adaptation model (ESAM) to address this problem from the perspective of domain adaptation (DA). ESAM regards displayed and non-displayed items as source and target domains respectively. Specifically, we design the attribute correlation alignment that considers the correlation between high-level attributes of the item to achieve distribution alignment. Furthermore, we introduce two effective regularization strategies, i.e. \textbackslash textit\{center-wise clustering\} and \textbackslash textit\{self-training\} to improve DA process. Without requiring any auxiliary information and auxiliary domains, ESAM transfers the knowledge from displayed items to non-displayed items for alleviating the distribution inconsistency. Experiments on two public datasets and a large-scale industrial dataset collected from Taobao demonstrate that ESAM achieves state-of-the-art performance, especially in the long-tail space. Besides, we deploy ESAM to the Taobao search engine, leading to significant improvement on online performance. The code is available at \textbackslash url\{https://github.com/A-bone1/ESAM.git\}},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Information Retrieval},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\SWHIRKBV\\Chen et al. - 2020 - ESAM Discriminative Domain Adaptation with Non-Di.pdf}
}

@online{chengSociallyResponsibleAI2021,
  title = {Socially {{Responsible AI Algorithms}}: {{Issues}}, {{Purposes}}, and {{Challenges}}},
  shorttitle = {Socially {{Responsible AI Algorithms}}},
  author = {Cheng, Lu and Varshney, Kush R. and Liu, Huan},
  date = {2021-06-25},
  eprint = {2101.02032},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2101.02032},
  urldate = {2021-07-29},
  abstract = {In the current era, people and society have grown increasingly reliant on Artificial Intelligence (AI) technologies. AI has the potential to drive us towards a future in which all of humanity flourishes. It also comes with substantial risks for oppression and calamity. Discussions about whether we should (re)trust AI have repeatedly emerged in recent years and in many quarters, including industry, academia, health care, services, and so on. Technologists and AI researchers have a responsibility to develop trustworthy AI systems. They have responded with great efforts of designing more responsible AI algorithms. However, existing technical solutions are narrow in scope and have been primarily directed towards algorithms for scoring or classification tasks, with an emphasis on fairness and unwanted bias. To build long-lasting trust between AI and human beings, we argue that the key is to think beyond algorithmic fairness and connect major aspects of AI that potentially cause AI’s indifferent behavior. In this survey, we provide a systematic framework of Socially Responsible AI Algorithms that aims to examine the subjects of AI indifference and the need for socially responsible AI algorithms, define the objectives, and introduce the means by which we may achieve these objectives. We further discuss how to leverage this framework to improve societal well-being through protection, information, and prevention/mitigation.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computers and Society},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\L9NUI6U9\\Cheng et al. - 2021 - Socially Responsible AI Algorithms Issues, Purpos.pdf}
}

@online{chongQuantifyingEffectsRecommendation2020,
  title = {Quantifying the {{Effects}} of {{Recommendation Systems}}},
  author = {Chong, Sunshine and Abeliuk, Andrés},
  date = {2020-02-03},
  eprint = {2002.01077},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2002.01077},
  urldate = {2021-07-28},
  abstract = {Recommendation systems today exert a strong influence on consumer behavior and individual perceptions of the world. By using collaborative filtering (CF) methods to create recommendations, it generates a continuous feedback loop in which user behavior becomes magnified in the algorithmic system. Popular items get recommended more frequently, creating the bias that affects and alters user preferences. In order to visualize and compare the different biases, we will analyze the effects of recommendation systems and quantify the inequalities resulting from them.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Human-Computer Interaction,Computer Science - Information Retrieval},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\J8ZJ8NDA\\Chong and Abeliuk - 2020 - Quantifying the Effects of Recommendation Systems.pdf}
}

@article{cialdiniSocialInfluenceCompliance2004,
  title = {Social Influence: Compliance and Conformity},
  shorttitle = {Social Influence},
  author = {Cialdini, Robert B. and Goldstein, Noah J.},
  date = {2004},
  journaltitle = {Annu Rev Psychol},
  volume = {55},
  eprint = {14744228},
  eprinttype = {pmid},
  pages = {591--621},
  issn = {0066-4308},
  doi = {10.1146/annurev.psych.55.090902.142015},
  abstract = {This review covers recent developments in the social influence literature, focusing primarily on compliance and conformity research published between 1997 and 2002. The principles and processes underlying a target's susceptibility to outside influences are considered in light of three goals fundamental to rewarding human functioning. Specifically, targets are motivated to form accurate perceptions of reality and react accordingly, to develop and preserve meaningful social relationships, and to maintain a favorable self-concept. Consistent with the current movement in compliance and conformity research, this review emphasizes the ways in which these goals interact with external forces to engender social influence processes that are subtle, indirect, and outside of awareness.},
  langid = {english},
  keywords = {Affect,Arousal,Humans,Motivation,Self Concept,Social Conformity}
}

@article{ciampagliaHowAlgorithmicPopularity2018,
  title = {How Algorithmic Popularity Bias Hinders or Promotes Quality},
  author = {Ciampaglia, Giovanni Luca and Nematzadeh, Azadeh and Menczer, Filippo and Flammini, Alessandro},
  date = {2018-12},
  journaltitle = {Sci Rep},
  volume = {8},
  number = {1},
  pages = {15951},
  issn = {2045-2322},
  doi = {10.1038/s41598-018-34203-2},
  url = {http://www.nature.com/articles/s41598-018-34203-2},
  urldate = {2021-07-29},
  langid = {english},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\NHYAFXWG\\Ciampaglia et al. - 2018 - How algorithmic popularity bias hinders or promote.pdf}
}

@inproceedings{covingtonDeepNeuralNetworks2016,
  title = {Deep {{Neural Networks}} for {{YouTube Recommendations}}},
  booktitle = {Proceedings of the 10th {{ACM Conference}} on {{Recommender Systems}}},
  author = {Covington, Paul and Adams, Jay and Sargin, Emre},
  date = {2016-09-07},
  pages = {191--198},
  publisher = {{ACM}},
  location = {{Boston Massachusetts USA}},
  doi = {10.1145/2959100.2959190},
  url = {https://dl.acm.org/doi/10.1145/2959100.2959190},
  urldate = {2021-07-28},
  abstract = {YouTube represents one of the largest scale and most sophisticated industrial recommendation systems in existence. In this paper, we describe the system at a high level and focus on the dramatic performance improvements brought by deep learning. The paper is split according to the classic two-stage information retrieval dichotomy: first, we detail a deep candidate generation model and then describe a separate deep ranking model. We also provide practical lessons and insights derived from designing, iterating and maintaining a massive recommendation system with enormous userfacing impact.},
  eventtitle = {{{RecSys}} '16: {{Tenth ACM Conference}} on {{Recommender Systems}}},
  isbn = {978-1-4503-4035-9},
  langid = {english},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\4LXCBGQM\\Covington et al. - 2016 - Deep Neural Networks for YouTube Recommendations.pdf}
}

@article{dacremaTroublingAnalysisReproducibility2021,
  title = {A {{Troubling Analysis}} of {{Reproducibility}} and {{Progress}} in {{Recommender Systems Research}}},
  author = {Dacrema, Maurizio Ferrari and Boglio, Simone and Cremonesi, Paolo and Jannach, Dietmar},
  date = {2021-03},
  journaltitle = {ACM Trans. Inf. Syst.},
  volume = {39},
  number = {2},
  eprint = {1911.07698},
  eprinttype = {arxiv},
  pages = {1--49},
  issn = {1046-8188, 1558-2868},
  doi = {10.1145/3434185},
  url = {http://arxiv.org/abs/1911.07698},
  urldate = {2021-11-15},
  abstract = {The design of algorithms that generate personalized ranked item lists is a central topic of research in the field of recommender systems. In the past few years, in particular, approaches based on deep learning (neural) techniques have become dominant in the literature. For all of them, substantial progress over the state-of-the-art is claimed. However, indications exist of certain problems in today's research practice, e.g., with respect to the choice and optimization of the baselines used for comparison, raising questions about the published claims. In order to obtain a better understanding of the actual progress, we have tried to reproduce recent results in the area of neural recommendation approaches based on collaborative filtering. The worrying outcome of the analysis of these recent works-all were published at prestigious scientific conferences between 2015 and 2018-is that 11 out of the 12 reproducible neural approaches can be outperformed by conceptually simple methods, e.g., based on the nearest-neighbor heuristics. None of the computationally complex neural methods was actually consistently better than already existing learning-based techniques, e.g., using matrix factorization or linear models. In our analysis, we discuss common issues in today's research practice, which, despite the many papers that are published on the topic, have apparently led the field to a certain level of stagnation.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Information Retrieval,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\DX22Z5F4\\1911.07698.pdf}
}

@article{dalessandroConscientiousClassificationData2017,
  title = {Conscientious {{Classification}}: {{A Data Scientist}}'s {{Guide}} to {{Discrimination-Aware Classification}}},
  shorttitle = {Conscientious {{Classification}}},
  author = {d' Alessandro, Brian and O'Neil, Cathy and LaGatta, Tom},
  options = {useprefix=true},
  date = {2017-06},
  journaltitle = {Big Data},
  volume = {5},
  number = {2},
  pages = {120--134},
  issn = {2167-6461, 2167-647X},
  doi = {10.1089/big.2016.0048},
  url = {http://www.liebertpub.com/doi/10.1089/big.2016.0048},
  urldate = {2021-07-29},
  abstract = {Recent research has helped to cultivate growing awareness that machine learning systems fueled by big data can create or exacerbate troubling disparities in society. Much of this research comes from outside of the practicing data science community, leaving its members with little concrete guidance to proactively address these concerns. This article introduces issues of discrimination to the data science community on its own terms. In it, we tour the familiar data mining process while providing a taxonomy of common practices that have the potential to produce unintended discrimination. We also survey how discrimination is commonly measured, and suggest how familiar development processes can be augmented to mitigate systems’ discriminatory potential. We advocate that data scientists should be intentional about modeling and reducing discriminatory outcomes. Without doing so, their efforts will result in perpetuating any systemic discrimination that may exist, but under a misleading veil of data-driven objectivity.},
  langid = {english},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\NQISN6Y7\\d'Alessandro et al. - 2017 - Conscientious Classification A Data Scientist's G.pdf}
}

@article{deldjooExplainingRecommenderSystems2021,
  title = {Explaining Recommender Systems Fairness and Accuracy through the Lens of Data Characteristics},
  author = {Deldjoo, Yashar and Bellogin, Alejandro and Di Noia, Tommaso},
  date = {2021-09},
  journaltitle = {Information Processing \& Management},
  volume = {58},
  number = {5},
  pages = {102662},
  issn = {03064573},
  doi = {10.1016/j.ipm.2021.102662},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0306457321001503},
  urldate = {2022-02-20},
  abstract = {The impact of data characteristics on the performance of classical recommender systems has been recently investigated and produced fruitful results about the relationship they have with recommendation accuracy. This work provides a systematic study on the impact of broadly chosen data characteristics (DCs) of recommender systems. This is applied to the accuracy and fairness of several variations of CF recommendation models. We focus on a suite of DCs that capture properties about the structure of the user–item interaction matrix, the rating frequency, item properties, or the distribution of rating values. Experimental validation of the proposed system involved large-scale experiments by performing 23,400 recommendation simulations on three real-world datasets in the movie (ML-100K and ML-1M) and book domains (BookCrossing). The validation results show that the investigated DCs in some cases can have up to 90\% of explanatory power – on several variations of classical CF algorithms –, while they can explain – in the best case – about 40\% of fairness results (measured according to user gender and age sensitive attributes). Therefore, this work evidences that it is more difficult to explain variations in performance when dealing with fairness dimension than accuracy.},
  langid = {english},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\Z6KJZ7MP\\Deldjoo et al. - 2021 - Explaining recommender systems fairness and accura.pdf}
}

@article{deldjooExplainingRecommenderSystems2021a,
  title = {Explaining Recommender Systems Fairness and Accuracy through the Lens of Data Characteristics},
  author = {Deldjoo, Yashar and Bellogin, Alejandro and Di Noia, Tommaso},
  date = {2021},
  journaltitle = {Information Processing \& Management},
  volume = {58},
  number = {5},
  pages = {102662},
  publisher = {{Elsevier}},
  isbn = {0306-4573}
}

@article{deldjooFlexibleFrameworkEvaluating2021,
  title = {A Flexible Framework for Evaluating User and Item Fairness in Recommender Systems},
  author = {Deldjoo, Yashar and Anelli, Vito Walter and Zamani, Hamed and Bellogín, Alejandro and Di Noia, Tommaso},
  date = {2021-01-27},
  journaltitle = {User Model User-Adap Inter},
  issn = {0924-1868, 1573-1391},
  doi = {10.1007/s11257-020-09285-1},
  url = {http://link.springer.com/10.1007/s11257-020-09285-1},
  urldate = {2021-07-28},
  abstract = {One common characteristic of research works focused on fairness evaluation (in machine learning) is that they call for some form of parity (equality) either in treatment—meaning they ignore the information about users’ memberships in protected classes during training—or in impact—by enforcing proportional beneficial outcomes to users in different protected classes. In the recommender systems community, fairness has been studied with respect to both users’ and items’ memberships in protected classes defined by some sensitive attributes (e.g., gender or race for users, revenue in a multi-stakeholder setting for items). Again here, the concept has been commonly interpreted as some form of equality—i.e., the degree to which the system is meeting the information needs of all its users in an equal sense. In this work, we propose a probabilistic framework based on generalized cross entropy (GCE) to measure fairness of a given recommendation model. The framework comes with a suite of advantages: first, it allows the system designer to define and measure fairness for both users and items and can be applied to any classification task; second, it can incorporate various notions of fairness as it does not rely on specific and predefined probability distributions and they can be defined at design time; finally, in its design it uses a gain factor, which can be flexibly defined to contemplate different accuracy-related metrics to measure fairness upon decision-support metrics (e.g., precision, recall) or rank-based measures (e.g., NDCG, MAP). An experimental evaluation on four real-world datasets shows the nuances captured by our proposed metric regarding fairness on different user and item attributes, where nearest-neighbor recommenders tend to obtain good results under equality constraints. We observed that when the users are clustered based on both their interaction with the system and other sensitive attributes, such as age or gender, algorithms with similar performance values get different behaviors with respect to user fairness due to the different way they process data for each user cluster.},
  langid = {english},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\7MMKV22N\\Deldjoo et al. - 2021 - A flexible framework for evaluating user and item .pdf}
}

@article{deldjooFlexibleFrameworkEvaluating2021a,
  title = {A Flexible Framework for Evaluating User and Item Fairness in Recommender Systems},
  author = {Deldjoo, Yashar and Anelli, Vito Walter and Zamani, Hamed and Bellogín, Alejandro and Di Noia, Tommaso},
  date = {2021-01-27},
  journaltitle = {User Model User-Adap Inter},
  issn = {0924-1868, 1573-1391},
  doi = {10.1007/s11257-020-09285-1},
  url = {http://link.springer.com/10.1007/s11257-020-09285-1},
  urldate = {2021-07-28},
  langid = {english},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\E5NT3DQE\\Deldjoo et al. - 2021 - A flexible framework for evaluating user and item .pdf}
}

@article{deshpandeItemBasedTopNRecommendation2004,
  title = {Item-{{Based Top-N Recommendation Algorithms}}},
  author = {Deshpande, Mukund and Karypis, George},
  date = {2004-01},
  journaltitle = {ACM Trans. Inf. Syst.},
  volume = {22},
  number = {1},
  pages = {143--177},
  issn = {1046-8188, 1558-2868},
  doi = {10.1145/963770.963776},
  url = {https://dl.acm.org/doi/10.1145/963770.963776},
  urldate = {2021-12-29},
  abstract = {The explosive growth of the world-wide-web and the emergence of e-commerce has led to the development of               recommender systems               ---a personalized information filtering technology used to identify a set of items that will be of interest to a certain user. User-based collaborative filtering is the most successful technology for building recommender systems to date and is extensively used in many commercial recommender systems. Unfortunately, the computational complexity of these methods grows linearly with the number of customers, which in typical commercial applications can be several millions. To address these scalability concerns model-based recommendation techniques have been developed. These techniques analyze the user--item matrix to discover relations between the different items and use these relations to compute the list of recommendations.In this article, we present one such class of model-based recommendation algorithms that first determines the similarities between the various items and then uses them to identify the set of items to be recommended. The key steps in this class of algorithms are (i) the method used to compute the similarity between the items, and (ii) the method used to combine these similarities in order to compute the similarity between a               basket               of items and a candidate recommender item. Our experimental evaluation on eight real datasets shows that these               item-based               algorithms are up to two orders of magnitude faster than the traditional user-neighborhood based recommender systems and provide recommendations with comparable or better quality.},
  langid = {english},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\XNQXS3M2\\Deshpande and Karypis - 2004 - Item-based top- N recommendation algorithms.pdf}
}

@article{diakopoulosAccountabilityAlgorithmicDecision2016,
  title = {Accountability in Algorithmic Decision Making},
  author = {Diakopoulos, Nicholas},
  date = {2016-01-25},
  journaltitle = {Commun. ACM},
  volume = {59},
  number = {2},
  pages = {56--62},
  issn = {0001-0782},
  doi = {10.1145/2844110},
  url = {https://doi.org/10.1145/2844110},
  urldate = {2022-01-30},
  abstract = {A view from computational journalism.}
}

@online{doranWhatDoesExplainable2017,
  title = {What {{Does Explainable AI Really Mean}}? {{A New Conceptualization}} of {{Perspectives}}},
  shorttitle = {What {{Does Explainable AI Really Mean}}?},
  author = {Doran, Derek and Schulz, Sarah and Besold, Tarek R.},
  date = {2017-10-02},
  eprint = {1710.00794},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1710.00794},
  urldate = {2022-01-30},
  abstract = {We characterize three notions of explainable AI that cut across research fields: opaque systems that offer no insight into its algorithmic mechanisms; interpretable systems where users can mathematically analyze its algorithmic mechanisms; and comprehensible systems that emit symbols enabling user-driven explanations of how a conclusion is reached. The paper is motivated by a corpus analysis of NIPS, ACL, COGSCI, and ICCV/ECCV paper titles showing differences in how work on explainable AI is positioned in various fields. We close by introducing a fourth notion: truly explainable systems, where automated reasoning is central to output crafted explanations without requiring human post processing as final step of the generative process.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\87V6DNXF\\Doran et al. - 2017 - What Does Explainable AI Really Mean A New Concep.pdf}
}

@article{doranWhatDoesExplainable2018,
  title = {What {{Does Explainable AI Really Mean}}? {{A New Conceptualization}} of {{Perspectives}}},
  shorttitle = {What {{Does Explainable AI Really Mean}}?},
  author = {Doran, Derek and Schulz, Sarah and Besold, Tarek R.},
  date = {2018-03-11},
  journaltitle = {CEUR Workshop Proceedings},
  volume = {2071},
  publisher = {{CEUR}},
  issn = {1613-0073},
  url = {http://ceur-ws.org/Vol-2071/},
  urldate = {2022-02-22},
  abstract = {We characterize three notions of explainable AI that cut across research fields: opaque systems that offer no insight into its algorithmic mechanisms; interpretable systems where users can mathemat- ically analyze its algorithmic mechanisms; and comprehensible systems that emit symbols enabling user-driven explanations of how a conclusion is reached. The paper is motivated by a corpus analysis of NIPS, ACL, COGSCI, and ICCV/ECCV paper titles showing differences in how work on explainable AI is positioned in various fields. We close by introducing a fourth notion: truly explainable systems, where automated reasoning is central to output crafted explanations without requiring human post processing as final step of the generative process.},
  langid = {english},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\YGXF5YZ5\\Doran et al. - 2018 - What Does Explainable AI Really Mean A New Concep.pdf;C\:\\Users\\Romanos\\Zotero\\storage\\CCMMVC8I\\18660.html}
}

@online{doshi-velezRigorousScienceInterpretable2017,
  title = {Towards {{A Rigorous Science}} of {{Interpretable Machine Learning}}},
  author = {Doshi-Velez, Finale and Kim, Been},
  date = {2017-03-02},
  eprint = {1702.08608},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1702.08608},
  urldate = {2022-01-22},
  abstract = {As machine learning systems become ubiquitous, there has been a surge of interest in interpretable machine learning: systems that provide explanation for their outputs. These explanations are often used to qualitatively assess other criteria such as safety or non-discrimination. However, despite the interest in interpretability, there is very little consensus on what interpretable machine learning is and how it should be measured. In this position paper, we first define interpretability and describe when interpretability is needed (and when it is not). Next, we suggest a taxonomy for rigorous evaluation and expose open questions towards a more rigorous science of interpretable machine learning.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\F5ZCRQXQ\\Doshi-Velez and Kim - 2017 - Towards A Rigorous Science of Interpretable Machin.pdf}
}

@book{dunhamDataMiningIntroductory2002,
  title = {Data {{Mining}}: {{Introductory}} and {{Advanced Topics}}},
  shorttitle = {Data {{Mining}}},
  author = {Dunham, Margaret H.},
  date = {2002},
  publisher = {{Prentice Hall PTR}},
  location = {{USA}},
  isbn = {978-0-13-088892-1},
  pagetotal = {350}
}

@online{dworkFairnessAwareness2011,
  title = {Fairness {{Through Awareness}}},
  author = {Dwork, Cynthia and Hardt, Moritz and Pitassi, Toniann and Reingold, Omer and Zemel, Rich},
  date = {2011-11-28},
  eprint = {1104.3913},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1104.3913},
  urldate = {2021-08-17},
  abstract = {We study fairness in classification, where individuals are classified, e.g., admitted to a university, and the goal is to prevent discrimination against individuals based on their membership in some group, while maintaining utility for the classifier (the university). The main conceptual contribution of this paper is a framework for fair classification comprising (1) a (hypothetical) task-specific metric for determining the degree to which individuals are similar with respect to the classification task at hand; (2) an algorithm for maximizing utility subject to the fairness constraint, that similar individuals are treated similarly. We also present an adaptation of our approach to achieve the complementary goal of “fair affirmative action,” which guarantees statistical parity (i.e., the demographics of the set of individuals receiving any classification are the same as the demographics of the underlying population), while treating similar individuals as similarly as possible. Finally, we discuss the relationship of fairness to privacy: when fairness implies privacy, and how tools developed in the context of differential privacy may be applied to fairness.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computational Complexity,Computer Science - Computers and Society},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\WLKM4GAN\\Dwork et al. - 2011 - Fairness Through Awareness.pdf}
}

@inproceedings{dworkFairnessAwareness2012,
  title = {Fairness through Awareness},
  booktitle = {Proceedings of the 3rd {{Innovations}} in {{Theoretical Computer Science Conference}}},
  author = {Dwork, Cynthia and Hardt, Moritz and Pitassi, Toniann and Reingold, Omer and Zemel, Richard},
  date = {2012-01-08},
  series = {{{ITCS}} '12},
  pages = {214--226},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/2090236.2090255},
  url = {https://doi.org/10.1145/2090236.2090255},
  urldate = {2022-02-22},
  abstract = {We study fairness in classification, where individuals are classified, e.g., admitted to a university, and the goal is to prevent discrimination against individuals based on their membership in some group, while maintaining utility for the classifier (the university). The main conceptual contribution of this paper is a framework for fair classification comprising (1) a (hypothetical) task-specific metric for determining the degree to which individuals are similar with respect to the classification task at hand; (2) an algorithm for maximizing utility subject to the fairness constraint, that similar individuals are treated similarly. We also present an adaptation of our approach to achieve the complementary goal of "fair affirmative action," which guarantees statistical parity (i.e., the demographics of the set of individuals receiving any classification are the same as the demographics of the underlying population), while treating similar individuals as similarly as possible. Finally, we discuss the relationship of fairness to privacy: when fairness implies privacy, and how tools developed in the context of differential privacy may be applied to fairness.},
  isbn = {978-1-4503-1115-1},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\43WQ9DKX\\Dwork et al. - 2012 - Fairness through awareness.pdf}
}

@article{edizelFaiRecSysMitigatingAlgorithmic2020,
  title = {{{FaiRecSys}}: Mitigating Algorithmic Bias in Recommender Systems},
  shorttitle = {{{FaiRecSys}}},
  author = {Edizel, Bora and Bonchi, Francesco and Hajian, Sara and Panisson, André and Tassa, Tamir},
  date = {2020-03},
  journaltitle = {Int J Data Sci Anal},
  volume = {9},
  number = {2},
  pages = {197--213},
  issn = {2364-415X, 2364-4168},
  doi = {10.1007/s41060-019-00181-5},
  url = {http://link.springer.com/10.1007/s41060-019-00181-5},
  urldate = {2021-07-28},
  abstract = {Recommendation and personalization are useful technologies which influence more and more our daily decisions. However, as we show empirically in this paper, the bias that exists in the real world and which is reflected in the training data can be modeled and amplified by recommender systems and in the end returned as biased recommendations to the users. This feedback process creates a self-perpetuating loop which progressively strengthens the filter bubbles we live in. Biased recommendations can also reinforce stereotypes such as those based on gender or ethnicity, possibly resulting in disparate impact. In this paper we address the problem of algorithmic bias in recommender systems. In particular, we highlight the connection between predictability of sensitive features and bias in the results of recommendations and we then offer a theoretically founded bound on recommendation bias based on that connection. We continue to formalize a fairness constraint and the price that one has to pay, in terms of alterations in the recommendation matrix, in order to achieve fair recommendations. Finally, we propose FaiRecSys—an algorithm that mitigates algorithmic bias by post-processing the recommendation matrix with minimum impact on the utility of recommendations provided to the end-users.},
  langid = {english},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\TQ5AIXNB\\Edizel et al. - 2020 - FaiRecSys mitigating algorithmic bias in recommen.pdf}
}

@online{ekstrandExploringAuthorGender2020,
  title = {Exploring {{Author Gender}} in {{Book Rating}} and {{Recommendation}}},
  author = {Ekstrand, Michael D. and Kluver, Daniel},
  date = {2020-07-24},
  eprint = {1808.07586},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1808.07586},
  urldate = {2021-07-28},
  abstract = {Collaborative filtering algorithms find useful patterns in rating and consumption data and exploit these patterns to guide users to good items. Many of the patterns in rating datasets reflect important real-world differences between the various users and items in the data; other patterns may be irrelevant or possibly undesirable for social or ethical reasons, particularly if they reflect undesired discrimination, such as discrimination in publishing or purchasing against authors who are women or ethnic minorities. In this work, we examine the response of collaborative filtering recommender algorithms to the distribution of their input data with respect to a dimension of social concern, namely content creator gender. Using publicly-available book ratings data, we measure the distribution of the genders of the authors of books in user rating profiles and recommendation lists produced from this data. We find that common collaborative filtering algorithms differ in the gender distribution of their recommendation lists, and in the relationship of that output distribution to user profile distribution.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Human-Computer Interaction,Computer Science - Information Retrieval},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\TCLFERTS\\Ekstrand and Kluver - 2020 - Exploring Author Gender in Book Rating and Recomme.pdf}
}

@article{ekstrandExploringAuthorGender2021,
  title = {Exploring Author Gender in Book Rating and Recommendation},
  author = {Ekstrand, Michael D. and Kluver, Daniel},
  date = {2021-02-04},
  journaltitle = {User Model User-Adap Inter},
  issn = {0924-1868, 1573-1391},
  doi = {10.1007/s11257-020-09284-2},
  url = {http://link.springer.com/10.1007/s11257-020-09284-2},
  urldate = {2021-07-28},
  abstract = {Collaborative filtering algorithms find useful patterns in rating and consumption data and exploit these patterns to guide users to good items. Many of these patterns reflect important real-world phenomena driving interactions between the various users and items; other patterns may be irrelevant or reflect undesired discrimination, such as discrimination in publishing or purchasing against authors who are women or ethnic minorities. In this work, we examine the response of collaborative filtering recommender algorithms to the distribution of their input data with respect to one dimension of social concern, namely content creator gender. Using publicly available book ratings data, we measure the distribution of the genders of the authors of books in user rating profiles and recommendation lists produced from this data. We find that common collaborative filtering algorithms tend to propagate at least some of each user’s tendency to rate or read male or female authors into their resulting recommendations, although they differ in both the strength of this propagation and the variance in the gender balance of the recommendation lists they produce. The data, experimental design, and statistical methods are designed to be reusable for studying potentially discriminatory social dimensions of recommendations in other domains and settings as well.},
  langid = {english},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\KTHHNB5Z\\Ekstrand and Kluver - 2021 - Exploring author gender in book rating and recomme.pdf}
}

@article{ekstrandLensKitPythonNextGeneration2020,
  title = {{{LensKit}} for {{Python}}: {{Next-Generation Software}} for {{Recommender System Experiments}}},
  shorttitle = {{{LensKit}} for {{Python}}},
  author = {Ekstrand, Michael D.},
  date = {2020-10-19},
  journaltitle = {Proceedings of the 29th ACM International Conference on Information \& Knowledge Management},
  eprint = {1809.03125},
  eprinttype = {arxiv},
  pages = {2999--3006},
  doi = {10.1145/3340531.3412778},
  url = {http://arxiv.org/abs/1809.03125},
  urldate = {2021-07-28},
  abstract = {LensKit is an open-source toolkit for building, researching, and learning about recommender systems. First released in 2010 as a Java framework, it has supported diverse published research, smallscale production deployments, and education in both MOOC and traditional classroom settings. In this paper, I present the next generation of the LensKit project, re-envisioning the original tool’s objectives as flexible Python package for supporting recommender systems research and development. LensKit for Python (LKPY) enables researchers and students to build robust, flexible, and reproducible experiments that make use of the large and growing PyData and Scientific Python ecosystem, including scikit-learn, and TensorFlow. To that end, it provides classical collaborative filtering implementations, recommender system evaluation metrics, data preparation routines, and tools for efficiently batch running recommendation algorithms, all usable in any combination with each other or with other Python software. This paper describes the design goals, use cases, and capabilities of LKPY, contextualized in a reflection on the successes and failures of the original LensKit for Java software.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Information Retrieval,Computer Science - Machine Learning,H.3.3},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\54WJQIP9\\Ekstrand - 2020 - LensKit for Python Next-Generation Software for R.pdf}
}

@dataset{ekstrandScriptsAllCool,
  title = {Scripts for {{All The Cool Kids}}, {{How Do They Fit In}}},
  author = {Ekstrand, Michael and Tian, Mucun and Azpiazu, Ion Madrazo and Ekstrand, Jennifer D. and Anuyah, Oghenemaro and McNeill, David and Pera, Maria Soledad},
  publisher = {{Boise State University}},
  doi = {10.18122/B2GM6F},
  url = {http://scholarworks.boisestate.edu/cs_scripts/4/},
  urldate = {2021-07-29},
  langid = {english},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\NDXBTHHK\\Ekstrand et al. - Scripts for All The Cool Kids, How Do They Fit In.pdf}
}

@dataset{ekstrandScriptsDemographicsCool,
  title = {Scripts for {{The Demographics}} of {{Cool}}},
  author = {Ekstrand, Michael and Pera, Maria Soledad},
  publisher = {{Boise State University}},
  doi = {10.18122/B2ND8P},
  url = {http://scholarworks.boisestate.edu/cs_scripts/3/},
  urldate = {2021-07-28},
  abstract = {Typical recommender evaluations treat users as an homogeneous unit. However, user subgroups often differ in their tastes, which can result more broadly in diverse recommender needs. Thus, these groups may have different degrees of satisfaction with the provided recommendations. We explore the offline top-N performance of collaborative filtering algorithms across two domains. We find that several strategies achieve higher accuracy for dominant demographic groups, thus increasing the overall performance for the strategy, without providing increased benefits for other users.},
  langid = {english},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\DYGAAV2X\\Ekstrand and Pera - Scripts for The Demographics of Cool.pdf}
}

@inproceedings{elahiAlgorithmicFairnessRecommender2021,
  title = {Beyond {{Algorithmic Fairness}} in {{Recommender Systems}}},
  booktitle = {Adjunct {{Proceedings}} of the 29th {{ACM Conference}} on {{User Modeling}}, {{Adaptation}} and {{Personalization}}},
  author = {Elahi, Mehdi and Abdollahpouri, Himan and Mansoury, Masoud and Torkamaan, Helma},
  date = {2021-06-21},
  pages = {41--46},
  publisher = {{ACM}},
  location = {{Utrecht Netherlands}},
  doi = {10.1145/3450614.3461685},
  url = {https://dl.acm.org/doi/10.1145/3450614.3461685},
  urldate = {2022-01-27},
  abstract = {Fairness is one of the crucial aspects of modern Recommender Systems which has recently drawn substantial attention from the community. Many recent works have addressed this aspect by studying the fairness of the recommendation through different forms of evaluation methodologies and metrics. However, the majority of these works have mainly concentrated on the recommendation algorithms and hence measured the fairness from the algorithmic viewpoint. While such viewpoint may still play an important role, it does not necessarily project a comprehensive picture of how the users may perceive the overall fairness of a recommender system. This paper extends the prior works and goes beyond the algorithmic fairness in recommender systems by highlighting the non-algorithmic viewpoint on the fairness in these systems. The paper proposes an evaluation methodology that can be used to assess the fairness of a recommender system perceived by its users. We have adopted a well-known model and re-formulated it to suit the particular characteristics of the recommender systems, and accordingly, their corresponding users. Our proposed methodology can be used in order to elicit the feedback of the users, along with three important dimensions, i.e., Engagement, Representation, and Action \& Expression. We have formed a set of survey questions that address the aforementioned dimensions, as a set of examples to assess the fairness in a recommender system.},
  eventtitle = {{{UMAP}} '21: 29th {{ACM Conference}} on {{User Modeling}}, {{Adaptation}} and {{Personalization}}},
  isbn = {978-1-4503-8367-7},
  langid = {english},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\XACW6LVL\\Elahi et al. - 2021 - Beyond Algorithmic Fairness in Recommender Systems.pdf}
}

@online{ElementsStatisticalLearning,
  title = {The {{Elements}} of {{Statistical Learning}} | {{BibSonomy}}},
  url = {https://www.bibsonomy.org/bibtex/2f58afc5c9793fcc8ad8389824e57984c/sb3000},
  urldate = {2021-12-19},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\PQYRMD7T\\sb3000.html}
}

@online{ElementsStatisticalLearninga,
  title = {The {{Elements}} of {{Statistical Learning}} | {{BibSonomy}}},
  url = {https://www.bibsonomy.org/bibtex/2f58afc5c9793fcc8ad8389824e57984c/sb3000},
  urldate = {2021-12-19}
}

@online{EqualityDiversityPolicy,
  title = {Equality and {{Diversity Policy}}},
  url = {https://www.amnesty.org.uk/equality-and-diversity-policy},
  urldate = {2022-02-20},
  abstract = {We are Amnesty International UK. We are ordinary people from across the world standing up for humanity and human rights.},
  organization = {{Amnesty International UK}},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\2FV2W9WS\\equality-and-diversity-policy.html}
}

@book{falkPracticalRecommenderSystems2019,
  title = {Practical Recommender Systems},
  author = {Falk, Kim},
  date = {2019},
  publisher = {{Manning}},
  location = {{Shelter Island, NY}},
  abstract = {Online recommender systems help users find movies, jobs, restaurants, even romance! There's an art in combining statistics, demographics, and query terms to achieve results that will delight them. Learn to build a recommender system the right way : it can make or break your application! "Practical recommender systems" explains how recommender systems work and shows how to create and apply them for your site. After covering the basics, you'll see how to collect user data and produce personalized recommendations. You'll learn how to use the most popular recommendation algorithms and see examples of them in action on sites like Amazon and Netflix. Finally, the book covers scaling problems and other issues you'll encounter as your site grows},
  isbn = {978-1-61729-270-5},
  pagetotal = {406},
  keywords = {Recommender systems (Information filtering)},
  annotation = {OCLC: ocn970761259},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\35BTEF7W\\Falk - 2019 - Practical recommender systems.pdf}
}

@article{fayyazRecommendationSystemsAlgorithms2020,
  title = {Recommendation {{Systems}}: {{Algorithms}}, {{Challenges}}, {{Metrics}}, and {{Business Opportunities}}},
  shorttitle = {Recommendation {{Systems}}},
  author = {Fayyaz, Zeshan and Ebrahimian, Mahsa and Nawara, Dina and Ibrahim, Ahmed and Kashef, Rasha},
  date = {2020-11-02},
  journaltitle = {Applied Sciences},
  volume = {10},
  number = {21},
  pages = {7748},
  issn = {2076-3417},
  doi = {10.3390/app10217748},
  url = {https://www.mdpi.com/2076-3417/10/21/7748},
  urldate = {2021-07-28},
  abstract = {Recommender systems are widely used to provide users with recommendations based on their preferences. With the ever-growing volume of information online, recommender systems have been a useful tool to overcome information overload. The utilization of recommender systems cannot be overstated, given its potential influence to ameliorate many over-choice challenges. There are many types of recommendation systems with different methodologies and concepts. Various applications have adopted recommendation systems, including e-commerce, healthcare, transportation, agriculture, and media. This paper provides the current landscape of recommender systems research and identifies directions in the field in various applications. This article provides an overview of the current state of the art in recommendation systems, their types, challenges, limitations, and business adoptions. To assess the quality of a recommendation system, qualitative evaluation metrics are discussed in the paper.},
  langid = {english},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\89MDW8Y6\\Fayyaz et al. - 2020 - Recommendation Systems Algorithms, Challenges, Me.pdf}
}

@online{feldmanCertifyingRemovingDisparate2015,
  title = {Certifying and Removing Disparate Impact},
  author = {Feldman, Michael and Friedler, Sorelle and Moeller, John and Scheidegger, Carlos and Venkatasubramanian, Suresh},
  date = {2015-07-15},
  eprint = {1412.3756},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1412.3756},
  urldate = {2021-07-29},
  abstract = {What does it mean for an algorithm to be biased? In U.S. law, unintentional bias is encoded via disparate impact, which occurs when a selection process has widely different outcomes for different groups, even as it appears to be neutral. This legal determination hinges on a definition of a protected class (ethnicity, gender) and an explicit description of the process.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computers and Society,Statistics - Machine Learning},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\AZP2X6GB\\Feldman et al. - 2015 - Certifying and removing disparate impact.pdf}
}

@article{ferraraRiseSocialBots2016,
  title = {The Rise of Social Bots},
  author = {Ferrara, Emilio and Varol, Onur and Davis, Clayton and Menczer, Filippo and Flammini, Alessandro},
  date = {2016-06-24},
  journaltitle = {Commun. ACM},
  volume = {59},
  number = {7},
  pages = {96--104},
  issn = {0001-0782},
  doi = {10.1145/2818717},
  url = {https://doi.org/10.1145/2818717},
  urldate = {2022-02-15},
  abstract = {Today's social bots are sophisticated and sometimes menacing. Indeed, their presence can endanger online ecosystems as well as our society.},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\TZR7ZCBA\\Ferrara et al. - 2016 - The rise of social bots.pdf}
}

@online{flamminiRiseSocialBots,
  title = {The {{Rise}} of {{Social Bots}}},
  author = {Flammini, Onur Varol, Clayton Davis, Filippo Menczer, Alessandro, Emilio Ferrara},
  url = {https://cacm.acm.org/magazines/2016/7/204021-the-rise-of-social-bots/fulltext},
  urldate = {2022-02-15},
  abstract = {Today\&\#39;s social bots are sophisticated and sometimes menacing. Indeed, their presence can endanger online ecosystems as well as our society.},
  langid = {english},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\Z4QGTF2Q\\fulltext.html}
}

@online{friedlerComparativeStudyFairnessenhancing2018,
  title = {A Comparative Study of Fairness-Enhancing Interventions in Machine Learning},
  author = {Friedler, Sorelle A. and Scheidegger, Carlos and Venkatasubramanian, Suresh and Choudhary, Sonam and Hamilton, Evan P. and Roth, Derek},
  date = {2018-02-12},
  eprint = {1802.04422},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1802.04422},
  urldate = {2021-07-29},
  abstract = {Computers are increasingly used to make decisions that have significant impact in people’s lives. Often, these predictions can affect different population subgroups disproportionately. As a result, the issue of fairness has received much recent interest, and a number of fairness-enhanced classifiers and predictors have appeared in the literature. This paper seeks to study the following questions: how do these different techniques fundamentally compare to one another, and what accounts for the differences? Specifically, we seek to bring attention to many under-appreciated aspects of such fairness-enhancing interventions. Concretely, we present the results of an open benchmark we have developed that lets us compare a number of different algorithms under a variety of fairness measures, and a large number of existing datasets. We find that although different algorithms tend to prefer specific formulations of fairness preservations, many of these measures strongly correlate with one another. In addition, we find that fairness-preserving algorithms tend to be sensitive to fluctuations in dataset composition (simulated in our benchmark by varying training-test splits), indicating that fairness interventions might be more brittle than previously thought.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computers and Society,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\64B8J2SB\\Friedler et al. - 2018 - A comparative study of fairness-enhancing interven.pdf}
}

@inproceedings{gaoCounteractingBiasIncreasing2020,
  title = {Counteracting {{Bias}} and {{Increasing Fairness}} in {{Search}} and {{Recommender Systems}}},
  booktitle = {Fourteenth {{ACM Conference}} on {{Recommender Systems}}},
  author = {Gao, Ruoyuan and Shah, Chirag},
  date = {2020-09-22},
  pages = {745--747},
  publisher = {{ACM}},
  location = {{Virtual Event Brazil}},
  doi = {10.1145/3383313.3411545},
  url = {https://dl.acm.org/doi/10.1145/3383313.3411545},
  urldate = {2021-07-28},
  eventtitle = {{{RecSys}} '20: {{Fourteenth ACM Conference}} on {{Recommender Systems}}},
  isbn = {978-1-4503-7583-2},
  langid = {english},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\2RLPHDN9\\Gao and Shah - 2020 - Counteracting Bias and Increasing Fairness in Sear.pdf}
}

@online{gardnerEvaluatingModelsLocal2020,
  title = {Evaluating {{Models}}' {{Local Decision Boundaries}} via {{Contrast Sets}}},
  author = {Gardner, Matt and Artzi, Yoav and Basmova, Victoria and Berant, Jonathan and Bogin, Ben and Chen, Sihao and Dasigi, Pradeep and Dua, Dheeru and Elazar, Yanai and Gottumukkala, Ananth and Gupta, Nitish and Hajishirzi, Hanna and Ilharco, Gabriel and Khashabi, Daniel and Lin, Kevin and Liu, Jiangming and Liu, Nelson F. and Mulcaire, Phoebe and Ning, Qiang and Singh, Sameer and Smith, Noah A. and Subramanian, Sanjay and Tsarfaty, Reut and Wallace, Eric and Zhang, Ally and Zhou, Ben},
  date = {2020-10-01},
  eprint = {2004.02709},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2004.02709},
  urldate = {2021-07-29},
  abstract = {Standard test sets for supervised learning evaluate in-distribution generalization. Unfortunately, when a dataset has systematic gaps (e.g., annotation artifacts), these evaluations are misleading: a model can learn simple decision rules that perform well on the test set but do not capture a dataset's intended capabilities. We propose a new annotation paradigm for NLP that helps to close systematic gaps in the test data. In particular, after a dataset is constructed, we recommend that the dataset authors manually perturb the test instances in small but meaningful ways that (typically) change the gold label, creating contrast sets. Contrast sets provide a local view of a model's decision boundary, which can be used to more accurately evaluate a model's true linguistic capabilities. We demonstrate the efficacy of contrast sets by creating them for 10 diverse NLP datasets (e.g., DROP reading comprehension, UD parsing, IMDb sentiment analysis). Although our contrast sets are not explicitly adversarial, model performance is significantly lower on them than on the original test sets---up to 25\textbackslash\% in some cases. We release our contrast sets as new evaluation benchmarks and encourage future dataset construction efforts to follow similar annotation processes.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\3GQBBME4\\Gardner et al. - 2020 - Evaluating Models' Local Decision Boundaries via C.pdf}
}

@article{geLongtermFairnessRecommendation2021,
  title = {Towards {{Long-term Fairness}} in {{Recommendation}}},
  author = {Ge, Yingqiang and Liu, Shuchang and Gao, Ruoyuan and Xian, Yikun and Li, Yunqi and Zhao, Xiangyu and Pei, Changhua and Sun, Fei and Ge, Junfeng and Ou, Wenwu and Zhang, Yongfeng},
  date = {2021-03-08},
  journaltitle = {Proceedings of the 14th ACM International Conference on Web Search and Data Mining},
  eprint = {2101.03584},
  eprinttype = {arxiv},
  pages = {445--453},
  doi = {10.1145/3437963.3441824},
  url = {http://arxiv.org/abs/2101.03584},
  urldate = {2022-01-26},
  abstract = {As Recommender Systems (RS) influence more and more people in their daily life, the issue of fairness in recommendation is becoming more and more important. Most of the prior approaches to fairness-aware recommendation have been situated in a static or one-shot setting, where the protected groups of items are fixed, and the model provides a one-time fairness solution based on fairnessconstrained optimization. This fails to consider the dynamic nature of the recommender systems, where attributes such as item popularity may change over time due to the recommendation policy and user engagement. For example, products that were once popular may become no longer popular, and vice versa. As a result, the system that aims to maintain long-term fairness on the item exposure in different popularity groups must accommodate this change in a timely fashion.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Information Retrieval},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\E9SFBDUX\\Ge et al. - 2021 - Towards Long-term Fairness in Recommendation.pdf}
}

@inproceedings{geyikFairnessAwareRankingSearch2019,
  title = {Fairness-{{Aware Ranking}} in {{Search}} \& {{Recommendation Systems}} with {{Application}} to {{LinkedIn Talent Search}}},
  booktitle = {Proceedings of the 25th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} \& {{Data Mining}}},
  author = {Geyik, Sahin Cem and Ambler, Stuart and Kenthapadi, Krishnaram},
  date = {2019-07-25},
  pages = {2221--2231},
  publisher = {{ACM}},
  location = {{Anchorage AK USA}},
  doi = {10.1145/3292500.3330691},
  url = {https://dl.acm.org/doi/10.1145/3292500.3330691},
  urldate = {2021-07-28},
  abstract = {We present a framework for quantifying and mitigating algorithmic bias in mechanisms designed for ranking individuals, typically used as part of web-scale search and recommendation systems. We first propose complementary measures to quantify bias with respect to protected attributes such as gender and age. We then present algorithms for computing fairness-aware re-ranking of results. For a given search or recommendation task, our algorithms seek to achieve a desired distribution of top ranked results with respect to one or more protected attributes. We show that such a framework can be tailored to achieve fairness criteria such as equality of opportunity and demographic parity depending on the choice of the desired distribution. We evaluate the proposed algorithms via extensive simulations over different parameter choices, and study the effect of fairness-aware ranking on both bias and utility measures. We finally present the online A/B testing results from applying our framework towards representative ranking in LinkedIn Talent Search, and discuss the lessons learned in practice. Our approach resulted in tremendous improvement in the fairness metrics (nearly three fold increase in the number of search queries with representative results) without affecting the business metrics, which paved the way for deployment to 100\% of LinkedIn Recruiter users worldwide. Ours is the first large-scale deployed framework for ensuring fairness in the hiring domain, with the potential positive impact for more than 630M LinkedIn members.},
  eventtitle = {{{KDD}} '19: {{The}} 25th {{ACM SIGKDD Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  isbn = {978-1-4503-6201-6},
  langid = {english},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\CENJIFBD\\Geyik et al. - 2019 - Fairness-Aware Ranking in Search & Recommendation .pdf}
}

@article{giniMeasurementInequalityIncomes1921,
  title = {Measurement of {{Inequality}} of {{Incomes}}},
  author = {Gini, Corrado},
  date = {1921},
  journaltitle = {The Economic Journal},
  volume = {31},
  number = {121},
  eprint = {2223319},
  eprinttype = {jstor},
  pages = {124--126},
  publisher = {{[Royal Economic Society, Wiley]}},
  issn = {0013-0133},
  doi = {10.2307/2223319},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\XSIP738T\\Gini - 1921 - Measurement of Inequality of Incomes.pdf}
}

@article{goldbergUsingCollaborativeFiltering1992,
  title = {Using Collaborative Filtering to Weave an Information Tapestry},
  author = {Goldberg, David and Nichols, David and Oki, Brian M. and Terry, Douglas},
  date = {1992-12},
  journaltitle = {Commun. ACM},
  volume = {35},
  number = {12},
  pages = {61--70},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/138859.138867},
  url = {https://dl.acm.org/doi/10.1145/138859.138867},
  urldate = {2021-07-28},
  langid = {english},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\YMQM6FCB\\Goldberg et al. - 1992 - Using collaborative filtering to weave an informat.pdf}
}

@article{goosLectureNotesComputer,
  title = {Lecture {{Notes}} in {{Computer Science}}},
  author = {Goos, Gerhard and Hartmanis, Juris and van Leeuwen, Jan and Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M and Mattern, Friedemann and Mitchell, John C and Naor, Moni and Nierstrasz, Oscar and Rangan, C Pandu and Steffen, Bernhard},
  options = {useprefix=true},
  pages = {770},
  langid = {english},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\BW3TZCCK\\Goos et al. - Lecture Notes in Computer Science.pdf}
}

@article{goosLectureNotesComputera,
  title = {Lecture {{Notes}} in {{Computer Science}}},
  author = {Goos, Gerhard and Hartmanis, Juris and van Leeuwen, Jan and Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M and Mattern, Friedemann and Mitchell, John C and Naor, Moni and Nierstrasz, Oscar and Rangan, C Pandu and Steffen, Bernhard},
  options = {useprefix=true},
  pages = {770},
  langid = {english},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\UYBXLBHF\\Goos et al. - Lecture Notes in Computer Science.pdf}
}

@article{goosLectureNotesComputerb,
  title = {Lecture {{Notes}} in {{Computer Science}}},
  author = {Goos, Gerhard and Hartmanis, Juris and van Leeuwen, Jan and Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M and Mattern, Friedemann and Mitchell, John C and Naor, Moni and Nierstrasz, Oscar and Rangan, C Pandu and Steffen, Bernhard},
  options = {useprefix=true},
  pages = {770},
  langid = {english},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\MNVIE659\\Goos et al. - Lecture Notes in Computer Science.pdf}
}

@article{goyalGraphEmbeddingTechniques2018,
  title = {Graph {{Embedding Techniques}}, {{Applications}}, and {{Performance}}: {{A Survey}}},
  shorttitle = {Graph {{Embedding Techniques}}, {{Applications}}, and {{Performance}}},
  author = {Goyal, Palash and Ferrara, Emilio},
  date = {2018-07},
  journaltitle = {Knowledge-Based Systems},
  volume = {151},
  eprint = {1705.02801},
  eprinttype = {arxiv},
  pages = {78--94},
  issn = {09507051},
  doi = {10.1016/j.knosys.2018.03.022},
  url = {http://arxiv.org/abs/1705.02801},
  urldate = {2022-03-06},
  abstract = {Graphs, such as social networks, word co-occurrence networks, and communication networks, occur naturally in various real-world applications. Analyzing them yields insight into the structure of society, language, and different patterns of communication. Many approaches have been proposed to perform the analysis. Recently, methods which use the representation of graph nodes in vector space have gained traction from the research community. In this survey, we provide a comprehensive and structured analysis of various graph embedding techniques proposed in the literature. We first introduce the embedding task and its challenges such as scalability, choice of dimensionality, and features to be preserved, and their possible solutions. We then present three categories of approaches based on factorization methods, random walks, and deep learning, with examples of representative algorithms in each category and analysis of their performance on various tasks. We evaluate these state-of-the-art methods on a few common datasets and compare their performance against one another. Our analysis concludes by suggesting some potential applications and future directions. We finally present the open-source Python library we developed, named GEM (Graph Embedding Methods, available at https://github.com/palash1992/GEM), which provides all presented algorithms within a unified interface to foster and facilitate research on the topic.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Social and Information Networks,Physics - Data Analysis; Statistics and Probability},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\LT2KA9W9\\Goyal and Ferrara - 2018 - Graph Embedding Techniques, Applications, and Perf.pdf}
}

@inproceedings{guoDeepFMFactorizationmachineBased2017,
  title = {{{DeepFM}}: A Factorization-Machine Based Neural Network for {{CTR}} Prediction},
  shorttitle = {{{DeepFM}}},
  booktitle = {Proceedings of the 26th {{International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Guo, Huifeng and Tang, Ruiming and Ye, Yunming and Li, Zhenguo and He, Xiuqiang},
  date = {2017-08-19},
  series = {{{IJCAI}}'17},
  pages = {1725--1731},
  publisher = {{AAAI Press}},
  location = {{Melbourne, Australia}},
  abstract = {Learning sophisticated feature interactions behind user behaviors is critical in maximizing CTR for recommender systems. Despite great progress, existing methods seem to have a strong bias towards low- or high-order interactions, or require expertise feature engineering. In this paper, we show that it is possible to derive an end-to-end learning model that emphasizes both low- and high-order feature interactions. The proposed model, DeepFM, combines the power of factorization machines for recommendation and deep learning for feature learning in a new neural network architecture. Compared to the latest Wide \& Deep model from Google, DeepFM has a shared input to its "wide" and "deep" parts, with no need of feature engineering besides raw features. Comprehensive experiments are conducted to demonstrate the effectiveness and efficiency of DeepFM over the existing models for CTR prediction, on both benchmark data and commercial data.},
  isbn = {978-0-9992411-0-3}
}

@online{haoHowFacebookGot,
  title = {How {{Facebook}} Got Addicted to Spreading Misinformation},
  author = {Hao, Karen},
  url = {https://www.technologyreview.com/2021/03/11/1020600/facebook-responsible-ai-misinformation/},
  urldate = {2022-01-12},
  abstract = {The company’s AI algorithms gave it an insatiable habit for lies and hate speech. Now the man who built them can't fix the problem.},
  langid = {english},
  organization = {{MIT Technology Review}}
}

@article{hardtEqualityOpportunitySupervised2016,
  title = {Equality of {{Opportunity}} in {{Supervised Learning}}},
  author = {Hardt, Moritz and Price, Eric and Srebro, Nathan},
  date = {2016-10-07},
  journaltitle = {Advances in neural information processing systems,},
  volume = {29},
  eprint = {1610.02413},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1610.02413},
  urldate = {2021-08-18},
  abstract = {We propose a criterion for discrimination against a specified sensitive attribute in supervised learning, where the goal is to predict some target based on available features. Assuming data about the predictor, target, and membership in the protected group are available, we show how to optimally adjust any learned predictor so as to remove discrimination according to our definition. Our framework also improves incentives by shifting the cost of poor classification from disadvantaged groups to the decision maker, who can respond by improving the classification accuracy.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\CUDDHMQW\\Hardt et al. - 2016 - Equality of Opportunity in Supervised Learning.pdf}
}

@article{harperMovieLensDatasetsHistory2015,
  title = {The {{MovieLens Datasets}}: {{History}} and {{Context}}},
  shorttitle = {The {{MovieLens Datasets}}},
  author = {Harper, F. Maxwell and Konstan, Joseph A.},
  date = {2015-12-22},
  journaltitle = {ACM Trans. Interact. Intell. Syst.},
  volume = {5},
  number = {4},
  pages = {19:1--19:19},
  issn = {2160-6455},
  doi = {10.1145/2827872},
  url = {https://doi.org/10.1145/2827872},
  urldate = {2021-11-22},
  abstract = {The MovieLens datasets are widely used in education, research, and industry. They are downloaded hundreds of thousands of times each year, reflecting their use in popular press programming books, traditional and online courses, and software. These datasets are a product of member activity in the MovieLens movie recommendation system, an active research platform that has hosted many experiments since its launch in 1997. This article documents the history of MovieLens and the MovieLens datasets. We include a discussion of lessons learned from running a long-standing, live research platform from the perspective of a research organization. We document best practices and limitations of using the MovieLens datasets in new research.},
  keywords = {Datasets,MovieLens,ratings,recommendations},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\FVTAWGKV\\Harper and Konstan - 2015 - The MovieLens Datasets History and Context.pdf}
}

@book{hastie2009elements,
  title = {The Elements of Statistical Learning: {{Data}} Mining, Inference, and Prediction},
  author = {Hastie, T. and Tibshirani, R. and Friedman, J.H.},
  date = {2009},
  series = {Springer Series in Statistics},
  publisher = {{Springer}},
  url = {https://books.google.gr/books?id=eBSgoAEACAAJ},
  isbn = {978-0-387-84884-6},
  lccn = {2008941148}
}

@incollection{heLightGCNSimplifyingPowering2020,
  title = {{{LightGCN}}: {{Simplifying}} and {{Powering Graph Convolution Network}} for {{Recommendation}}},
  shorttitle = {{{LightGCN}}},
  booktitle = {Proceedings of the 43rd {{International ACM SIGIR Conference}} on {{Research}} and {{Development}} in {{Information Retrieval}}},
  author = {He, Xiangnan and Deng, Kuan and Wang, Xiang and Li, Yan and Zhang, YongDong and Wang, Meng},
  date = {2020-07-25},
  pages = {639--648},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  url = {https://doi.org/10.1145/3397271.3401063},
  urldate = {2021-07-29},
  abstract = {Graph Convolution Network (GCN) has become new state-of-the-art for collaborative filtering. Nevertheless, the reasons of its effectiveness for recommendation are not well understood. Existing work that adapts GCN to recommendation lacks thorough ablation analyses on GCN, which is originally designed for graph classification tasks and equipped with many neural network operations. However, we empirically find that the two most common designs in GCNs -- feature transformation and nonlinear activation -- contribute little to the performance of collaborative filtering. Even worse, including them adds to the difficulty of training and degrades recommendation performance. In this work, we aim to simplify the design of GCN to make it more concise and appropriate for recommendation. We propose a new model named LightGCN, including only the most essential component in GCN -- neighborhood aggregation -- for collaborative filtering. Specifically, LightGCN learns user and item embeddings by linearly propagating them on the user-item interaction graph, and uses the weighted sum of the embeddings learned at all layers as the final embedding. Such simple, linear, and neat model is much easier to implement and train, exhibiting substantial improvements (about 16.0\% relative improvement on average) over Neural Graph Collaborative Filtering (NGCF) -- a state-of-the-art GCN-based recommender model -- under exactly the same experimental setting. Further analyses are provided towards the rationality of the simple LightGCN from both analytical and empirical perspectives.},
  isbn = {978-1-4503-8016-4},
  keywords = {collaborative filtering,embedding propagation,graph neural network,recommendation}
}

@online{heNeuralCollaborativeFiltering2017,
  title = {Neural {{Collaborative Filtering}}},
  author = {He, Xiangnan and Liao, Lizi and Zhang, Hanwang and Nie, Liqiang and Hu, Xia and Chua, Tat-Seng},
  date = {2017-08-25},
  eprint = {1708.05031},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1708.05031},
  urldate = {2021-07-29},
  abstract = {In recent years, deep neural networks have yielded immense success on speech recognition, computer vision and natural language processing. However, the exploration of deep neural networks on recommender systems has received relatively less scrutiny. In this work, we strive to develop techniques based on neural networks to tackle the key problem in recommendation -- collaborative filtering -- on the basis of implicit feedback. Although some recent work has employed deep learning for recommendation, they primarily used it to model auxiliary information, such as textual descriptions of items and acoustic features of musics. When it comes to model the key factor in collaborative filtering -- the interaction between user and item features, they still resorted to matrix factorization and applied an inner product on the latent features of users and items. By replacing the inner product with a neural architecture that can learn an arbitrary function from data, we present a general framework named NCF, short for Neural network-based Collaborative Filtering. NCF is generic and can express and generalize matrix factorization under its framework. To supercharge NCF modelling with non-linearities, we propose to leverage a multi-layer perceptron to learn the user-item interaction function. Extensive experiments on two real-world datasets show significant improvements of our proposed NCF framework over the state-of-the-art methods. Empirical evidence shows that using deeper layers of neural networks offers better recommendation performance.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Information Retrieval},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\TTHWYZFS\\He et al. - 2017 - Neural Collaborative Filtering.pdf;C\:\\Users\\Romanos\\Zotero\\storage\\EYCSD3Z8\\1708.html}
}

@article{herlockerEvaluatingCollaborativeFiltering2004,
  title = {Evaluating Collaborative Filtering Recommender Systems},
  author = {Herlocker, Jonathan L. and Konstan, Joseph A. and Terveen, Loren G. and Riedl, John T.},
  date = {2004-01},
  journaltitle = {ACM Trans. Inf. Syst.},
  volume = {22},
  number = {1},
  pages = {5--53},
  issn = {1046-8188, 1558-2868},
  doi = {10.1145/963770.963772},
  url = {https://dl.acm.org/doi/10.1145/963770.963772},
  urldate = {2021-07-28},
  abstract = {Recommender systems have been evaluated in many, often incomparable, ways. In this article, we review the key decisions in evaluating collaborative filtering recommender systems: the user tasks being evaluated, the types of analysis and datasets being used, the ways in which prediction quality is measured, the evaluation of prediction attributes other than quality, and the user-based evaluation of the system as a whole. In addition to reviewing the evaluation strategies used by prior researchers, we present empirical results from the analysis of various accuracy metrics on one content domain where all the tested metrics collapsed roughly into three equivalence classes. Metrics within each equivalency class were strongly correlated, while metrics from different equivalency classes were uncorrelated.},
  langid = {english},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\VAXD7S6D\\Herlocker et al. - 2004 - Evaluating collaborative filtering recommender sys.pdf}
}

@article{herm-stapelbergCrowdFewMeasuring2020,
  title = {The Crowd against the Few: {{Measuring}} the Impact of Expert Recommendations},
  shorttitle = {The Crowd against the Few},
  author = {Herm-Stapelberg, Nils and Rothlauf, Franz},
  date = {2020-11},
  journaltitle = {Decision Support Systems},
  volume = {138},
  pages = {113345},
  issn = {01679236},
  doi = {10.1016/j.dss.2020.113345},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0167923620301007},
  urldate = {2021-07-28},
  abstract = {A large amount of research on recommender systems has focused on improving the accuracy of suggestions in offline settings. However, this focus and the commonly used techniques can lead to a “filter bubble”, severely limiting the diversity of content discovered by users. Several offline studies show that this can be mitigated by using experts for recommendation. In contrast to standard recommender systems, experts are able to generate more diverse recommendations and increase the novelty of given suggestions. They can be used in missing-data or cold-start scenarios and reduce noise in the users' ratings. This paper examines the impact of employed experts' recommendations on user behavior for a real-world recommender system on a popular video-on-demand website, provided by a large television network. We study whether the potential benefits of experts lead to differences in user behavior, user perceptions and properties of given recommendations (e.g., diversity). We find that enriching a state-of-the-art system with the suggestions of employed experts can significantly increase platform use. Even though expert recommendations are used less frequently and are less successful than ex­ pected, users watch a greater number of clips, use more recommendations, and come back to the website more frequently when they receive expert suggestions. When searching for other influencing factors, we find that experts generate more diverse recommendations and improve the taste coverage of the system keeping user satisfaction unaffected. In summary, our results show large benefits of using employed experts and have im­ plications for the design and use of recommender systems in real-world scenarios.},
  langid = {english},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\VUEH3IQ2\\Herm-Stapelberg and Rothlauf - 2020 - The crowd against the few Measuring the impact of.pdf}
}

@book{hoffmannSimulationNeuronalerNetze1992,
  title = {Simulation Neuronaler Netze},
  author = {Hoffmann, Norbert},
  date = {1992},
  publisher = {{Vieweg+Teubner Verlag}},
  location = {{Wiesbaden}},
  doi = {10.1007/978-3-322-83200-9},
  url = {http://link.springer.com/10.1007/978-3-322-83200-9},
  urldate = {2021-12-23},
  isbn = {978-3-322-83201-6 978-3-322-83200-9},
  langid = {german}
}

@inproceedings{huCollaborativeFilteringImplicit2008,
  title = {Collaborative {{Filtering}} for {{Implicit Feedback Datasets}}},
  booktitle = {2008 {{Eighth IEEE International Conference}} on {{Data Mining}}},
  author = {Hu, Yifan and Koren, Yehuda and Volinsky, Chris},
  date = {2008-12},
  pages = {263--272},
  publisher = {{IEEE}},
  location = {{Pisa, Italy}},
  doi = {10.1109/ICDM.2008.22},
  url = {http://ieeexplore.ieee.org/document/4781121/},
  urldate = {2021-08-08},
  abstract = {A common task of recommender systems is to improve customer experience through personalized recommendations based on prior implicit feedback. These systems passively track different sorts of user behavior, such as purchase history, watching habits and browsing activity, in order to model user preferences. Unlike the much more extensively researched explicit feedback, we do not have any direct input from the users regarding their preferences. In particular, we lack substantial evidence on which products consumer dislike. In this work we identify unique properties of implicit feedback datasets. We propose treating the data as indication of positive and negative preference associated with vastly varying confidence levels. This leads to a factor model which is especially tailored for implicit feedback recommenders. We also suggest a scalable optimization procedure, which scales linearly with the data size. The algorithm is used successfully within a recommender system for television shows. It compares favorably with well tuned implementations of other known methods. In addition, we offer a novel way to give explanations to recommendations given by this factor model.},
  eventtitle = {2008 {{Eighth IEEE International Conference}} on {{Data Mining}} ({{ICDM}})},
  isbn = {978-0-7695-3502-9},
  langid = {english},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\QSM5HAAF\\Hu et al. - 2008 - Collaborative Filtering for Implicit Feedback Data.pdf}
}

@article{hugSurprisePythonLibrary2020,
  title = {Surprise: {{A Python}} Library for Recommender Systems},
  shorttitle = {Surprise},
  author = {Hug, Nicolas},
  date = {2020-08-05},
  journaltitle = {JOSS},
  volume = {5},
  number = {52},
  pages = {2174},
  issn = {2475-9066},
  doi = {10.21105/joss.02174},
  url = {https://joss.theoj.org/papers/10.21105/joss.02174},
  urldate = {2021-07-28},
  abstract = {Recommender systems aim at providing users with a list of recommendations of items that a service offers. For example, a video streaming service will typically rely on a recommender system to propose a personalized list of movies or series to each of its users. A typical problem in recommendation is that of rating prediction: given an incomplete dataset of useritem interations which take the form of numerical ratings (e.g. on a scale from 1 to 5), the goal is to predict the missing ratings for all remaining user-item pairs.},
  langid = {english},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\AFS9C46L\\Hug - 2020 - Surprise A Python library for recommender systems.pdf}
}

@article{hugSurprisePythonLibrary2020a,
  title = {Surprise: {{A Python}} Library for Recommender Systems},
  shorttitle = {Surprise},
  author = {Hug, Nicolas},
  date = {2020-08-05},
  journaltitle = {Journal of Open Source Software},
  volume = {5},
  number = {52},
  pages = {2174},
  issn = {2475-9066},
  doi = {10.21105/joss.02174},
  url = {https://joss.theoj.org/papers/10.21105/joss.02174},
  urldate = {2022-01-18},
  abstract = {Hug, N., (2020). Surprise: A Python library for recommender systems. Journal of Open Source Software, 5(52), 2174, https://doi.org/10.21105/joss.02174},
  langid = {english},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\RLVJDQ6A\\Hug - 2020 - Surprise A Python library for recommender systems.pdf;C\:\\Users\\Romanos\\Zotero\\storage\\I3WJ4CNE\\joss.html}
}

@article{idrissiSystematicLiteratureReview2020,
  title = {A Systematic Literature Review of Sparsity Issues in Recommender Systems},
  author = {Idrissi, Nouhaila and Zellou, Ahmed},
  date = {2020-02-11},
  journaltitle = {Soc. Netw. Anal. Min.},
  volume = {10},
  number = {1},
  pages = {15},
  issn = {1869-5469},
  doi = {10.1007/s13278-020-0626-2},
  url = {https://doi.org/10.1007/s13278-020-0626-2},
  urldate = {2022-01-29},
  abstract = {The tremendous expansion of information available on the web voraciously bombards users, leaving them unable to make decisions and having no way of stepping back to process it all. Recommender systems have emerged in this context as a solution to assist users by providing them with choices of appropriate and relevant items according to their preferences and interests. However, despite their success in many fields and application domains, they still suffer from the main limitation, known as the sparsity problem. The latter refers to the situation where insufficient transactional and feedback data are available for inferring specific user’s similarities, which affects the accuracy and performance of the recommender system. This paper provides a systematic literature review to investigate, analyze, and discuss the existing relevant contributions and efforts that use new concepts and tools to alleviate the sparsity issues. We have investigated the contributed similarity measures and have uncovered proposed approaches in different types of recommender systems. We have also identified the types of side information more commonly employed by recommender systems. Furthermore, we have examined the criteria that should be valued to enhance recommendation accuracy on sparse data. Each selected article was evaluated for its ability to mitigate the sparsity impediment. Our findings emphasize and accentuate the importance of sparsity in recommender systems and provide researchers and practitioners with insights on proposed solutions and their limitations, which contributes to the development of more powerful systems that can significantly solve the sparsity hurdle and thus enhance further the accuracy and efficiency of recommendations.},
  langid = {english},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\SYGMHJK9\\Idrissi and Zellou - 2020 - A systematic literature review of sparsity issues .pdf}
}

@article{jadidinejadHowSensitiveRecommendation,
  title = {How {{Sensitive}} Is {{Recommendation Systems}}’ {{Offline Evaluation}} to {{Popularity}}?},
  author = {Jadidinejad, Amir H and Macdonald, Craig and Ounis, Iadh},
  pages = {4},
  abstract = {Datasets used for the offline evaluation of recommender systems are collected through user interactions with an already deployed recommender system. However, such datasets can be subject to different types of biases including a system’s popularity bias. In this paper, we focus on assessing the influence of popularity on the offline evaluation of recommendation systems. Our insights from a deeper analysis based on popularity-stratified sampling reveal that the current offline evaluation of recommendation systems are sensitive to popular items, raising questions about conclusions driven from the offline comparison of recommendation models.},
  langid = {english},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\PTF68QUG\\Jadidinejad et al. - How Sensitive is Recommendation Systems’ Offline E.pdf}
}

@inproceedings{jannachRecommendationsPurpose2016,
  title = {Recommendations with a {{Purpose}}},
  booktitle = {Proceedings of the 10th {{ACM Conference}} on {{Recommender Systems}}},
  author = {Jannach, Dietmar and Adomavicius, Gediminas},
  date = {2016-09-07},
  pages = {7--10},
  publisher = {{ACM}},
  location = {{Boston Massachusetts USA}},
  doi = {10.1145/2959100.2959186},
  url = {https://dl.acm.org/doi/10.1145/2959100.2959186},
  urldate = {2021-07-28},
  abstract = {The purpose of recommenders is often summarized as “help the users find relevant items”, and the predominant operationalization of this goal has been to focus on the ability to numerically estimate the users’ preferences for unseen items or to provide users with item lists ranked in accordance to the estimated preferences. This dominant, albeit narrow, view of the recommendation problem has been tremendously helpful in advancing research in different ways, e.g., through the establishment of standardized evaluation procedures and metrics. In reality, recommender systems can serve a variety of purposes from the point of view of both consumers and providers. Most of the purposes, however, are significantly underexplored, even though many of them are arguably more aligned with the real-world expectations for recommenders than our current predominant paradigm. Therefore, it is important to revisit our conceptualizations of the potential goals of recommenders and their operationalization as research problems. In this paper, we discuss a framework of recommendation goals and purposes and highlight possible future directions and challenges related to the operationalization of such alternative problem formulations.},
  eventtitle = {{{RecSys}} '16: {{Tenth ACM Conference}} on {{Recommender Systems}}},
  isbn = {978-1-4503-4035-9},
  langid = {english},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\35LBHA8L\\Jannach and Adomavicius - 2016 - Recommendations with a Purpose.pdf}
}

@article{jannachWhatRecommendersRecommend2015,
  title = {What Recommenders Recommend: An Analysis of Recommendation Biases and Possible Countermeasures},
  shorttitle = {What Recommenders Recommend},
  author = {Jannach, Dietmar and Lerche, Lukas and Kamehkhosh, Iman and Jugovac, Michael},
  date = {2015-12},
  journaltitle = {User Model User-Adap Inter},
  volume = {25},
  number = {5},
  pages = {427--491},
  issn = {0924-1868, 1573-1391},
  doi = {10.1007/s11257-015-9165-3},
  url = {http://link.springer.com/10.1007/s11257-015-9165-3},
  urldate = {2021-07-28},
  langid = {english},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\HTFTVEJ6\\Jannach et al. - 2015 - What recommenders recommend an analysis of recomm.pdf}
}

@article{jarvelinCumulatedGainbasedEvaluation2002,
  title = {Cumulated Gain-Based Evaluation of {{IR}} Techniques},
  author = {Järvelin, Kalervo and Kekäläinen, Jaana},
  date = {2002-10},
  journaltitle = {ACM Trans. Inf. Syst.},
  volume = {20},
  number = {4},
  pages = {422--446},
  issn = {1046-8188, 1558-2868},
  doi = {10.1145/582415.582418},
  url = {https://dl.acm.org/doi/10.1145/582415.582418},
  urldate = {2021-12-21},
  abstract = {Modern large retrieval environments tend to overwhelm their users by their large output. Since all documents are not of equal relevance to their users, highly relevant documents should be identified and ranked first for presentation. In order to develop IR techniques in this direction, it is necessary to develop evaluation approaches and methods that credit IR methods for their ability to retrieve highly relevant documents. This can be done by extending traditional evaluation methods, that is, recall and precision based on binary relevance judgments, to graded relevance judgments. Alternatively, novel measures based on graded relevance judgments may be developed. This article proposes several novel measures that compute the cumulative gain the user obtains by examining the retrieval result up to a given ranked position. The first one accumulates the relevance scores of retrieved documents along the ranked result list. The second one is similar but applies a discount factor to the relevance scores in order to devaluate late-retrieved documents. The third one computes the relative-to-the-ideal performance of IR techniques, based on the cumulative gain they are able to yield. These novel measures are defined and discussed and their use is demonstrated in a case study using TREC data: sample system run results for 20 queries in TREC-7. As a relevance base we used novel graded relevance judgments on a four-point scale. The test results indicate that the proposed measures credit IR methods for their ability to retrieve highly relevant documents and allow testing of statistical significance of effectiveness differences. The graphs based on the measures also provide insight into the performance IR techniques and allow interpretation, for example, from the user point of view.},
  langid = {english},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\HZB4TF38\\Järvelin and Kekäläinen - 2002 - Cumulated gain-based evaluation of IR techniques.pdf}
}

@article{jarvelinIREvaluationMethods2000,
  title = {{{IR}} Evaluation Methods for Retrieving Highly Relevant Documents},
  author = {Järvelin, Kalervo and Kekäläinen, Jaana},
  date = {2000},
  journaltitle = {ACM SIGIR Forum},
  volume = {51},
  number = {2},
  pages = {8},
  abstract = {This paper proposes evaluation methods based on the use of non-dichotomous relevance judgements in IR experiments. It is argued that evaluation methods should credit IR methods for their ability to retrieve highly relevant documents. This is desirable from the user point of view in modem large IR environments. The proposed methods are (1) a novel application of P-R curves and average precision computations based on separate recall bases for documents of different degrees of relevance, and (2) two novel measures computing the cumulative gain the user obtains by examining the retrieval result up to a given ranked position. We then demonstrate the use of these evaluation methods in a case study on the effectiveness of query types, based on combinations of query structures and expansion, in retrieving documents of various degrees of relevance. The test was run with a best match retrieval system (InQuery I) in a text database consisting of newspaper articles. The results indicate that the tested strong query structures are most effective in retrieving highly relevant documents. The differences between the query types are practically essential and statistically significant. More generally, the novel evaluation methods and the case demonstrate that non-dichotomous relevance assessments are applicable in IR experiments, may reveal interesting phenomena, and allow harder testing of IR methods.},
  langid = {english},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\6JYLUEJJ\\Jirvelin and Kekiiliinen - 2017 - IR evaluation methods for retrieving highly releva.pdf}
}

@inproceedings{jawaheerComparisonImplicitExplicit2010,
  title = {Comparison of Implicit and Explicit Feedback from an Online Music Recommendation Service},
  booktitle = {Proceedings of the 1st {{International Workshop}} on {{Information Heterogeneity}} and {{Fusion}} in {{Recommender Systems}} - {{HetRec}} '10},
  author = {Jawaheer, Gawesh and Szomszor, Martin and Kostkova, Patty},
  date = {2010},
  pages = {47--51},
  publisher = {{ACM Press}},
  location = {{Barcelona, Spain}},
  doi = {10.1145/1869446.1869453},
  url = {http://portal.acm.org/citation.cfm?doid=1869446.1869453},
  urldate = {2021-07-28},
  abstract = {Explicit and implicit feedback exhibits different characteristics of users’ preferences with both pros and cons. However, a combination of these two types of feedback provides another paradigm for recommender systems (RS). Their combination in a user preference model presents a number of challenges but can also overcome the problems associated with each other. In order to build an effective RS on combination of both types of feedback, we need to have comparative data allowing an understanding of the computation of user preferences. In this paper, we provide an overview of the differentiating characteristics of explicit and implicit feedback using datasets mined from Last.fm, an online music station and recommender service. The datasets consisted of explicit positive feedback (by loving tracks) and implicit feedback which is inherently positive (the number of times a track is played). Rather than relying on just one type of feedback, we present techniques for extracting user preferences from both. In order to compare and contrast the performances of these techniques, we carried out experiments using the Taste recommender system engine and the Last.fm datasets. Our results show that implicit and explicit positive feedback complements each other, with similar performances despite their different characteristics.},
  eventtitle = {The 1st {{International Workshop}}},
  isbn = {978-1-4503-0407-8},
  langid = {english},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\VP8MJIEZ\\Jawaheer et al. - 2010 - Comparison of implicit and explicit feedback from .pdf}
}

@article{kaelblingReinforcementLearningSurvey1996,
  title = {Reinforcement {{Learning}}: {{A Survey}}},
  shorttitle = {Reinforcement {{Learning}}},
  author = {Kaelbling, L. P. and Littman, M. L. and Moore, A. W.},
  date = {1996-05-01},
  journaltitle = {Journal of Artificial Intelligence Research},
  volume = {4},
  pages = {237--285},
  issn = {1076-9757},
  doi = {10.1613/jair.301},
  url = {https://www.jair.org/index.php/jair/article/view/10166},
  urldate = {2022-02-02},
  abstract = {This paper surveys the field of reinforcement learning from    a computer-science perspective. It is written to be accessible to    researchers familiar with machine learning.  Both the historical basis    of the field and a broad selection of current work are summarized.    Reinforcement learning is the problem faced by an agent that learns    behavior through trial-and-error interactions with a dynamic    environment.  The work described here has a resemblance to work in    psychology, but differs considerably in the details and in the use of    the word ``reinforcement.''  The paper discusses central issues of    reinforcement learning, including trading off exploration and    exploitation, establishing the foundations of the field via Markov    decision theory, learning from delayed reinforcement, constructing    empirical models to accelerate learning, making use of generalization    and hierarchy, and coping with hidden state.  It concludes with a    survey of some implemented systems and an assessment of the practical    utility of current methods for reinforcement learning.},
  langid = {english},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\P3WL4P7C\\Kaelbling et al. - 1996 - Reinforcement Learning A Survey.pdf}
}

@article{kamishimaRecommendationIndependence,
  title = {Recommendation {{Independence}}},
  author = {Kamishima, Toshihiro and Akaho, Shotaro and Asoh, Hideki and Sakuma, Jun},
  pages = {15},
  abstract = {This paper studies a recommendation algorithm whose outcomes are not influenced by specified information. It is useful in contexts potentially unfair decision should be avoided, such as job-applicant recommendations that are not influenced by socially sensitive information. An algorithm that could exclude the influence of sensitive information would thus be useful for job-matching with fairness. We call the condition between a recommendation outcome and a sensitive feature Recommendation Independence, which is formally defined as statistical independence between the outcome and the feature. Our previous independence-enhanced algorithms simply matched the means of predictions between sub-datasets consisting of the same sensitive value. However, this approach could not remove the sensitive information represented by the second or higher moments of distributions. In this paper, we develop new methods that can deal with the second moment, i.e., variance, of recommendation outcomes without increasing the computational complexity. These methods can more strictly remove the sensitive information, and experimental results demonstrate that our new algorithms can more effectively eliminate the factors that undermine fairness. Additionally, we explore potential applications for independenceenhanced recommendation, and discuss its relation to other concepts, such as recommendation diversity.},
  langid = {english},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\Z9FDE8QU\\Kamishima et al. - Recommendation Independence.pdf}
}

@inproceedings{kamishimaRecommendationIndependence2018,
  title = {Recommendation {{Independence}}},
  booktitle = {Proceedings of the 1st {{Conference}} on {{Fairness}}, {{Accountability}} and {{Transparency}}},
  author = {Kamishima, Toshihiro and Akaho, Shotaro and Asoh, Hideki and Sakuma, Jun},
  date = {2018-01-21},
  pages = {187--201},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v81/kamishima18a.html},
  urldate = {2022-02-22},
  abstract = {This paper studies a recommendation algorithm whose outcomes are not influenced by specified information. It is useful in contexts potentially unfair decision should be avoided, such as job-applicant recommendations that are not influenced by socially sensitive information. An algorithm that could exclude the influence of sensitive information would thus be useful for job-matching with fairness. We call the condition between a recommendation outcome and a sensitive feature Recommendation Independence, which is formally defined as statistical independence between the outcome and the feature. Our previous independence-enhanced algorithms simply matched the means of predictions between sub-datasets consisting of the same sensitive value. However, this approach could not remove the sensitive information represented by the second or higher moments of distributions. In this paper, we develop new methods that can deal with the second moment, i.e., variance, of recommendation outcomes without increasing the computational complexity. These methods can more strictly remove the sensitive information, and experimental results demonstrate that our new algorithms can more effectively eliminate the factors that undermine fairness. Additionally, we explore potential applications for independence-enhanced recommendation, and discuss its relation to other concepts, such as recommendation diversity.},
  eventtitle = {Conference on {{Fairness}}, {{Accountability}} and {{Transparency}}},
  langid = {english},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\I7IDUL2J\\Kamishima et al. - 2018 - Recommendation Independence.pdf;C\:\\Users\\Romanos\\Zotero\\storage\\ZU4BRJME\\Kamishima et al. - 2018 - Recommendation Independence.pdf}
}

@article{kapsalisBiasFairnessRecommender,
  title = {Bias and Fairness in  Recommender Systems},
  author = {Kapsalis, Romanos},
  pages = {10},
  langid = {english},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\6ZC3FPI8\\Kapsalis - Bias and fairness in  recommender systems.pdf}
}

@article{kearnsPreventingFairnessGerrymandering,
  title = {Preventing {{Fairness Gerrymandering}}:{{Auditing}} and {{Learning}} for {{Subgroup Fairness}}},
  author = {Kearns, Michael and Neel, Seth and Roth, Aaron and Wu, Zhiwei Steven},
  pages = {9},
  abstract = {We introduce a new family of fairness definitions that interpolate between statistical and individual notions of fairness, obtaining some of the best properties of each. We show that checking whether these notions are satisfied is computationally hard in the worst case, but give practical oracle-efficient algorithms for learning subject to these constraints, and confirm our findings with experiments.},
  langid = {english},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\PRIIDRGY\\Kearns et al. - Preventing Fairness GerrymanderingAuditing and Le.pdf}
}

@inproceedings{kearnsPreventingFairnessGerrymandering2018,
  title = {Preventing {{Fairness Gerrymandering}}: {{Auditing}} and {{Learning}} for {{Subgroup Fairness}}},
  shorttitle = {Preventing {{Fairness Gerrymandering}}},
  booktitle = {Proceedings of the 35th {{International Conference}} on {{Machine Learning}}},
  author = {Kearns, Michael and Neel, Seth and Roth, Aaron and Wu, Zhiwei Steven},
  date = {2018-07-03},
  pages = {2564--2572},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v80/kearns18a.html},
  urldate = {2022-02-22},
  abstract = {The most prevalent notions of fairness in machine learning fix a small collection of pre-defined groups (such as race or gender), and then ask for approximate parity of some statistic of the classifier (such as false positive rate) across these groups. Constraints of this form are susceptible to fairness gerrymandering, in which a classifier is fair on each individual group, but badly violates the fairness constraint on structured subgroups, such as certain combinations of protected attribute values. We thus consider fairness across exponentially or infinitely many subgroups, defined by a structured class of functions over the protected attributes. We first prove that the problem of auditing subgroup fairness for both equality of false positive rates and statistical parity is computationally equivalent to the problem of weak agnostic learning — which means it is hard in the worst case, even for simple structured subclasses. However, it also suggests that common heuristics for learning can be applied to successfully solve the auditing problem in practice. We then derive an algorithm that provably converges in a polynomial number of steps to the best subgroup-fair distribution over classifiers, given access to an oracle which can solve the agnostic learning problem. The algorithm is based on a formulation of subgroup fairness as a zero-sum game between a Learner (the primal player) and an Auditor (the dual player). We implement a variant of this algorithm using heuristic oracles, and show that we can effectively both audit and learn fair classifiers on a real dataset.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\8CNHQDI9\\Kearns et al. - 2018 - Preventing Fairness Gerrymandering Auditing and L.pdf}
}

@inproceedings{keskarLargebatchTrainingDeep2017,
  title = {On Large-Batch Training for Deep Learning: {{Generalization}} Gap and Sharp Minima},
  shorttitle = {On Large-Batch Training for Deep Learning},
  author = {Keskar, Nitish Shirish and Nocedal, Jorge and Tang, Ping Tak Peter and Mudigere, Dheevatsa and Smelyanskiy, Mikhail},
  date = {2017},
  location = {{Toulon, France.}},
  url = {http://www.scopus.com/inward/record.url?scp=85084433203&partnerID=8YFLogxK},
  urldate = {2022-02-22},
  abstract = {The stochastic gradient descent (SGD) method and its variants are algorithms of choice for many Deep Learning tasks. These methods operate in a small-batch regime wherein a fraction of the training data, say 32-512 data points, is sampled to compute an approximation to the gradient. It has been observed in practice that when using a larger batch there is a degradation in the quality of the model, as measured by its ability to generalize. We investigate the cause for this generalization drop in the large-batch regime and present numerical evidence that supports the view that large-batch methods tend to converge to sharp minimizers of the training and testing functions-and as is well known, sharp minima lead to poorer generalization. In contrast, small-batch methods consistently converge to flat minimizers, and our experiments support a commonly held view that this is due to the inherent noise in the gradient estimation. We discuss several strategies to attempt to help large-batch methods eliminate this generalization gap.},
  eventtitle = {5th {{International Conference}} on {{Learning Representations}}, {{ICLR}} 2017}
}

@online{khanCostSensitiveLearning2017,
  title = {Cost {{Sensitive Learning}} of {{Deep Feature Representations}} from {{Imbalanced Data}}},
  author = {Khan, Salman H. and Hayat, Munawar and Bennamoun, Mohammed and Sohel, Ferdous and Togneri, Roberto},
  date = {2017-03-23},
  eprint = {1508.03422},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1508.03422},
  urldate = {2021-07-29},
  abstract = {Class imbalance is a common problem in the case of real-world object detection and classification tasks. Data of some classes is abundant making them an over-represented majority, and data of other classes is scarce, making them an underrepresented minority. This imbalance makes it challenging for a classifier to appropriately learn the discriminating boundaries of the majority and minority classes. In this work, we propose a cost-sensitive deep neural network which can automatically learn robust feature representations for both the majority and minority classes. During training, our learning procedure jointly optimizes the class-dependent costs and the neural network parameters. The proposed approach is applicable to both binary and multiclass problems without any modification. Moreover, as opposed to data level approaches, we do not alter the original data distribution which results in a lower computational cost during the training process. We report the results of our experiments on six major image classification datasets and show that the proposed approach significantly outperforms the baseline algorithms. Comparisons with popular data sampling techniques and cost-sensitive classifiers demonstrate the superior performance of our proposed method.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\8FK7BRN7\\Khan et al. - 2017 - Cost Sensitive Learning of Deep Feature Representa.pdf}
}

@article{kimMachineLearningTechniques,
  title = {Machine {{Learning Techniques}} for {{Accountability}}},
  author = {Kim, Been and Doshi-Velez, Finale},
  pages = {6},
  langid = {english},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\NUGTPED6\\Kim and Doshi-Velez - Machine Learning Techniques for Accountability.pdf}
}

@inproceedings{kingmaAdamMethodStochastic2015,
  title = {Adam: {{A Method}} for {{Stochastic Optimization}}},
  shorttitle = {Adam},
  booktitle = {3rd {{International Conference}} on {{Learning Representations}}, {{ICLR}} 2015, {{San Diego}}, {{CA}}, {{USA}}, {{May}} 7-9, 2015, {{Conference Track Proceedings}}},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  editor = {Bengio, Yoshua and LeCun, Yann},
  date = {2015},
  url = {http://arxiv.org/abs/1412.6980},
  urldate = {2022-02-22},
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.}
}

@online{kingmaAdamMethodStochastic2017,
  title = {Adam: {{A Method}} for {{Stochastic Optimization}}},
  shorttitle = {Adam},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  date = {2017-01-29},
  eprint = {1412.6980},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1412.6980},
  urldate = {2021-12-27},
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\5FFS3H9X\\Kingma and Ba - 2017 - Adam A Method for Stochastic Optimization.pdf}
}

@inproceedings{korenFactorizationMeetsNeighborhood2008,
  title = {Factorization Meets the Neighborhood: A Multifaceted Collaborative Filtering Model},
  shorttitle = {Factorization Meets the Neighborhood},
  booktitle = {Proceedings of the 14th {{ACM SIGKDD}} International Conference on {{Knowledge}} Discovery and Data Mining},
  author = {Koren, Yehuda},
  date = {2008-08-24},
  series = {{{KDD}} '08},
  pages = {426--434},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/1401890.1401944},
  url = {https://doi.org/10.1145/1401890.1401944},
  urldate = {2021-08-10},
  abstract = {Recommender systems provide users with personalized suggestions for products or services. These systems often rely on Collaborating Filtering (CF), where past transactions are analyzed in order to establish connections between users and products. The two more successful approaches to CF are latent factor models, which directly profile both users and products, and neighborhood models, which analyze similarities between products or users. In this work we introduce some innovations to both approaches. The factor and neighborhood models can now be smoothly merged, thereby building a more accurate combined model. Further accuracy improvements are achieved by extending the models to exploit both explicit and implicit feedback by the users. The methods are tested on the Netflix data. Results are better than those previously published on that dataset. In addition, we suggest a new evaluation metric, which highlights the differences among methods, based on their performance at a top-K recommendation task.},
  isbn = {978-1-60558-193-4},
  keywords = {collaborative filtering,recommender systems},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\86MVIZJQ\\Koren - 2008 - Factorization meets the neighborhood a multifacet.pdf}
}

@article{korenMatrixFactorizationTechniques2009,
  title = {Matrix {{Factorization Techniques}} for {{Recommender Systems}}},
  author = {Koren, Yehuda and Bell, Robert and Volinsky, Chris},
  date = {2009-08},
  journaltitle = {Computer},
  volume = {42},
  number = {8},
  pages = {30--37},
  issn = {0018-9162},
  doi = {10.1109/MC.2009.263},
  url = {http://ieeexplore.ieee.org/document/5197422/},
  urldate = {2021-07-28},
  langid = {english},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\ACX4B5H3\\Koren et al. - 2009 - Matrix Factorization Techniques for Recommender Sy.pdf}
}

@online{kowaldUnfairnessPopularityBias2019,
  title = {The {{Unfairness}} of {{Popularity Bias}} in {{Music Recommendation}}: {{A Reproducibility Study}}},
  shorttitle = {The {{Unfairness}} of {{Popularity Bias}} in {{Music Recommendation}}},
  author = {Kowald, Dominik and Schedl, Markus and Lex, Elisabeth},
  date = {2019-12-19},
  eprint = {1912.04696},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1912.04696},
  urldate = {2021-07-29},
  abstract = {Research has shown that recommender systems are typically biased towards popular items, which leads to less popular items being underrepresented in recommendations. The recent work of Abdollahpouri et al. in the context of movie recommendations has shown that this popularity bias leads to unfair treatment of both long-tail items as well as users with little interest in popular items. In this paper, we reproduce the analyses of Abdollahpouri et al. in the context of music recommendation. Specifically, we investigate three user groups from the Last.fm music platform that are categorized based on how much their listening preferences deviate from the most popular music among all Last.fm users in the dataset: (i) low-mainstream users, (ii) medium-mainstream users, and (iii) high-mainstream users. In line with Abdollahpouri et al., we find that state-of-the-art recommendation algorithms favor popular items also in the music domain. However, their proposed Group Average Popularity metric yields different results for Last.fm than for the movie domain, presumably due to the larger number of available items (i.e., music artists) in the Last.fm dataset we use. Finally, we compare the accuracy results of the recommendation algorithms for the three user groups and find that the low-mainstreaminess group significantly receives the worst recommendations.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Information Retrieval,Computer Science - Machine Learning,Computer Science - Social and Information Networks},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\DY6PM5U7\\Kowald et al. - 2019 - The Unfairness of Popularity Bias in Music Recomme.pdf}
}

@article{krollAccountableAlgorithms,
  title = {Accountable {{Algorithms}}},
  author = {Kroll, Joshua A and Huey, Joanna and Barocas, Solon and Felten, Edward W and Reidenberg, Joel R and Robinson, David G and Yu, Harlan},
  journaltitle = {University of Pennsylvania Law Review},
  volume = {165},
  pages = {74},
  langid = {english},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\B6PW3BCP\\Kroll et al. - Accountable Algorithms.pdf}
}

@article{kusnerCounterfactualFairness2018,
  title = {Counterfactual {{Fairness}}},
  author = {Kusner, Matt J. and Loftus, Joshua R. and Russell, Chris and Silva, Ricardo},
  date = {2018-03-08},
  journaltitle = {Advances in neural information processing systems},
  volume = {30},
  eprint = {1703.06856},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1703.06856},
  urldate = {2021-08-17},
  abstract = {Machine learning can impact people with legal or ethical consequences when it is used to automate decisions in areas such as insurance, lending, hiring, and predictive policing. In many of these scenarios, previous decisions have been made that are unfairly biased against certain subpopulations, for example those of a particular race, gender, or sexual orientation. Since this past data may be biased, machine learning predictors must account for this to avoid perpetuating or creating discriminatory practices. In this paper, we develop a framework for modeling fairness using tools from causal inference. Our definition of counterfactual fairness captures the intuition that a decision is fair towards an individual if it is the same in (a) the actual world and (b) a counterfactual world where the individual belonged to a different demographic group. We demonstrate our framework on a real-world problem of fair prediction of success in law school.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computers and Society,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\MYNGZ7E3\\Kusner et al. - 2018 - Counterfactual Fairness.pdf}
}

@article{lepriFairTransparentAccountable2018,
  title = {Fair, {{Transparent}}, and {{Accountable Algorithmic Decision-making Processes}}: {{The Premise}}, the {{Proposed Solutions}}, and the {{Open Challenges}}},
  shorttitle = {Fair, {{Transparent}}, and {{Accountable Algorithmic Decision-making Processes}}},
  author = {Lepri, Bruno and Oliver, Nuria and Letouzé, Emmanuel and Pentland, Alex and Vinck, Patrick},
  date = {2018-12},
  journaltitle = {Philos. Technol.},
  volume = {31},
  number = {4},
  pages = {611--627},
  issn = {2210-5433, 2210-5441},
  doi = {10.1007/s13347-017-0279-x},
  url = {http://link.springer.com/10.1007/s13347-017-0279-x},
  urldate = {2022-01-30},
  langid = {english},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\KBPSMSJL\\Lepri et al. - 2018 - Fair, Transparent, and Accountable Algorithmic Dec.pdf}
}

@online{liangVariationalAutoencodersCollaborative2018,
  title = {Variational {{Autoencoders}} for {{Collaborative Filtering}}},
  author = {Liang, Dawen and Krishnan, Rahul G. and Hoffman, Matthew D. and Jebara, Tony},
  date = {2018-02-15},
  eprint = {1802.05814},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1802.05814},
  urldate = {2022-01-22},
  abstract = {We extend variational autoencoders (vaes) to collaborative filtering for implicit feedback. This non-linear probabilistic model enables us to go beyond the limited modeling capacity of linear factor models which still largely dominate collaborative filtering research. We introduce a generative model with multinomial likelihood and use Bayesian inference for parameter estimation. Despite widespread use in language modeling and economics, the multinomial likelihood receives less attention in the recommender systems literature. We introduce a different regularization parameter for the learning objective, which proves to be crucial for achieving competitive performance. Remarkably, there is an efficient way to tune the parameter using annealing. The resulting model and learning algorithm has information-theoretic connections to maximum entropy discrimination and the information bottleneck principle. Empirically, we show that the proposed approach significantly outperforms several state-of-the-art baselines, including two recently-proposed neural network approaches, on several real-world datasets. We also provide extended experiments comparing the multinomial likelihood with other commonly used likelihood functions in the latent factor collaborative filtering literature and show favorable results. Finally, we identify the pros and cons of employing a principled Bayesian inference approach and characterize settings where it provides the most significant improvements.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Information Retrieval,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\TX4JURNY\\Liang et al. - 2018 - Variational Autoencoders for Collaborative Filteri.pdf}
}

@inproceedings{liangVariationalAutoencodersCollaborative2018a,
  title = {Variational {{Autoencoders}} for {{Collaborative Filtering}}},
  booktitle = {Proceedings of the 2018 {{World Wide Web Conference}}},
  author = {Liang, Dawen and Krishnan, Rahul G. and Hoffman, Matthew D. and Jebara, Tony},
  date = {2018-04-23},
  series = {{{WWW}} '18},
  pages = {689--698},
  publisher = {{International World Wide Web Conferences Steering Committee}},
  location = {{Republic and Canton of Geneva, CHE}},
  doi = {10.1145/3178876.3186150},
  url = {https://doi.org/10.1145/3178876.3186150},
  urldate = {2022-02-22},
  abstract = {We extend variational autoencoders (VAEs) to collaborative filtering for implicit feedback. This non-linear probabilistic model enables us to go beyond the limited modeling capacity of linear factor models which still largely dominate collaborative filtering research.We introduce a generative model with multinomial likelihood and use Bayesian inference for parameter estimation. Despite widespread use in language modeling and economics, the multinomial likelihood receives less attention in the recommender systems literature. We introduce a different regularization parameter for the learning objective, which proves to be crucial for achieving competitive performance. Remarkably, there is an efficient way to tune the parameter using annealing. The resulting model and learning algorithm has information-theoretic connections to maximum entropy discrimination and the information bottleneck principle. Empirically, we show that the proposed approach significantly outperforms several state-of-the-art baselines, including two recently-proposed neural network approaches, on several real-world datasets. We also provide extended experiments comparing the multinomial likelihood with other commonly used likelihood functions in the latent factor collaborative filtering literature and show favorable results. Finally, we identify the pros and cons of employing a principled Bayesian inference approach and characterize settings where it provides the most significant improvements.},
  isbn = {978-1-4503-5639-8},
  keywords = {bayesian models,collaborative filtering,implicit feedback,recommender systems,variational autoencoder},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\IUPESEX8\\Liang et al. - 2018 - Variational Autoencoders for Collaborative Filteri.pdf}
}

@inproceedings{liCIKM2021Tutorial2021,
  title = {{{CIKM}} 2021 {{Tutorial}} on {{Fairness}} of {{Machine Learning}} in {{Recommender Systems}}},
  booktitle = {Proceedings of the 30th {{ACM International Conference}} on {{Information}} \& {{Knowledge Management}}},
  author = {Li, Yunqi and Ge, Yingqiang and Zhang, Yongfeng},
  date = {2021-10-26},
  pages = {4857--4860},
  publisher = {{ACM}},
  location = {{Virtual Event Queensland Australia}},
  doi = {10.1145/3459637.3483280},
  url = {https://dl.acm.org/doi/10.1145/3459637.3483280},
  urldate = {2022-01-27},
  abstract = {Recently, there has been growing attention on fairness considerations in machine learning. As one of the most pervasive applications of machine learning, recommender systems are gaining increasing and critical impacts on human and society since a growing number of users use them for information seeking and decision making. Therefore, it is crucial to address the potential unfairness problems in recommendation, which may hurt users’ or providers’ satisfaction in recommender systems as well as the interests of the platforms. The tutorial focuses on the foundations and algorithms for fairness in recommendation. It also presents a brief introduction about fairness in basic machine learning tasks such as classification and ranking. The tutorial will introduce the taxonomies of current fairness definitions and evaluation metrics for fairness concerns. We will introduce previous works about fairness in recommendation and also put forward future fairness research directions. The tutorial aims at introducing and communicating fairness in recommendation methods to the community, as well as gathering researchers and practitioners interested in this research direction for discussions, idea communications, and research promotions.},
  eventtitle = {{{CIKM}} '21: {{The}} 30th {{ACM International Conference}} on {{Information}} and {{Knowledge Management}}},
  isbn = {978-1-4503-8446-9},
  langid = {english},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\7S6XXZ3Z\\Li et al. - 2021 - CIKM 2021 Tutorial on Fairness of Machine Learning.pdf}
}

@article{liLeaveNoUser2021,
  title = {Leave {{No User Behind}}: {{Towards Improving}} the {{Utility}} of {{Recommender Systems}} for {{Non-mainstream Users}}},
  shorttitle = {Leave {{No User Behind}}},
  author = {Li, Roger Zhe and Urbano, Julián and Hanjalic, Alan},
  date = {2021-03-08},
  journaltitle = {Proceedings of the 14th ACM International Conference on Web Search and Data Mining},
  eprint = {2102.01744},
  eprinttype = {arxiv},
  pages = {103--111},
  doi = {10.1145/3437963.3441769},
  url = {http://arxiv.org/abs/2102.01744},
  urldate = {2021-07-28},
  abstract = {In a collaborative-filtering recommendation scenario, biases in the data will likely propagate in the learned recommendations. In this paper we focus on the so-called mainstream bias: the tendency of a recommender system to provide better recommendations to users who have a mainstream taste, as opposed to non-mainstream users. We propose NAECF, a conceptually simple but effective idea to address this bias. The idea consists of adding an autoencoder (AE) layer when learning user and item representations with text-based Convolutional Neural Networks. The AEs, one for the users and one for the items, serve as adversaries to the process of minimizing the rating prediction error when learning how to recommend. They enforce that the specific unique properties of all users and items are sufficiently well incorporated and preserved in the learned representations. These representations, extracted as the bottlenecks of the corresponding AEs, are expected to be less biased towards mainstream users, and to provide more balanced recommendation utility across all users. Our experimental results confirm these expectations, significantly improving the recommendations for nonmainstream users while maintaining the recommendation quality for mainstream users. Our results emphasize the importance of deploying extensive content-based features, such as online reviews, in order to better represent users and items to maximize the debiasing effect.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Information Retrieval},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\MLTGJUNC\\Li et al. - 2021 - Leave No User Behind Towards Improving the Utilit.pdf}
}

@inproceedings{linCalibrationCollaborativeFiltering2020,
  title = {Calibration in {{Collaborative Filtering Recommender Systems}}: A {{User-Centered Analysis}}},
  shorttitle = {Calibration in {{Collaborative Filtering Recommender Systems}}},
  booktitle = {Proceedings of the 31st {{ACM Conference}} on {{Hypertext}} and {{Social Media}}},
  author = {Lin, Kun and Sonboli, Nasim and Mobasher, Bamshad and Burke, Robin},
  date = {2020-07-13},
  pages = {197--206},
  publisher = {{ACM}},
  location = {{Virtual Event USA}},
  doi = {10.1145/3372923.3404793},
  url = {https://dl.acm.org/doi/10.1145/3372923.3404793},
  urldate = {2021-07-28},
  abstract = {Recommender systems learn from past user preferences in order to predict future user interests and provide users with personalized suggestions. Previous research has demonstrated that biases in user profiles in the aggregate can influence the recommendations to users who do not share the majority preference. One consequence of this bias propagation effect is miscalibration, a mismatch between the types or categories of items that a user prefers and the items provided in recommendations. In this paper, we conduct a systematic analysis aimed at identifying key characteristics in user profiles that might lead to miscalibrated recommendations. We consider several categories of profile characteristics, including similarity to the average user, propensity towards popularity, profile diversity, and preference intensity. We develop predictive models of miscalibration and use these models to identify the most important features correlated with miscalibration, given different algorithms and dataset characteristics. Our analysis is intended to help system designers predict miscalibration effects and to develop recommendation algorithms with improved calibration properties.},
  eventtitle = {{{HT}} '20: 31st {{ACM Conference}} on {{Hypertext}} and {{Social Media}}},
  isbn = {978-1-4503-7098-1},
  langid = {english},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\V24LL8K6\\Lin et al. - 2020 - Calibration in Collaborative Filtering Recommender.pdf}
}

@article{lindenAmazonComRecommendations2003,
  title = {Amazon.Com Recommendations: Item-to-Item Collaborative Filtering},
  shorttitle = {Amazon.Com Recommendations},
  author = {Linden, G. and Smith, B. and York, J.},
  date = {2003-01},
  journaltitle = {IEEE Internet Computing},
  volume = {7},
  number = {1},
  pages = {76--80},
  issn = {1941-0131},
  doi = {10.1109/MIC.2003.1167344},
  abstract = {Recommendation algorithms are best known for their use on e-commerce Web sites, where they use input about a customer's interests to generate a list of recommended items. Many applications use only the items that customers purchase and explicitly rate to represent their interests, but they can also use other attributes, including items viewed, demographic data, subject interests, and favorite artists. At Amazon.com, we use recommendation algorithms to personalize the online store for each customer. The store radically changes based on customer interests, showing programming titles to a software engineer and baby toys to a new mother. There are three common approaches to solving the recommendation problem: traditional collaborative filtering, cluster models, and search-based methods. Here, we compare these methods with our algorithm, which we call item-to-item collaborative filtering. Unlike traditional collaborative filtering, our algorithm's online computation scales independently of the number of customers and number of items in the product catalog. Our algorithm produces recommendations in real-time, scales to massive data sets, and generates high quality recommendations.},
  eventtitle = {{{IEEE Internet Computing}}},
  keywords = {Advertising,Aggregates,Clustering algorithms,Collaboration,Demography,Electronic mail,Filtering algorithms,Information filtering,Information filters,Pediatrics},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\RXPYYNPN\\1167344.html}
}

@incollection{linMitigatingSentimentBias2021,
  title = {Mitigating {{Sentiment Bias}} for {{Recommender Systems}}},
  booktitle = {Proceedings of the 44th {{International ACM SIGIR Conference}} on {{Research}} and {{Development}} in {{Information Retrieval}}},
  author = {Lin, Chen and Liu, Xinyi and Xv, Guipeng and Li, Hui},
  date = {2021-07-11},
  pages = {31--40},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  url = {https://doi.org/10.1145/3404835.3462943},
  urldate = {2022-01-09},
  abstract = {Biases and de-biasing in recommender systems (RS) have become a research hotspot recently. This paper reveals an unexplored type of bias, i.e., sentiment bias. Through an empirical study, we find that many RS models provide more accurate recommendations on user/item groups having more positive feedback (i.e., positive users/items) than on user/item groups having more negative feedback (i.e., negative users/items). We show that sentiment bias is different from existing biases such as popularity bias: positive users/items do not have more user feedback (i.e., either more ratings or longer reviews). The existence of sentiment bias leads to low-quality recommendations to critical users and unfair recommendations for niche items. We discuss the factors that cause sentiment bias. Then, to fix the sources of sentiment bias, we propose a general de-biasing framework with three strategies manifesting in different regularizers that can be easily plugged into RS models without changing model architectures. Experiments on various RS models and benchmark datasets have verified the effectiveness of our de-biasing framework. To our best knowledge, sentiment bias and its de-biasing have not been studied before. We hope that this work can help strengthen the study of biases and de-biasing in RS.},
  isbn = {978-1-4503-8037-9},
  keywords = {de-biasing,recommender systems,sentiment bias}
}

@incollection{liPersonalizedFairnessBased2021,
  title = {Towards {{Personalized Fairness}} Based on {{Causal Notion}}},
  booktitle = {Proceedings of the 44th {{International ACM SIGIR Conference}} on {{Research}} and {{Development}} in {{Information Retrieval}}},
  author = {Li, Yunqi and Chen, Hanxiong and Xu, Shuyuan and Ge, Yingqiang and Zhang, Yongfeng},
  date = {2021-07-11},
  pages = {1054--1063},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  url = {https://doi.org/10.1145/3404835.3462966},
  urldate = {2022-01-28},
  abstract = {Recommender systems are gaining increasing and critical impacts on human and society since a growing number of users use them for information seeking and decision making. Therefore, it is crucial to address the potential unfairness problems in recommendations. Just like users have personalized preferences on items, users' demands for fairness are also personalized in many scenarios. Therefore, it is important to providepersonalized fair recommendations for users to satisfy theirpersonalized fairness demands. Besides, previous works on fair recommendation mainly focus on association-based fairness. However, it is important to advance from associative fairness notions to causal fairness notions for assessing fairness more properly in recommender systems. Based on the above considerations, this paper focuses on achieving personalized counterfactual fairness for users in recommender systems. To this end, we introduce a framework for achieving counterfactually fair recommendations through adversary learning by generating feature-independent user embeddings for recommendation. The framework allows recommender systems to achieve personalized fairness for users while also covering non-personalized situations. Experiments on two real-world datasets with shallow and deep recommendation algorithms show that our method can generate fairer recommendations for users with a desirable recommendation performance.},
  isbn = {978-1-4503-8037-9},
  keywords = {adversary learning,counterfactual fairness,personalized fairness,recommender system}
}

@online{liuPersonalizingFairnessawareReranking2018,
  title = {Personalizing {{Fairness-aware Re-ranking}}},
  author = {Liu, Weiwen and Burke, Robin},
  date = {2018-09-12},
  eprint = {1809.02921},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1809.02921},
  urldate = {2021-07-28},
  abstract = {Personalized recommendation brings about novel challenges in ensuring fairness, especially in scenarios in which users are not the only stakeholders involved in the recommender system. For example, the system may want to ensure that items from different providers have a fair chance of being recommended. To solve this problem, we propose a Fairness-Aware Re-ranking algorithm (FAR) to balance the ranking quality and provider-side fairness. We iteratively generate the ranking list by trading off between accuracy and the coverage of the providers. Although fair treatment of providers is desirable, users may differ in their receptivity to the addition of this type of diversity. Therefore, personalized user tolerance towards provider diversification is incorporated. Experiments are conducted on both synthetic and real-world data. The results show that our proposed re-ranking algorithm can significantly promote fairness with a slight sacrifice in accuracy and can do so while being attentive to individual user differences.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Information Retrieval},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\HLEC3EG9\\Liu and Burke - 2018 - Personalizing Fairness-aware Re-ranking.pdf}
}

@article{lundgardMeasuringJusticeMachine2020,
  title = {Measuring Justice in Machine Learning},
  author = {Lundgard, Alan},
  date = {2020-01-27},
  journaltitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
  eprint = {2009.10050},
  eprinttype = {arxiv},
  pages = {680--680},
  doi = {10.1145/3351095.3372838},
  url = {http://arxiv.org/abs/2009.10050},
  urldate = {2021-07-29},
  abstract = {How can we build more just machine learning systems? To answer this question, we need to know both what justice is and how to tell whether one system is more or less just than another. That is, we need both a definition and a measure of justice. Theories of distributive justice hold that justice can be measured (in part) in terms of the fair distribution of benefits and burdens across people in society. Recently, the field known as fair machine learning has turned to John Rawls's theory of distributive justice for inspiration and operationalization. However, philosophers known as capability theorists have long argued that Rawls's theory uses the wrong measure of justice, thereby encoding biases against people with disabilities. If these theorists are right, is it possible to operationalize Rawls's theory in machine learning systems without also encoding its biases? In this paper, I draw on examples from fair machine learning to suggest that the answer to this question is no: the capability theorists' arguments against Rawls's theory carry over into machine learning systems. But capability theorists don't only argue that Rawls's theory uses the wrong measure, they also offer an alternative measure. Which measure of justice is right? And has fair machine learning been using the wrong one?},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computers and Society,I.2.0,J.1.0,K.4.1},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\TQD2DBV6\\Lundgard - 2020 - Measuring justice in machine learning.pdf}
}

@article{maasRectifierNonlinearitiesImprove,
  title = {Rectifier {{Nonlinearities Improve Neural Network Acoustic Models}}},
  author = {Maas, Andrew L and Hannun, Awni Y and Ng, Andrew Y},
  pages = {6},
  abstract = {Deep neural network acoustic models produce substantial gains in large vocabulary continuous speech recognition systems. Emerging work with rectified linear (ReL) hidden units demonstrates additional gains in final system performance relative to more commonly used sigmoidal nonlinearities. In this work, we explore the use of deep rectifier networks as acoustic models for the 300 hour Switchboard conversational speech recognition task. Using simple training procedures without pretraining, networks with rectifier nonlinearities produce 2\% absolute reductions in word error rates over their sigmoidal counterparts. We analyze hidden layer representations to quantify differences in how ReL units encode inputs as compared to sigmoidal units. Finally, we evaluate a variant of the ReL unit with a gradient more amenable to optimization in an attempt to further improve deep rectifier networks.},
  langid = {english},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\CHGXWENT\\Maas et al. - Rectiﬁer Nonlinearities Improve Neural Network Aco.pdf}
}

@article{maloneArtificialIntelligenceFuture,
  title = {Artificial {{Intelligence}} and the {{Future}} of {{Work}}},
  author = {Malone, Thomas W and Rus, Daniela and Laubacher, Robert},
  journaltitle = {MIT Work of the Future},
  pages = {39},
  langid = {english},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\NPAUPLQF\\Malone et al. - ARTIFICIAL INTELLIGENCE AND THE FUTURE OF WORK.pdf}
}

@inproceedings{mansouryAutomatingRecommenderSystems2018,
  title = {Automating Recommender Systems Experimentation with Librec-Auto},
  booktitle = {Proceedings of the 12th {{ACM Conference}} on {{Recommender Systems}}},
  author = {Mansoury, Masoud and Burke, Robin and Ordonez-Gauger, Aldo and Sepulveda, Xavier},
  date = {2018-09-27},
  pages = {500--501},
  publisher = {{ACM}},
  location = {{Vancouver British Columbia Canada}},
  doi = {10.1145/3240323.3241614},
  url = {https://dl.acm.org/doi/10.1145/3240323.3241614},
  urldate = {2021-08-11},
  abstract = {Recommender systems research often requires the creation and execution of large numbers of algorithmic experiments to determine the sensitivity of results to the values of various hyperparameters. Existing recommender systems platforms fail to provide a basis for systematic experimentation of this type. In this paper, we describe librec-auto, a wrapper for the well-known LibRec library, which provides an environment that supports automated experimentation.},
  eventtitle = {{{RecSys}} '18: {{Twelfth ACM Conference}} on {{Recommender Systems}}},
  isbn = {978-1-4503-5901-6},
  langid = {english},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\5UMVIZD2\\Mansoury et al. - 2018 - Automating recommender systems experimentation wit.pdf}
}

@online{mansouryFlatterBetterPercentile2019,
  title = {Flatter Is Better: {{Percentile Transformations}} for {{Recommender Systems}}},
  shorttitle = {Flatter Is Better},
  author = {Mansoury, Masoud and Burke, Robin and Mobasher, Bamshad},
  date = {2019-07-10},
  eprint = {1907.07766},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1907.07766},
  urldate = {2021-07-28},
  abstract = {It is well known that explicit user ratings in recommender systems are biased towards high ratings, and that users differ significantly in their usage of the rating scale. Implementers usually compensate for these issues through rating normalization or the inclusion of a user bias term in factorization models. However, these methods adjust only for the central tendency of users’ distributions. In this work, we demonstrate that lack of flatness in rating distributions is negatively correlated with recommendation performance. We propose a rating transformation model that compensates for skew in the rating distribution as well as its central tendency by converting ratings into percentile values as a pre-processing step before recommendation generation. This transformation flattens the rating distribution, better compensates for differences in rating distributions, and improves recommendation performance. We also show a smoothed version of this transformation designed to yield more intuitive results for users with very narrow rating distributions. A comprehensive set of experiments show improved ranking performance for these percentile transformations with state-of-the-art recommendation algorithms in four real-world data sets.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Information Retrieval,Computer Science - Machine Learning},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\9ULEIGT9\\Mansoury et al. - 2019 - Flatter is better Percentile Transformations for .pdf}
}

@article{markusRoleExplainabilityCreating2021,
  title = {The Role of Explainability in Creating Trustworthy Artificial Intelligence for Health Care: {{A}} Comprehensive Survey of the Terminology, Design Choices, and Evaluation Strategies},
  shorttitle = {The Role of Explainability in Creating Trustworthy Artificial Intelligence for Health Care},
  author = {Markus, Aniek F. and Kors, Jan A. and Rijnbeek, Peter R.},
  date = {2021-01-01},
  journaltitle = {Journal of Biomedical Informatics},
  volume = {113},
  pages = {103655},
  issn = {1532-0464},
  doi = {10.1016/j.jbi.2020.103655},
  url = {https://www.sciencedirect.com/science/article/pii/S1532046420302835},
  urldate = {2022-02-09},
  abstract = {Artificial intelligence (AI) has huge potential to improve the health and well-being of people, but adoption in clinical practice is still limited. Lack of transparency is identified as one of the main barriers to implementation, as clinicians should be confident the AI system can be trusted. Explainable AI has the potential to overcome this issue and can be a step towards trustworthy AI. In this paper we review the recent literature to provide guidance to researchers and practitioners on the design of explainable AI systems for the health-care domain and contribute to formalization of the field of explainable AI. We argue the reason to demand explainability determines what should be explained as this determines the relative importance of the properties of explainability (i.e. interpretability and fidelity). Based on this, we propose a framework to guide the choice between classes of explainable AI methods (explainable modelling versus post-hoc explanation; model-based, attribution-based, or example-based explanations; global and local explanations). Furthermore, we find that quantitative evaluation metrics, which are important for objective standardized evaluation, are still lacking for some properties (e.g. clarity) and types of explanations (e.g. example-based methods). We conclude that explainable modelling can contribute to trustworthy AI, but the benefits of explainability still need to be proven in practice and complementary measures might be needed to create trustworthy AI in health care (e.g. reporting data quality, performing extensive (external) validation, and regulation).},
  langid = {english},
  keywords = {Explainable artificial intelligence,Explainable modelling,Interpretability,Post-hoc explanation,Trustworthy artificial intelligence},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\J9T2INJK\\Markus et al. - 2021 - The role of explainability in creating trustworthy.pdf;C\:\\Users\\Romanos\\Zotero\\storage\\HVW4V4YH\\S1532046420302835.html}
}

@article{marlinCollaborativeFilteringMissing,
  title = {Collaborative {{Filtering}} and the {{Missing}} at {{Random Assumption}}},
  author = {Marlin, Benjamin M and Zemel, Richard S and Roweis, Sam and Slaney, Malcolm},
  pages = {9},
  abstract = {Rating prediction is an important application, and a popular research topic in collaborative filtering. However, both the validity of learning algorithms, and the validity of standard testing procedures rest on the assumption that missing ratings are missing at random (MAR). In this paper we present the results of a user study in which we collect a random sample of ratings from current users of an online radio service. An analysis of the rating data collected in the study shows that the sample of random ratings has markedly different properties than ratings of user-selected songs. When asked to report on their own rating behaviour, a large number of users indicate they believe their opinion of a song does affect whether they choose to rate that song, a violation of the MAR condition. Finally, we present experimental results showing that incorporating an explicit model of the missing data mechanism can lead to significant improvements in prediction performance on the random sample of ratings.},
  langid = {english},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\VQDFFHXJ\\Marlin et al. - Collaborative Filtering and the Missing at Random .pdf}
}

@inproceedings{mcauleyImagebasedRecommendationsStyles2015,
  title = {Image-Based {{Recommendations}} on {{Styles}} and {{Substitutes}}},
  booktitle = {Proceedings of the 38th {{International ACM SIGIR Conference}} on {{Research}} and {{Development}} in {{Information Retrieval}}},
  author = {McAuley, Julian and Targett, Christopher and Shi, Qinfeng and van den Hengel, Anton},
  options = {useprefix=true},
  date = {2015-08-09},
  series = {{{SIGIR}} '15},
  pages = {43--52},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/2766462.2767755},
  url = {https://doi.org/10.1145/2766462.2767755},
  urldate = {2022-02-22},
  abstract = {Humans inevitably develop a sense of the relationships between objects, some of which are based on their appearance. Some pairs of objects might be seen as being alternatives to each other (such as two pairs of jeans), while others may be seen as being complementary (such as a pair of jeans and a matching shirt). This information guides many of the choices that people make, from buying clothes to their interactions with each other. We seek here to model this human sense of the relationships between objects based on their appearance. Our approach is not based on fine-grained modeling of user annotations but rather on capturing the largest dataset possible and developing a scalable method for uncovering human notions of the visual relationships within. We cast this as a network inference problem defined on graphs of related images, and provide a large-scale dataset for the training and evaluation of the same. The system we develop is capable of recommending which clothes and accessories will go well together (and which will not), amongst a host of other applications.},
  isbn = {978-1-4503-3621-5},
  keywords = {metric learning,recommender systems,visual features},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\24NQIUPZ\\McAuley et al. - 2015 - Image-Based Recommendations on Styles and Substitu.pdf}
}

@online{mcauleyImagebasedRecommendationsStyles2016,
  title = {Image-Based {{Recommendations}} on {{Styles}} and {{Substitutes}}},
  author = {McAuley, Julian and Targett, Christopher and Shi, Qinfeng and van den Hengel, Anton},
  date = {2016-06-15},
  eprint = {1506.04757},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1506.04757},
  urldate = {2021-11-22},
  abstract = {Humans inevitably develop a sense of the relationships between objects, some of which are based on their appearance. Some pairs of objects might be seen as being alternatives to each other (such as two pairs of jeans), while others may be seen as being complementary (such as a pair of jeans and a matching shirt). This information guides many of the choices that people make, from buying clothes to their interactions with each other. We seek here to model this human sense of the relationships between objects based on their appearance. Our approach is not based on fine-grained modeling of user annotations but rather on capturing the largest dataset possible and developing a scalable method for uncovering human notions of the visual relationships within. We cast this as a network inference problem defined on graphs of related images, and provide a large-scale dataset for the training and evaluation of the same. The system we develop is capable of recommending which clothes and accessories will go well together (and which will not), amongst a host of other applications.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Information Retrieval},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\G2KZ5DX8\\McAuley et al. - 2015 - Image-based Recommendations on Styles and Substitu.pdf}
}

@online{mehrabiSurveyBiasFairness2019,
  title = {A {{Survey}} on {{Bias}} and {{Fairness}} in {{Machine Learning}}},
  author = {Mehrabi, Ninareh and Morstatter, Fred and Saxena, Nripsuta and Lerman, Kristina and Galstyan, Aram},
  date = {2019-09-17},
  eprint = {1908.09635},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1908.09635},
  urldate = {2021-07-29},
  abstract = {With the widespread use of AI systems and applications in our everyday lives, it is important to take fairness issues into consideration while designing and engineering these types of systems. Such systems can be used in many sensitive environments to make important and life-changing decisions; thus, it is crucial to ensure that the decisions do not reflect discriminatory behavior toward certain groups or populations. We have recently seen work in machine learning, natural language processing, and deep learning that addresses such challenges in different subdomains. With the commercialization of these systems, researchers are becoming aware of the biases that these applications can contain and have attempted to address them. In this survey we investigated different real-world applications that have shown biases in various ways, and we listed different sources of biases that can affect AI applications. We then created a taxonomy for fairness definitions that machine learning researchers have defined in order to avoid the existing bias in AI systems. In addition to that, we examined different domains and subdomains in AI showing what researchers have observed with regard to unfair outcomes in the state-of-the-art methods and how they have tried to address them. There are still many future directions and solutions that can be taken to mitigate the problem of bias in AI systems. We are hoping that this survey will motivate researchers to tackle these issues in the near future by observing existing work in their respective fields.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\JL3LZRML\\Mehrabi et al. - 2019 - A Survey on Bias and Fairness in Machine Learning.pdf}
}

@article{mehrabiSurveyBiasFairness2021,
  title = {A {{Survey}} on {{Bias}} and {{Fairness}} in {{Machine Learning}}},
  author = {Mehrabi, Ninareh and Morstatter, Fred and Saxena, Nripsuta and Lerman, Kristina and Galstyan, Aram},
  date = {2021-07-13},
  journaltitle = {ACM Comput. Surv.},
  volume = {54},
  number = {6},
  pages = {115:1--115:35},
  issn = {0360-0300},
  doi = {10.1145/3457607},
  url = {https://doi.org/10.1145/3457607},
  urldate = {2022-02-22},
  abstract = {With the widespread use of artificial intelligence (AI) systems and applications in our everyday lives, accounting for fairness has gained significant importance in designing and engineering of such systems. AI systems can be used in many sensitive environments to make important and life-changing decisions; thus, it is crucial to ensure that these decisions do not reflect discriminatory behavior toward certain groups or populations. More recently some work has been developed in traditional machine learning and deep learning that address such challenges in different subdomains. With the commercialization of these systems, researchers are becoming more aware of the biases that these applications can contain and are attempting to address them. In this survey, we investigated different real-world applications that have shown biases in various ways, and we listed different sources of biases that can affect AI applications. We then created a taxonomy for fairness definitions that machine learning researchers have defined to avoid the existing bias in AI systems. In addition to that, we examined different domains and subdomains in AI showing what researchers have observed with regard to unfair outcomes in the state-of-the-art methods and ways they have tried to address them. There are still many future directions and solutions that can be taken to mitigate the problem of bias in AI systems. We are hoping that this survey will motivate researchers to tackle these issues in the near future by observing existing work in their respective fields.},
  keywords = {deep learning,Fairness and bias in artificial intelligence,machine learning,natural language processing,representation learning},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\PG5IZH6L\\Mehrabi et al. - 2021 - A Survey on Bias and Fairness in Machine Learning.pdf}
}

@article{milanoRecommenderSystemsTheir2020,
  title = {Recommender Systems and Their Ethical Challenges},
  author = {Milano, Silvia and Taddeo, Mariarosaria and Floridi, Luciano},
  date = {2020-12},
  journaltitle = {AI \& Soc},
  volume = {35},
  number = {4},
  pages = {957--967},
  issn = {0951-5666, 1435-5655},
  doi = {10.1007/s00146-020-00950-y},
  url = {http://link.springer.com/10.1007/s00146-020-00950-y},
  urldate = {2021-07-28},
  abstract = {This article presents the first, systematic analysis of the ethical challenges posed by recommender systems through a literature review. The article identifies six areas of concern, and maps them onto a proposed taxonomy of different kinds of ethical impact. The analysis uncovers a gap in the literature: currently user-centred approaches do not consider the interests of a variety of other stakeholders—as opposed to just the receivers of a recommendation—in assessing the ethical impacts of a recommender system.},
  langid = {english},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\5G4UYXBT\\Milano et al. - 2020 - Recommender systems and their ethical challenges.pdf}
}

@book{mitchellMachineLearning1997,
  title = {Machine {{Learning}}},
  author = {Mitchell, Tom M},
  date = {1997},
  publisher = {{McGraw-Hill Education}},
  abstract = {Mitchell covers the field of machine learning, the study of algorithms that allow computer programs to automatically improve through experience and that automatically infer general laws from specific data.},
  isbn = {978-0-07-042807-2 978-0-07-115467-3},
  langid = {english},
  annotation = {OCLC: 61321007}
}

@book{molnarInterpretableMachineLearning2019,
  title = {Interpretable {{Machine Learning}}},
  author = {Molnar, Christoph},
  date = {2019},
  url = {https://christophm.github.io/interpretable-ml-book/},
  urldate = {2022-01-30},
  abstract = {Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners to make machine learning decisions interpretable.},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\NXPCGUUD\\interpretable-ml-book.html}
}

@book{molnarTaxonomyInterpretabilityMethods,
  title = {3.2 {{Taxonomy}} of {{Interpretability Methods}} | {{Interpretable Machine Learning}}},
  author = {Molnar, Christoph},
  url = {https://christophm.github.io/interpretable-ml-book/taxonomy-of-interpretability-methods.html},
  urldate = {2022-01-30},
  abstract = {Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners to make machine learning decisions interpretable.},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\IF3RI52G\\taxonomy-of-interpretability-methods.html}
}

@article{montavonMethodsInterpretingUnderstanding2018,
  title = {Methods for {{Interpreting}} and {{Understanding Deep Neural Networks}}},
  author = {Montavon, Grégoire and Samek, Wojciech and Müller, Klaus-Robert},
  date = {2018-02},
  journaltitle = {Digital Signal Processing},
  volume = {73},
  eprint = {1706.07979},
  eprinttype = {arxiv},
  pages = {1--15},
  issn = {10512004},
  doi = {10.1016/j.dsp.2017.10.011},
  url = {http://arxiv.org/abs/1706.07979},
  urldate = {2022-01-22},
  abstract = {This paper provides an entry point to the problem of interpreting a deep neural network model and explaining its predictions. It is based on a tutorial given at ICASSP 2017. It introduces some recently proposed techniques of interpretation, along with theory, tricks and recommendations, to make most efficient use of these techniques on real data. It also discusses a number of practical applications.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\AMMR3VZS\\Montavon et al. - 2018 - Methods for Interpreting and Understanding Deep Ne.pdf}
}

@online{mulderOperationalizingFramingSupport2021,
  title = {Operationalizing {{Framing}} to {{Support Multiperspective Recommendations}} of {{Opinion Pieces}}},
  author = {Mulder, Mats and Inel, Oana and Oosterman, Jasper and Tintarev, Nava},
  date = {2021-03-24},
  eprint = {2101.06141},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2101.06141},
  urldate = {2021-07-28},
  abstract = {Diversity in personalized news recommender systems is often defined as dissimilarity, and based on topic diversity (e.g., corona versus farmers strike). Diversity in news media, however, is understood as multiperspectivity (e.g., different opinions on corona measures), and arguably a key responsibility of the press in a democratic society. While viewpoint diversity is often considered synonymous with source diversity in communication science domain, in this paper, we take a computational view. We operationalize the notion of framing, adopted from communication science. We apply this notion to a re-ranking of topic-relevant recommended lists, to form the basis of a novel viewpoint diversification method. Our offline evaluation indicates that the proposed method is capable of enhancing the viewpoint diversity of recommendation lists according to a diversity metric from literature. In an online study, on the Blendle platform, a Dutch news aggregator platform, with more than 2000 users, we found that users are willing to consume viewpoint diverse news recommendations. We also found that presentation characteristics significantly influence the reading behaviour of diverse recommendations. These results suggest that future research on presentation aspects of recommendations can be just as important as novel viewpoint diversification methods to truly achieve multiperspectivity in online news environments.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computers and Society,Computer Science - Information Retrieval},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\FC4Q3E4P\\Mulder et al. - 2021 - Operationalizing Framing to Support Multiperspecti.pdf}
}

@inproceedings{ningSLIMSparseLinear2011,
  title = {{{SLIM}}: {{Sparse Linear Methods}} for {{Top-N Recommender Systems}}},
  shorttitle = {{{SLIM}}},
  author = {Ning, Xia and Karypis, George},
  date = {2011-12-01},
  pages = {497--506},
  publisher = {{IEEE}},
  doi = {10.1109/ICDM.2011.134},
  abstract = {This paper focuses on developing effective and efficient algorithms for top-N recommender systems. A novel Sparse Linear Method (SLIM) is proposed, which generates top-N recommendations by aggregating from user purchase/rating profiles. A sparse aggregation coefficient matrix W is learned from SLIM by solving an `1-norm and `2-norm regularized optimization problem. W is demonstrated to produce high quality recommendations and its sparsity allows SLIM to generate recommendations very fast. A comprehensive set of experiments is conducted by comparing the SLIM method and other state-of-the-art top-N recommendation methods. The experiments show that SLIM achieves significant improvements both in run time performance and recommendation quality over the best existing methods.},
  eventtitle = {2011 {{IEEE}} 11th {{International Conference}} on {{Data Mining}}},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\2KTAY2PS\\Ning and Karypis - 2011 - SLIM Sparse Linear Methods for Top-N Recommender .pdf}
}

@inproceedings{NIPS2017_e6384711,
  title = {Beyond Parity: {{Fairness}} Objectives for Collaborative Filtering},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Yao, Sirui and Huang, Bert},
  editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
  date = {2017},
  series = {{{NIPS}}'17},
  volume = {30},
  pages = {2925--2934},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2017/file/e6384711491713d29bc63fc5eeb5ba4f-Paper.pdf},
  abstract = {We study fairness in collaborative-filtering recommender systems, which are sensitive to discrimination that exists in historical data. Biased data can lead collaborative-filtering methods to make unfair predictions for users from minority groups. We identify the insufficiency of existing fairness metrics and propose four new metrics that address different forms of unfairness. These fairness metrics can be optimized by adding fairness terms to the learning objective. Experiments on synthetic and real data show that our new metrics can better measure fairness than the baseline, and that the fairness objectives effectively help reduce unfairness.},
  isbn = {978-1-5108-6096-4}
}

@inproceedings{oardImplicitFeedbackRecommender1998,
  title = {Implicit Feedback for Recommender Systems},
  booktitle = {Proceedings of the {{AAAI}} Workshop on Recommender Systems},
  author = {Oard, Douglas W. and Kim, Jinmook},
  date = {1998},
  volume = {83},
  pages = {81--83},
  publisher = {{AAAI}}
}

@article{oardImplicitFeedbackRecommender1998a,
  title = {Implicit {{Feedback}} for {{Recommender Systems}}},
  author = {Oard, Douglas W and Kim, Jinmook},
  date = {1998},
  volume = {83},
  pages = {3},
  abstract = {Canimplicitfeedbacksubstitute for explicit ratings in recommendseyrstems?If so, wecouldavoidthe difficulties associatedwithgatheringexplicit ratings fromusers. How, then, can wecaptureuseful informationunobtrusively,and howmight weuse that information to makerecommendations? In this paperweidentify three types of implicit feedbackandsuggesttwostrategies for usingimplicit feedback to makerecommendations.},
  langid = {english},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\A95AEV2L\\Oard and Kim - Implicit Feedback for Recommender Systems.pdf}
}

@online{orphanouMitigatingBiasAlgorithmic2021,
  title = {Mitigating {{Bias}} in {{Algorithmic Systems}}: {{A Fish-Eye View}} of {{Problems}} and {{Solutions Across Domains}}},
  shorttitle = {Mitigating {{Bias}} in {{Algorithmic Systems}}},
  author = {Orphanou, Kalia and Otterbacher, Jahna and Kleanthous, Styliani and Batsuren, Khuyagbaatar and Giunchiglia, Fausto and Bogina, Veronika and Tal, Avital Shulner and AlanHartman and Kuflik, Tsvi},
  date = {2021-03-31},
  eprint = {2103.16953},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2103.16953},
  urldate = {2021-07-28},
  abstract = {Mitigating bias in algorithmic systems is a critical issue drawing attention across communities within the information and computer sciences. Given the complexity of the problem and the involvement of multiple stakeholders – including developers, end-users and third-parties – there is a need to understand the landscape of the sources of bias, and the solutions being proposed to address them. This survey provides a “fish-eye view," examining approaches across four areas of research. The literature describes three steps toward a comprehensive treatment – bias detection, fairness management and explainability management – and underscores the need to work from within the system as well as from the perspective of stakeholders in the broader context.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {A.1,Computer Science - Computers and Society,K.4.0},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\V66KMV2E\\Orphanou et al. - 2021 - Mitigating Bias in Algorithmic Systems A Fish-Eye.pdf}
}

@article{paateroPositiveMatrixFactorization1994,
  title = {Positive Matrix Factorization: {{A}} Non-Negative Factor Model with Optimal Utilization of Error Estimates of Data Values},
  shorttitle = {Positive Matrix Factorization},
  author = {Paatero, Pentti and Tapper, Unto},
  date = {1994},
  journaltitle = {Environmetrics},
  volume = {5},
  number = {2},
  pages = {111--126},
  issn = {1099-095X},
  doi = {10.1002/env.3170050203},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/env.3170050203},
  urldate = {2021-08-08},
  abstract = {A new variant ‘PMF’ of factor analysis is described. It is assumed that X is a matrix of observed data and σ is the known matrix of standard deviations of elements of X. Both X and σ are of dimensions n × m. The method solves the bilinear matrix problem X = GF + E where G is the unknown left hand factor matrix (scores) of dimensions n × p, F is the unknown right hand factor matrix (loadings) of dimensions p × m, and E is the matrix of residuals. The problem is solved in the weighted least squares sense: G and F are determined so that the Frobenius norm of E divided (element-by-element) by σ is minimized. Furthermore, the solution is constrained so that all the elements of G and F are required to be non-negative. It is shown that the solutions by PMF are usually different from any solutions produced by the customary factor analysis (FA, i.e. principal component analysis (PCA) followed by rotations). Usually PMF produces a better fit to the data than FA. Also, the result of PF is guaranteed to be non-negative, while the result of FA often cannot be rotated so that all negative entries would be eliminated. Different possible application areas of the new method are briefly discussed. In environmental data, the error estimates of data can be widely varying and non-negativity is often an essential feature of the underlying models. Thus it is concluded that PMF is better suited than FA or PCA in many environmental applications. Examples of successful applications of PMF are shown in companion papers.},
  langid = {english},
  keywords = {Alternating regression,Error estimates,Factor analysis,Principal component analysis,Repetitive measurements,Scaling,Weighted least squares},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/env.3170050203},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\GB4365HJ\\env.html}
}

@inproceedings{parkLongTailRecommender2008,
  title = {The Long Tail of Recommender Systems and How to Leverage It},
  booktitle = {Proceedings of the 2008 {{ACM}} Conference on {{Recommender}} Systems},
  author = {Park, Yoon-Joo and Tuzhilin, Alexander},
  date = {2008-10-23},
  series = {{{RecSys}} '08},
  pages = {11--18},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/1454008.1454012},
  url = {https://doi.org/10.1145/1454008.1454012},
  urldate = {2022-02-13},
  abstract = {The paper studies the Long Tail problem of recommender systems when many items in the Long Tail have only few ratings, thus making it hard to use them in recommender systems. The approach presented in the paper splits the whole itemset into the head and the tail parts and clusters only the tail items. Then recommendations for the tail items are based on the ratings in these clusters and for the head items on the ratings of individual items. If such partition and clustering are done properly, we show that this reduces the recommendation error rates for the tail items, while maintaining reasonable computational performance.},
  isbn = {978-1-60558-093-7},
  keywords = {clustering,data mining,long tail,recommendation},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\V3IMESUG\\Park and Tuzhilin - 2008 - The long tail of recommender systems and how to le.pdf}
}

@book{pasqualeBlackBoxSociety2015,
  title = {The {{Black Box Society}}: {{The Secret Algorithms That Control Money}} and {{Information}}},
  shorttitle = {The {{Black Box Society}}},
  author = {Pasquale, Frank},
  date = {2015-01-05},
  publisher = {{Harvard University Press}},
  doi = {10.4159/harvard.9780674736061},
  url = {https://www.degruyter.com/document/doi/10.4159/harvard.9780674736061/html},
  urldate = {2022-01-30},
  isbn = {978-0-674-73606-1}
}

@online{pessachAlgorithmicFairness2020,
  title = {Algorithmic {{Fairness}}},
  author = {Pessach, Dana and Shmueli, Erez},
  date = {2020-01-21},
  eprint = {2001.09784},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/2001.09784},
  urldate = {2021-07-28},
  abstract = {An increasing number of decisions regarding the daily lives of human beings are being controlled by artificial intelligence (AI) algorithms in spheres ranging from healthcare, transportation, and education to college admissions, recruitment, provision of loans and many more realms. Since they now touch on many aspects of our lives, it is crucial to develop AI algorithms that are not only accurate but also objective and fair. Recent studies have shown that algorithmic decision-making may be inherently prone to unfairness, even when there is no intention for it. This paper presents an overview of the main concepts of identifying, measuring and improving algorithmic fairness when using AI algorithms. The paper begins by discussing the causes of algorithmic bias and unfairness and the common definitions and measures for fairness. Fairness-enhancing mechanisms are then reviewed and divided into pre-process, in-process and post-process mechanisms. A comprehensive comparison of the mechanisms is then conducted, towards a better understanding of which mechanisms should be used in different scenarios. The paper then describes the most commonly used fairnessrelated datasets in this field. Finally, the paper ends by reviewing several emerging research sub-fields of algorithmic fairness.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computers and Society,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\74VSFMBU\\Pessach and Shmueli - 2020 - Algorithmic Fairness.pdf}
}

@online{pitouraFairnessRankingsRecommendations2021,
  title = {Fairness in {{Rankings}} and {{Recommendations}}: {{An Overview}}},
  shorttitle = {Fairness in {{Rankings}} and {{Recommendations}}},
  author = {Pitoura, Evaggelia and Stefanidis, Kostas and Koutrika, Georgia},
  date = {2021-08-31},
  eprint = {2104.05994},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2104.05994},
  urldate = {2022-01-22},
  abstract = {We increasingly depend on a variety of datadriven algorithmic systems to assist us in many aspects of life. Search engines and recommender systems amongst others are used as sources of information and to help us in making all sort of decisions from selecting restaurants and books, to choosing friends and careers. This has given rise to important concerns regarding the fairness of such systems. In this work, we aim at presenting a toolkit of definitions, models and methods used for ensuring fairness in rankings and recommendations. Our objectives are three-fold: (a) to provide a solid framework on a novel, quickly evolving, and impactful domain, (b) to present related methods and put them into perspective, and (c) to highlight open challenges and research paths for future work.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Databases},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\FXL58N5P\\Pitoura et al. - 2021 - Fairness in Rankings and Recommendations An Overv.pdf}
}

@dataset{pitouraFairnessRankingsRecommenders2020,
  title = {Fairness in {{Rankings}} and {{Recommenders}}},
  author = {Pitoura, Evaggelia and Koutrika, Georgia and Stefanidis, Kostas},
  date = {2020},
  publisher = {{OpenProceedings.org}},
  doi = {10.5441/002/EDBT.2020.86},
  url = {https://openproceedings.org/2020/conf/edbt/paper_T3.pdf},
  urldate = {2021-07-28},
  abstract = {With the growing complexity of the available online information, search engines via rankings and recommender systems come to the rescue, providing suggestions to users about items of potential interest, from movies and products to news articles and even potential friends. Such results and suggestions aim at covering the user information needs and play an important role in guiding users’ decisions and in forming their opinions.},
  langid = {english},
  version = {1},
  keywords = {Database Technology},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\8PWNRK4G\\Pitoura et al. - 2020 - Fairness in Rankings and Recommenders.pdf}
}

@book{POLITEIATOYPLATONA,
  title = {Η ΠΟΛΙΤΕΙΑ ΤΟΥ ΠΛΑΤΩΝΑ},
  url = {https://www.politeianet.gr/books/9789602206706-platon-pournaras-i-politeia-tou-platona-42592},
  urldate = {2022-01-26},
  abstract = {Ο Πλάτωνας κατέχει την πρώτη θέση στη δυτική φιλοσοφική παράδοση. Αυτός επενόησε, θεμελίωσε και τοποθέτησε τη φιλοσοφία επικεφαλής όλων των επιστημών. Ανάμεσα στην πλειάδα των δραματικών διαλόγων, που έγραψε ο Πλάτωνας, η  Πολιτεία  έρχεται σε πρώτη σε έκταση και περιεχόμενο. Οι ριζοσπαστικές αντιλήψεις που διατυπώνονται στην  Πολιτεία  σχετικά με την ανθρώπινη ζωή, και με την πολιτική κοινότητα σ},
  langid = {el-gr},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\ZGMGIVI6\\9789602206706-platon-pournaras-i-politeia-tou-platona-42592.html}
}

@article{polonioliEthicsScientificRecommender2021,
  title = {The Ethics of Scientific Recommender Systems},
  author = {Polonioli, Andrea},
  date = {2021-02},
  journaltitle = {Scientometrics},
  volume = {126},
  number = {2},
  pages = {1841--1848},
  issn = {0138-9130, 1588-2861},
  doi = {10.1007/s11192-020-03766-1},
  url = {http://link.springer.com/10.1007/s11192-020-03766-1},
  urldate = {2021-07-28},
  abstract = {Scientific recommender systems have become increasingly popular as a tool to overcome information overload, allowing researchers to access fresh and relevant content. However, this article presents an analysis of the most pressing ethical challenges posed by recommender systems in the context of scientific research. In particular, it is argued that scientific recommender systems may risk isolating scholars in information bubbles and insulating them from exposure to different viewpoints. Further, they also risk suffering from popularity biases which may lead to a winner-takes-all scenario and reinforce discrepancies in recognition received by eminent scientists and unknown researchers. The article concludes with recommendations for scientists, journals, and digital libraries to facilitate progress in the study of the ethics of scientific recommender systems.},
  langid = {english},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\XTENC7PD\\Polonioli - 2021 - The ethics of scientific recommender systems.pdf}
}

@article{portugalUseMachineLearning2018,
  title = {The Use of Machine Learning Algorithms in Recommender Systems: {{A}} Systematic Review},
  shorttitle = {The Use of Machine Learning Algorithms in Recommender Systems},
  author = {Portugal, Ivens and Alencar, Paulo and Cowan, Donald},
  date = {2018-05},
  journaltitle = {Expert Systems with Applications},
  volume = {97},
  pages = {205--227},
  issn = {09574174},
  doi = {10.1016/j.eswa.2017.12.020},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0957417417308333},
  urldate = {2021-07-28},
  abstract = {Recommender systems use algorithms to provide users with product or service recommendations. Recently, these systems have been using machine learning algorithms from the field of artificial intelligence. However, choosing a suitable machine learning algorithm for a recommender system is difficult because of the number of algorithms described in the literature. Researchers and practitioners developing recommender systems are left with little information about the current approaches in algorithm usage. Moreover, the development of a recommender system using a machine learning algorithm often has problems and open questions that must be evaluated, so software engineers know where to focus research efforts. This paper presents a systematic review of the literature that analyzes the use of machine learning algorithms in recommender systems and identifies research opportunities for software engineering research. The study concludes that Bayesian and decision tree algorithms are widely used in recommender systems because of their relative simplicity, and that requirement and design phases of recommender system development appear to offer opportunities for further research.},
  langid = {english},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\4UR864P7\\Portugal et al. - 2018 - The use of machine learning algorithms in recommen.pdf}
}

@inproceedings{qinAttributebasedPropensityUnbiased2020,
  title = {Attribute-Based {{Propensity}} for {{Unbiased Learning}} in {{Recommender Systems}}: {{Algorithm}} and {{Case Studies}}},
  shorttitle = {Attribute-Based {{Propensity}} for {{Unbiased Learning}} in {{Recommender Systems}}},
  booktitle = {Proceedings of the 26th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} \& {{Data Mining}}},
  author = {Qin, Zhen and Chen, Suming J. and Metzler, Donald and Noh, Yongwoo and Qin, Jingzheng and Wang, Xuanhui},
  date = {2020-08-23},
  pages = {2359--2367},
  publisher = {{ACM}},
  location = {{Virtual Event CA USA}},
  doi = {10.1145/3394486.3403285},
  url = {https://dl.acm.org/doi/10.1145/3394486.3403285},
  urldate = {2021-07-28},
  abstract = {Many modern recommender systems train their models based on a large amount of implicit user feedback data. Due to the inherent bias in this data (e.g., position bias), learning from it directly can lead to suboptimal models. Recently, unbiased learning was proposed to address such problems by leveraging counterfactual techniques like inverse propensity weighting (IPW). In these methods, propensity scores estimation is usually limited to item’s display position in a single user interface (UI).},
  eventtitle = {{{KDD}} '20: {{The}} 26th {{ACM SIGKDD Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  isbn = {978-1-4503-7998-4},
  langid = {english},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\55A2QKLS\\Qin et al. - 2020 - Attribute-based Propensity for Unbiased Learning i.pdf}
}

@article{quangVIETNAMNATIONALUNIVERSITY,
  title = {{{VIETNAM NATIONAL UNIVERSITY OF HOCHIMINH CITY THE INTERNATIONAL UNIVERSITY SCHOOL OF COMPUTER SCIENCE AND ENGINEERING}}},
  author = {Quang, Pham Minh},
  pages = {58},
  langid = {english},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\45J4CEFQ\\Quang - VIETNAM NATIONAL UNIVERSITY OF HOCHIMINH CITY THE .pdf}
}

@article{ramosNegativeImpactSocial2020,
  title = {On the Negative Impact of Social Influence in Recommender Systems: {{A}} Study of Bribery in Collaborative Hybrid Algorithms},
  shorttitle = {On the Negative Impact of Social Influence in Recommender Systems},
  author = {Ramos, Guilherme and Boratto, Ludovico and Caleiro, Carlos},
  date = {2020-03},
  journaltitle = {Information Processing \& Management},
  volume = {57},
  number = {2},
  pages = {102058},
  issn = {03064573},
  doi = {10.1016/j.ipm.2019.102058},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0306457318308768},
  urldate = {2021-07-28},
  abstract = {Recommender systems are based on inherent forms of social influence. Indeed, suggestions are provided to the users based on the opinions of peers. Given the relevance that ratings have nowadays to push the sales of an item, sellers might decide to bribe users so that they rate or change the ratings given to items, thus increasing the sellers’ reputation. Hence, by exploiting the fact that influential users can lead an item to get recommended, bribing can become an effective way to negatively exploit social influence and introduce a bias in the recommendations. Given that bribing is forbidden but still employed by sellers, we propose a novel matrix completion algorithm that performs hybrid memory-based collaborative filtering using an approximation of Kolmogorov complexity. We also propose a framework to study the bribery effect and the bribery resistance of our approach. Our theoretical analysis, validated through experiments on real-world datasets, shows that our approach is an effective way to counter bribing while, with state-of-theart algorithms, sellers can bribe a large part of the users.},
  langid = {english},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\68BFX4S5\\Ramos et al. - 2020 - On the negative impact of social influence in reco.pdf}
}

@article{rastegarpanahFightingFireFire2019,
  title = {Fighting {{Fire}} with {{Fire}}: {{Using Antidote Data}} to {{Improve Polarization}} and {{Fairness}} of {{Recommender Systems}}},
  shorttitle = {Fighting {{Fire}} with {{Fire}}},
  author = {Rastegarpanah, Bashir and Gummadi, Krishna P. and Crovella, Mark},
  date = {2019-01-30},
  journaltitle = {Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining},
  eprint = {1812.01504},
  eprinttype = {arxiv},
  pages = {231--239},
  doi = {10.1145/3289600.3291002},
  url = {http://arxiv.org/abs/1812.01504},
  urldate = {2021-07-28},
  abstract = {The increasing role of recommender systems in many aspects of society makes it essential to consider how such systems may impact social good. Various modifications to recommendation algorithms have been proposed to improve their performance for specific socially relevant measures. However, previous proposals are often not easily adapted to different measures, and they generally require the ability to modify either existing system inputs, the system’s algorithm, or the system’s outputs. As an alternative, in this paper we introduce the idea of improving the social desirability of recommender system outputs by adding more data to the input, an approach we view as providing ‘antidote’ data to the system. We formalize the antidote data problem, and develop optimization-based solutions. We take as our model system the matrix factorization approach to recommendation, and we propose a set of measures to capture the polarization or fairness of recommendations. We then show how to generate antidote data for each measure, pointing out a number of computational efficiencies, and discuss the impact on overall system accuracy. Our experiments show that a modest budget for antidote data can lead to significant improvements in the polarization or fairness of recommendations.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Information Retrieval,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\BEBSIL3X\\Rastegarpanah et al. - 2019 - Fighting Fire with Fire Using Antidote Data to Im.pdf}
}

@inproceedings{rendleBPRBayesianPersonalized2009,
  title = {{{BPR}}: {{Bayesian}} Personalized Ranking from Implicit Feedback},
  shorttitle = {{{BPR}}},
  booktitle = {Proceedings of the {{Twenty-Fifth Conference}} on {{Uncertainty}} in {{Artificial Intelligence}}},
  author = {Rendle, Steffen and Freudenthaler, Christoph and Gantner, Zeno and Schmidt-Thieme, Lars},
  date = {2009-06-18},
  series = {{{UAI}} '09},
  pages = {452--461},
  publisher = {{AUAI Press}},
  location = {{Arlington, Virginia, USA}},
  abstract = {Item recommendation is the task of predicting a personalized ranking on a set of items (e.g. websites, movies, products). In this paper, we investigate the most common scenario with implicit feedback (e.g. clicks, purchases). There are many methods for item recommendation from implicit feedback like matrix factorization (MF) or adaptive k-nearest-neighbor (kNN). Even though these methods are designed for the item prediction task of personalized ranking, none of them is directly optimized for ranking. In this paper we present a generic optimization criterion BPR-Opt for personalized ranking that is the maximum posterior estimator derived from a Bayesian analysis of the problem. We also provide a generic learning algorithm for optimizing models with respect to BPR-Opt. The learning method is based on stochastic gradient descent with bootstrap sampling. We show how to apply our method to two state-of-the-art recommender models: matrix factorization and adaptive kNN. Our experiments indicate that for the task of personalized ranking our optimization method outperforms the standard learning techniques for MF and kNN. The results show the importance of optimizing models for the right criterion.},
  isbn = {978-0-9749039-5-8},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\MXB6R5AG\\Rendle et al. - 2009 - BPR Bayesian personalized ranking from implicit f.pdf}
}

@online{rendleBPRBayesianPersonalized2012,
  title = {{{BPR}}: {{Bayesian Personalized Ranking}} from {{Implicit Feedback}}},
  shorttitle = {{{BPR}}},
  author = {Rendle, Steffen and Freudenthaler, Christoph and Gantner, Zeno and Schmidt-Thieme, Lars},
  date = {2012-05-09},
  eprint = {1205.2618},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1205.2618},
  urldate = {2021-07-29},
  abstract = {Item recommendation is the task of predicting a personalized ranking on a set of items (e.g. websites, movies, products). In this paper, we investigate the most common scenario with implicit feedback (e.g. clicks, purchases). There are many methods for item recommendation from implicit feedback like matrix factorization (MF) or adaptive knearest-neighbor (kNN). Even though these methods are designed for the item prediction task of personalized ranking, none of them is directly optimized for ranking. In this paper we present a generic optimization criterion BPR-Opt for personalized ranking that is the maximum posterior estimator derived from a Bayesian analysis of the problem. We also provide a generic learning algorithm for optimizing models with respect to BPR-Opt. The learning method is based on stochastic gradient descent with bootstrap sampling. We show how to apply our method to two state-of-the-art recommender models: matrix factorization and adaptive kNN. Our experiments indicate that for the task of personalized ranking our optimization method outperforms the standard learning techniques for MF and kNN. The results show the importance of optimizing models for the right criterion.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Information Retrieval,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\VHSH7LHD\\Rendle et al. - 2012 - BPR Bayesian Personalized Ranking from Implicit F.pdf;C\:\\Users\\Romanos\\Zotero\\storage\\IDNW8YFL\\1205.html}
}

@inproceedings{rendleFactorizationMachines2010,
  title = {Factorization {{Machines}}},
  booktitle = {2010 {{IEEE International Conference}} on {{Data Mining}}},
  author = {Rendle, Steffen},
  date = {2010-12},
  pages = {995--1000},
  issn = {2374-8486},
  doi = {10.1109/ICDM.2010.127},
  abstract = {In this paper, we introduce Factorization Machines (FM) which are a new model class that combines the advantages of Support Vector Machines (SVM) with factorization models. Like SVMs, FMs are a general predictor working with any real valued feature vector. In contrast to SVMs, FMs model all interactions between variables using factorized parameters. Thus they are able to estimate interactions even in problems with huge sparsity (like recommender systems) where SVMs fail. We show that the model equation of FMs can be calculated in linear time and thus FMs can be optimized directly. So unlike nonlinear SVMs, a transformation in the dual form is not necessary and the model parameters can be estimated directly without the need of any support vector in the solution. We show the relationship to SVMs and the advantages of FMs for parameter estimation in sparse settings. On the other hand there are many different factorization models like matrix factorization, parallel factor analysis or specialized models like SVD++, PITF or FPMC. The drawback of these models is that they are not applicable for general prediction tasks but work only with special input data. Furthermore their model equations and optimization algorithms are derived individually for each task. We show that FMs can mimic these models just by specifying the input data (i.e. the feature vectors). This makes FMs easily applicable even for users without expert knowledge in factorization models.},
  eventtitle = {2010 {{IEEE International Conference}} on {{Data Mining}}},
  keywords = {Computational modeling,Data models,Equations,factorization machine,Frequency modulation,Mathematical model,Predictive models,sparse data,support vector machine,Support vector machines,tensor factorization},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\DEWKBRAW\\5694074.html}
}

@inproceedings{rendleImprovingPairwiseLearning2014,
  title = {Improving Pairwise Learning for Item Recommendation from Implicit Feedback},
  booktitle = {Proceedings of the 7th {{ACM}} International Conference on {{Web}} Search and Data Mining},
  author = {Rendle, Steffen and Freudenthaler, Christoph},
  date = {2014-02-24},
  pages = {273--282},
  publisher = {{ACM}},
  location = {{New York New York USA}},
  doi = {10.1145/2556195.2556248},
  url = {https://dl.acm.org/doi/10.1145/2556195.2556248},
  urldate = {2021-09-19},
  abstract = {Pairwise algorithms are popular for learning recommender systems from implicit feedback. For each user, or more generally context, they try to discriminate between a small set of selected items and the large set of remaining (irrelevant) items. Learning is typically based on stochastic gradient descent (SGD) with uniformly drawn pairs. In this work, we show that convergence of such SGD learning algorithms slows down considerably if the item popularity has a tailed distribution. We propose a non-uniform item sampler to overcome this problem. The proposed sampler is context-dependent and oversamples informative pairs to speed up convergence. An efficient implementation with constant amortized runtime costs is developed. Furthermore, it is shown how the proposed learning algorithm can be applied to a large class of recommender models. The properties of the new learning algorithm are studied empirically on two real-world recommender system problems. The experiments indicate that the proposed adaptive sampler improves the state-of-the art learning algorithm largely in convergence without negative effects on prediction quality or iteration runtime.},
  eventtitle = {{{WSDM}} 2014: {{Seventh ACM International Conference}} on {{Web Search}} and {{Data Mining}}},
  isbn = {978-1-4503-2351-2},
  langid = {english},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\SZX3ZV2B\\Rendle and Freudenthaler - 2014 - Improving pairwise learning for item recommendatio.pdf}
}

@inproceedings{resnickGroupLensOpenArchitecture1994,
  title = {{{GroupLens}}: An Open Architecture for Collaborative Filtering of Netnews},
  shorttitle = {{{GroupLens}}},
  booktitle = {Proceedings of the 1994 {{ACM}} Conference on {{Computer}} Supported Cooperative Work  - {{CSCW}} '94},
  author = {Resnick, Paul and Iacovou, Neophytos and Suchak, Mitesh and Bergstrom, Peter and Riedl, John},
  date = {1994},
  pages = {175--186},
  publisher = {{ACM Press}},
  location = {{Chapel Hill, North Carolina, United States}},
  doi = {10.1145/192844.192905},
  url = {http://portal.acm.org/citation.cfm?doid=192844.192905},
  urldate = {2021-08-10},
  eventtitle = {The 1994 {{ACM}} Conference},
  isbn = {978-0-89791-689-9},
  langid = {english},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\GVQ9RGY2\\Resnick et al. - 1994 - GroupLens an open architecture for collaborative .pdf}
}

@article{resnickRecommenderSystems1997,
  title = {Recommender Systems},
  author = {Resnick, Paul and Varian, Hal R.},
  date = {1997-03},
  journaltitle = {Commun. ACM},
  volume = {40},
  number = {3},
  pages = {56--58},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/245108.245121},
  url = {https://dl.acm.org/doi/10.1145/245108.245121},
  urldate = {2022-02-06},
  langid = {english},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\BNMRHRGS\\Resnick and Varian - 1997 - Recommender systems.pdf}
}

@book{ricciRecommenderSystemsHandbook2011,
  title = {Recommender {{Systems Handbook}}},
  editor = {Ricci, Francesco and Rokach, Lior and Shapira, Bracha and Kantor, Paul B.},
  date = {2011},
  publisher = {{Springer US}},
  location = {{Boston, MA}},
  doi = {10.1007/978-0-387-85820-3},
  url = {http://link.springer.com/10.1007/978-0-387-85820-3},
  urldate = {2021-07-28},
  isbn = {978-0-387-85819-7 978-0-387-85820-3},
  langid = {english},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\ZJUVMYPY\\Ricci et al. - 2011 - Recommender Systems Handbook.pdf}
}

@article{riegerNaturalLanguageMitigation,
  title = {Toward {{Natural Language Mitigation Strategies}} for {{Cognitive Biases}} in {{Recommender Systems}}},
  author = {Rieger, Alisa and Theune, Mariet and Tintarev, Nava},
  pages = {5},
  abstract = {Cognitive biases in the context of consuming online information filtered by recommender systems may lead to sub-optimal choices. One approach to mitigate such biases is through interface and interaction design. This survey reviews studies focused on cognitive bias mitigation of recommender system users during two processes: 1) item selection and 2) preference elicitation. It highlights a number of promising directions for Natural Language Generation research for mitigating cognitive bias including: the need for personalization, as well as for transparency and control.},
  langid = {english},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\PAV6Y2PI\\Rieger et al. - Toward Natural Language Mitigation Strategies for .pdf}
}

@book{rosenDiscreteMathematicsIts2002,
  title = {Discrete {{Mathematics}} and {{Its Applications}}},
  author = {Rosen, Kenneth H.},
  date = {2002},
  edition = {5},
  publisher = {{McGraw-Hill Higher Education}},
  abstract = {From the Publisher: This text is designed for the sophomore/junior level introduction to discrete mathematics taken by students preparing for future coursework in areas such as math,computer science and engineering. Rosen has become a bestseller largely due to how effectively it addresses the main portion of the discrete market,which is typically characterized as the mid to upper level in rigor. The strength of Rosen's approach has been the effective balance of theory with relevant applications,as well as the overall comprehensive nature of the topic coverage.},
  isbn = {978-0-07-242434-8},
  pagetotal = {928}
}

@article{rumelhartLearningRepresentationsBackpropagating1986,
  title = {Learning Representations by Back-Propagating Errors},
  author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
  date = {1986-10},
  journaltitle = {Nature},
  volume = {323},
  number = {6088},
  pages = {533--536},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/323533a0},
  url = {https://www.nature.com/articles/323533a0},
  urldate = {2022-03-07},
  abstract = {We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal ‘hidden’ units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1.},
  issue = {6088},
  langid = {english},
  keywords = {Humanities and Social Sciences,multidisciplinary,Science},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\SJAE6ZB3\\323533a0.html}
}

@online{saleiroAequitasBiasFairness2019,
  title = {Aequitas: {{A Bias}} and {{Fairness Audit Toolkit}}},
  shorttitle = {Aequitas},
  author = {Saleiro, Pedro and Kuester, Benedict and Hinkson, Loren and London, Jesse and Stevens, Abby and Anisfeld, Ari and Rodolfa, Kit T. and Ghani, Rayid},
  date = {2019-04-29},
  eprint = {1811.05577},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1811.05577},
  urldate = {2021-08-11},
  abstract = {Recent work has raised concerns on the risk of unintended bias in AI systems being used nowadays that can affect individuals unfairly based on race, gender or religion, among other possible characteristics. While a lot of bias metrics and fairness definitions have been proposed in recent years, there is no consensus on which metric/definition should be used and there are very few available resources to operationalize them. Therefore, despite recent awareness, auditing for bias and fairness when developing and deploying AI systems is not yet a standard practice. We present Aequitas, an open source bias and fairness audit toolkit that was released in 2018 and it is an intuitive and easy to use addition to the machine learning workflow, enabling users to seamlessly test models for several bias and fairness metrics in relation to multiple population sub-groups. Aequitas facilitates informed and equitable decisions around developing and deploying algorithmic decision making systems for both data scientists, machine learning researchers and policymakers.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computers and Society,Computer Science - Machine Learning},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\2RS77IRE\\Saleiro et al. - 2019 - Aequitas A Bias and Fairness Audit Toolkit.pdf}
}

@article{samuelStudiesMachineLearning1959,
  title = {Some {{Studies}} in {{Machine Learning Using}} the {{Game}} of {{Checkers}}},
  author = {Samuel, A L},
  date = {1959},
  pages = {21},
  abstract = {Two machine-learning procedures have been investigated in some detail using the game of checkers. Enough work has been done to verify the fact that a computer can be programmed so that it will learn to playa better game of checkers than can be played by the person who wrote the program. Furthermore, it can learn to do this in a remarkably short period of time 18 or 10 hours of machine-playing time) when given only the rules of the game, a sense of direction, and a redundant and incomplete Jist of parameters which are thought to have something to do with the game, but whose correct signs and relative weights are unknown and unspecified. The principles of machine learning verified by these experiments are, of course, applicable to many other situations.},
  langid = {english},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\HYA5RJV9\\Samuel - 1959 - Some Studies in Machine Learning Using the Game of.pdf}
}

@article{samuelStudiesMachineLearning1959a,
  title = {Some Studies in Machine Learning Using the Game of Checkers},
  author = {Samuel, A. L.},
  date = {1959-07-01},
  journaltitle = {IBM J. Res. Dev.},
  volume = {3},
  number = {3},
  pages = {210--229},
  issn = {0018-8646},
  doi = {10.1147/rd.33.0210},
  url = {https://doi.org/10.1147/rd.33.0210},
  urldate = {2022-02-22},
  abstract = {Two machine-learning procedures have been investigated in some detail using the game of checkers. Enough work has been done to verify the fact that a computer can be programmed so that it will learn to play a better game of checkers than can be played by the person who wrote the program. Furthermore, it can learn to do this in a remarkably short period of time (8 or 10 hours of machine-playing time) when given only the rules of the game, a sense of direction, and a redundant and incomplete list of parameters which are thought to have something to do with the game, but whose correct signs and relative weights are unknown and unspecified. The principles of machine learning verified by these experiments are, of course, applicable to many other situations.},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\GWZP93QR\\Samuel - 1959 - Some studies in machine learning using the game of.pdf}
}

@article{sanchez-morenoExploitingUserSocial2020,
  title = {Exploiting the {{User Social Context}} to {{Address Neighborhood Bias}} in {{Collaborative Filtering Music Recommender Systems}}},
  author = {Sánchez-Moreno, Diego and López Batista, Vivian and Vicente, M. Dolores Muñoz and Sánchez Lázaro, Ángel Luis and Moreno-García, María N.},
  date = {2020-09-11},
  journaltitle = {Information},
  volume = {11},
  number = {9},
  pages = {439},
  issn = {2078-2489},
  doi = {10.3390/info11090439},
  url = {https://www.mdpi.com/2078-2489/11/9/439},
  urldate = {2021-07-28},
  abstract = {Recent research in the field of recommender systems focuses on the incorporation of social information into collaborative filtering methods to improve the reliability of recommendations. Social networks enclose valuable data regarding user behavior and connections that can be exploited in this area to infer knowledge about user preferences and social influence. The fact that streaming music platforms have some social functionalities also allows this type of information to be used for music recommendation. In this work, we take advantage of the friendship structure to address a type of recommendation bias derived from the way collaborative filtering methods compute the neighborhood. These methods restrict the rating predictions for a user to the items that have been rated by their nearest neighbors while leaving out other items that might be of his/her interest. This problem is different from the popularity bias caused by the power-law distribution of the item rating frequency (long-tail), well-known in the music domain, although both shortcomings can be related. Our proposal is based on extending and diversifying the neighborhood by capturing trust and homophily effects between users through social structure metrics. The results show an increase in potentially recommendable items while reducing recommendation error rates.},
  langid = {english},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\SHLNGH4W\\Sánchez-Moreno et al. - 2020 - Exploiting the User Social Context to Address Neig.pdf}
}

@inproceedings{santosExploitingQueryReformulations2010,
  title = {Exploiting Query Reformulations for Web Search Result Diversification},
  booktitle = {Proceedings of the 19th International Conference on {{World}} Wide Web},
  author = {Santos, Rodrygo L.T. and Macdonald, Craig and Ounis, Iadh},
  date = {2010-04-26},
  series = {{{WWW}} '10},
  pages = {881--890},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/1772690.1772780},
  url = {https://doi.org/10.1145/1772690.1772780},
  urldate = {2022-01-04},
  abstract = {When a Web user's underlying information need is not clearly specified from the initial query, an effective approach is to diversify the results retrieved for this query. In this paper, we introduce a novel probabilistic framework for Web search result diversification, which explicitly accounts for the various aspects associated to an underspecified query. In particular, we diversify a document ranking by estimating how well a given document satisfies each uncovered aspect and the extent to which different aspects are satisfied by the ranking as a whole. We thoroughly evaluate our framework in the context of the diversity task of the TREC 2009 Web track. Moreover, we exploit query reformulations provided by three major Web search engines (WSEs) as a means to uncover different query aspects. The results attest the effectiveness of our framework when compared to state-of-the-art diversification approaches in the literature. Additionally, by simulating an upper-bound query reformulation mechanism from official TREC data, we draw useful insights regarding the effectiveness of the query reformulations generated by the different WSEs in promoting diversity.},
  isbn = {978-1-60558-799-8},
  keywords = {diversity,relevance,web search},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\S2F8XAJE\\Santos et al. - 2010 - Exploiting query reformulations for web search res.pdf}
}

@inproceedings{sarwarItembasedCollaborativeFiltering2001,
  title = {Item-Based Collaborative Filtering Recommendation Algorithms},
  booktitle = {Proceedings of the Tenth International Conference on {{World Wide Web}}  - {{WWW}} '01},
  author = {Sarwar, Badrul and Karypis, George and Konstan, Joseph and Reidl, John},
  date = {2001},
  pages = {285--295},
  publisher = {{ACM Press}},
  location = {{Hong Kong, Hong Kong}},
  doi = {10.1145/371920.372071},
  url = {http://portal.acm.org/citation.cfm?doid=371920.372071},
  urldate = {2021-07-28},
  eventtitle = {The Tenth International Conference},
  isbn = {978-1-58113-348-6},
  langid = {english},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\DGPBUCLA\\Sarwar et al. - 2001 - Item-based collaborative filtering recommendation .pdf}
}

@article{scarselliGraphNeuralNetwork2009,
  title = {The {{Graph Neural Network Model}}},
  author = {Scarselli, Franco and Gori, Marco and Tsoi, Ah Chung and Hagenbuchner, Markus and Monfardini, Gabriele},
  date = {2009-01},
  journaltitle = {IEEE Transactions on Neural Networks},
  volume = {20},
  number = {1},
  pages = {61--80},
  issn = {1941-0093},
  doi = {10.1109/TNN.2008.2005605},
  abstract = {Many underlying relationships among data in several areas of science and engineering, e.g., computer vision, molecular chemistry, molecular biology, pattern recognition, and data mining, can be represented in terms of graphs. In this paper, we propose a new neural network model, called graph neural network (GNN) model, that extends existing neural network methods for processing the data represented in graph domains. This GNN model, which can directly process most of the practically useful types of graphs, e.g., acyclic, cyclic, directed, and undirected, implements a function tau(G,n) isin IRm that maps a graph G and one of its nodes n into an m-dimensional Euclidean space. A supervised learning algorithm is derived to estimate the parameters of the proposed GNN model. The computational cost of the proposed algorithm is also considered. Some experimental results are shown to validate the proposed learning algorithm, and to demonstrate its generalization capabilities.},
  eventtitle = {{{IEEE Transactions}} on {{Neural Networks}}},
  keywords = {Biological system modeling,Biology,Chemistry,Computer vision,Data engineering,Data mining,graph neural networks (GNNs),graph processing,Graphical domains,Neural networks,Parameter estimation,Pattern recognition,recursive neural networks,Supervised learning},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\AASFEGWI\\Scarselli et al. - 2009 - The Graph Neural Network Model.pdf;C\:\\Users\\Romanos\\Zotero\\storage\\993WYPA6\\4700287.html}
}

@incollection{schaferCollaborativeFilteringRecommender2007,
  title = {Collaborative {{Filtering Recommender Systems}}},
  booktitle = {The {{Adaptive Web}}: {{Methods}} and {{Strategies}} of {{Web Personalization}}},
  author = {Schafer, J. Ben and Frankowski, Dan and Herlocker, Jon and Sen, Shilad},
  editor = {Brusilovsky, Peter and Kobsa, Alfred and Nejdl, Wolfgang},
  date = {2007},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {291--324},
  publisher = {{Springer}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-72079-9_9},
  url = {https://doi.org/10.1007/978-3-540-72079-9_9},
  urldate = {2022-01-23},
  abstract = {One of the potent personalization technologies powering the adaptive web is collaborative filtering. Collaborative filtering (CF) is the process of filtering or evaluating items through the opinions of other people. CF technology brings together the opinions of large interconnected communities on the web, supporting filtering of substantial quantities of data. In this chapter we introduce the core concepts of collaborative filtering, its primary uses for users of the adaptive web, the theory and practice of CF algorithms, and design decisions regarding rating systems and acquisition of ratings. We also discuss how to evaluate CF systems, and the evolution of rich interaction interfaces. We close the chapter with discussions of the challenges of privacy particular to a CF recommendation service and important open research questions in the field.},
  isbn = {978-3-540-72079-9},
  langid = {english},
  keywords = {Association Rule Mining,Collaborative Filter,Explicit Rating,News Article,Recommender System},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\CYXXSMNY\\Schafer et al. - 2007 - Collaborative Filtering Recommender Systems.pdf}
}

@inproceedings{scheinMethodsMetricsColdstart2002,
  title = {Methods and Metrics for Cold-Start Recommendations},
  booktitle = {Proceedings of the 25th Annual International {{ACM SIGIR}} Conference on {{Research}} and Development in Information Retrieval},
  author = {Schein, Andrew I. and Popescul, Alexandrin and Ungar, Lyle H. and Pennock, David M.},
  date = {2002-08-11},
  series = {{{SIGIR}} '02},
  pages = {253--260},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/564376.564421},
  url = {https://doi.org/10.1145/564376.564421},
  urldate = {2022-01-23},
  abstract = {We have developed a method for recommending items that combines content and collaborative data under a single probabilistic framework. We benchmark our algorithm against a naïve Bayes classifier on the cold-start problem, where we wish to recommend items that no one in the community has yet rated. We systematically explore three testing methodologies using a publicly available data set, and explain how these methods apply to specific real-world applications. We advocate heuristic recommenders when benchmarking to give competent baseline performance. We introduce a new performance metric, the CROC curve, and demonstrate empirically that the various components of our testing strategy combine to obtain deeper understanding of the performance characteristics of recommender systems. Though the emphasis of our testing is on cold-start recommending, our methods for recommending and evaluation are general.},
  isbn = {978-1-58113-561-9},
  keywords = {collaborative filtering,content-based filtering,graphical models,information retrieval,performance evaluation,recommender systems},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\9LGGCMRN\\Schein et al. - 2002 - Methods and metrics for cold-start recommendations.pdf}
}

@article{selimiUseRecommenderSystems,
  title = {The Use of {{Recommender Systems}} in Web Technology and an In-Depth Analysis of {{Cold State}} Problem},
  author = {Selimi, Denis and Nuci, Krenare PIREVA},
  pages = {24},
  abstract = {In the WWW (World Wide Web), dynamic development and spread of data has resulted a tremendous amount of information available on the Internet, yet user is unable to find relevant information in a short span of time. Consequently, a system called recommendation system developed to help users find their infromation with ease through their browsing activities. In other words, recommender systems are tools for interacting with large amount of information that provide personalized view for prioritizing items likely to be of keen for users. They have developed over the years in artificial intelligence techniques that include machine learning and data mining amongst many to mention. Furthermore, the recommendation systems have personalized on an e-commerce, on-line applications such as Amazon.com, Netflix, and Booking.com. As a result, this has inspired many researchers to extend the reach of recommendation systems into new sets of challenges and problem areas that are yet to be truly solved, primarily a problem with the case of making a recommendation to a new user that is called cold-state (i.e. cold-start) user problem where the new user might likely not yield much of information searched. Therfore, the purpose of this paper is to tackle the said cold-start problem with a few effecient methods and challenges, as well as identify and overview the current state of recommendation system as a whole.},
  langid = {english},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\TJS4TTW7\\Selimi and Nuci - The use of Recommender Systems in web technology a.pdf}
}

@incollection{shaniEvaluatingRecommendationSystems2011,
  title = {Evaluating {{Recommendation Systems}}},
  booktitle = {Recommender {{Systems Handbook}}},
  author = {Shani, Guy and Gunawardana, Asela},
  editor = {Ricci, Francesco and Rokach, Lior and Shapira, Bracha and Kantor, Paul B.},
  date = {2011},
  pages = {257--297},
  publisher = {{Springer US}},
  location = {{Boston, MA}},
  doi = {10.1007/978-0-387-85820-3_8},
  url = {http://link.springer.com/10.1007/978-0-387-85820-3_8},
  urldate = {2021-07-28},
  abstract = {Recommender systems are now popular both commercially and in the research community, where many approaches have been suggested for providing recommendations. In many cases a system designer that wishes to employ a recommendation system must choose between a set of candidate approaches. A first step towards selecting an appropriate algorithm is to decide which properties of the application to focus upon when making this choice. Indeed, recommendation systems have a variety of properties that may affect user experience, such as accuracy, robustness, scalability, and so forth. In this paper we discuss how to compare recommenders based on a set of properties that are relevant for the application. We focus on comparative studies, where a few algorithms are compared using some evaluation metric, rather than absolute benchmarking of algorithms. We describe experimental settings appropriate for making choices between algorithms. We review three types of experiments, starting with an offline setting, where recommendation approaches are compared without user interaction, then reviewing user studies, where a small group of subjects experiment with the system and report on the experience, and finally describe large scale online experiments, where real user populations interact with the system. In each of these cases we describe types of questions that can be answered, and suggest protocols for experimentation. We also discuss how to draw trustworthy conclusions from the conducted experiments. We then review a large set of properties, and explain how to evaluate systems given relevant properties. We also survey a large set of evaluation metrics in the context of the property that they evaluate.},
  isbn = {978-0-387-85819-7 978-0-387-85820-3},
  langid = {english},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\N4EMVNHC\\Shani and Gunawardana - 2011 - Evaluating Recommendation Systems.pdf}
}

@online{sonboliWinnerDynamicLotteries2020,
  title = {"{{And}} the {{Winner Is}}...": {{Dynamic Lotteries}} for {{Multi-group Fairness-Aware Recommendation}}},
  shorttitle = {"{{And}} the {{Winner Is}}..."},
  author = {Sonboli, Nasim and Burke, Robin and Mattei, Nicholas and Eskandanian, Farzad and Gao, Tian},
  date = {2020-09-05},
  eprint = {2009.02590},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2009.02590},
  urldate = {2021-07-28},
  abstract = {As recommender systems are being designed and deployed for an increasing number of socially-consequential applications, it has become important to consider what properties of fairness these systems exhibit. There has been considerable research on recommendation fairness. However, we argue that the previous literature has been based on simple, uniform and often uni-dimensional notions of fairness assumptions that do not recognize the real-world complexities of fairness-aware applications. In this paper, we explicitly represent the design decisions that enter into the trade-off between accuracy and fairness across multiply-defined and intersecting protected groups, supporting multiple fairness metrics. The framework also allows the recommender to adjust its performance based on the historical view of recommendations that have been delivered over a time horizon, dynamically rebalancing between fairness concerns. Within this framework, we formulate lottery-based mechanisms for choosing between fairness concerns, and demonstrate their performance in two recommendation domains.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Information Retrieval,Computer Science - Machine Learning},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\YNPJ86UE\\Sonboli et al. - 2020 - And the Winner Is... Dynamic Lotteries for Mult.pdf}
}

@inproceedings{steckCalibratedRecommendations2018,
  title = {Calibrated Recommendations},
  booktitle = {Proceedings of the 12th {{ACM Conference}} on {{Recommender Systems}}},
  author = {Steck, Harald},
  date = {2018-09-27},
  pages = {154--162},
  publisher = {{ACM}},
  location = {{Vancouver British Columbia Canada}},
  doi = {10.1145/3240323.3240372},
  url = {https://dl.acm.org/doi/10.1145/3240323.3240372},
  urldate = {2021-07-28},
  abstract = {When a user has watched, say, 70 romance movies and 30 action movies, then it is reasonable to expect the personalized list of recommended movies to be comprised of about 70\% romance and 30\% action movies as well. This important property is known as calibration, and recently received renewed attention in the context of fairness in machine learning. In the recommended list of items, calibration ensures that the various (past) areas of interest of a user are reflected with their corresponding proportions. Calibration is especially important in light of the fact that recommender systems optimized toward accuracy (e.g., ranking metrics) in the usual offline-setting can easily lead to recommendations where the lesser interests of a user get crowded out by the user’s main interests–which we show empirically as well as in thought-experiments. This can be prevented by calibrated recommendations. To this end, we outline metrics for quantifying the degree of calibration, as well as a simple yet effective re-ranking algorithm for post-processing the output of recommender systems.},
  eventtitle = {{{RecSys}} '18: {{Twelfth ACM Conference}} on {{Recommender Systems}}},
  isbn = {978-1-4503-5901-6},
  langid = {english},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\2MY4GEIP\\Steck - 2018 - Calibrated recommendations.pdf}
}

@online{StreamlitFastestWay,
  title = {Streamlit • {{The}} Fastest Way to Build and Share Data Apps},
  url = {https://streamlit.io/},
  urldate = {2021-11-09},
  abstract = {Streamlit is an open-source app framework for Machine Learning and Data Science teams. Create beautiful data apps in hours, not weeks. All in pure Python. All for free.},
  langid = {english},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\BB5IGS3L\\streamlit.io.html}
}

@inproceedings{sunAreWeEvaluating2020,
  title = {Are {{We Evaluating Rigorously}}? {{Benchmarking Recommendation}} for {{Reproducible Evaluation}} and {{Fair Comparison}}},
  shorttitle = {Are {{We Evaluating Rigorously}}?},
  booktitle = {Fourteenth {{ACM Conference}} on {{Recommender Systems}}},
  author = {Sun, Zhu and Yu, Di and Fang, Hui and Yang, Jie and Qu, Xinghua and Zhang, Jie and Geng, Cong},
  date = {2020-09-22},
  pages = {23--32},
  publisher = {{ACM}},
  location = {{Virtual Event Brazil}},
  doi = {10.1145/3383313.3412489},
  url = {https://dl.acm.org/doi/10.1145/3383313.3412489},
  urldate = {2021-07-28},
  abstract = {With tremendous amount of recommendation algorithms proposed every year, one critical issue has attracted a considerable amount of attention: there are no effective benchmarks for evaluation, which leads to two major concerns, i.e., unreproducible evaluation and unfair comparison. This paper aims to conduct rigorous (i.e., reproducible and fair) evaluation for implicit-feedback based top-N recommendation algorithms. We first systematically review 85 recommendation papers published at eight top-tier conferences (e.g., RecSys, SIGIR) to summarize important evaluation factors, e.g., data splitting and parameter tuning strategies, etc. Through a holistic empirical study, the impacts of different factors on recommendation performance are then analyzed in-depth. Following that, we create benchmarks with standardized procedures and provide the performance of seven well-tuned state-of-the-arts across six metrics on six widely-used datasets as a reference for later study. Additionally, we release a user-friendly Python toolkit, which differs from existing ones in addressing the broad scope of rigorous evaluation for recommendation. Overall, our work sheds light on the issues in recommendation evaluation and lays the foundation for further investigation. Our code and datasets are available at GitHub (https://github.com/AmazingDD/daisyRec).},
  eventtitle = {{{RecSys}} '20: {{Fourteenth ACM Conference}} on {{Recommender Systems}}},
  isbn = {978-1-4503-7583-2},
  langid = {english},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\EEJQN7RR\\Sun et al. - 2020 - Are We Evaluating Rigorously Benchmarking Recomme.pdf}
}

@book{sunsteinEchoChambersBush2001,
  title = {Echo {{Chambers}}: {{Bush V}}. {{Gore}}, {{Impeachment}}, and {{Beyond}}},
  shorttitle = {Echo {{Chambers}}},
  author = {Sunstein, Cass R.},
  date = {2001},
  eprint = {sEgHAAAACAAJ},
  eprinttype = {googlebooks},
  publisher = {{Princeton University Press}},
  isbn = {978-1-4008-0905-9},
  langid = {english},
  pagetotal = {34}
}

@online{suryamattuMachineBias2016,
  title = {Machine {{Bias}}},
  author = {Surya Mattu and Julia Angwin and Jeff Larson and Lauren Kirchner},
  date = {2016},
  url = {https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing?token=Gg58888u2U5db3W3CsuKrD0LD_VQJReQ},
  urldate = {2021-08-19},
  abstract = {There’s software used across the country to predict future criminals. And it’s biased against blacks.},
  langid = {english},
  organization = {{ProPublica}},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\XY5YZUP3\\machine-bias-risk-assessments-in-criminal-sentencing.html}
}

@inproceedings{takacsAlternatingLeastSquares2012,
  title = {Alternating Least Squares for Personalized Ranking},
  booktitle = {Proceedings of the Sixth {{ACM}} Conference on {{Recommender}} Systems - {{RecSys}} '12},
  author = {Takács, Gábor and Tikk, Domonkos},
  date = {2012},
  pages = {83},
  publisher = {{ACM Press}},
  location = {{Dublin, Ireland}},
  doi = {10.1145/2365952.2365972},
  url = {http://dl.acm.org/citation.cfm?doid=2365952.2365972},
  urldate = {2021-07-28},
  abstract = {Two flavors of the recommendation problem are the explicit and the implicit feedback settings. In the explicit feedback case, users rate items and the user–item preference relationship can be modelled on the basis of the ratings. In the harder but more common implicit feedback case, the system has to infer user preferences from indirect information: presence or absence of events, such as a user viewed an item. One approach for handling implicit feedback is to minimize a ranking objective function instead of the conventional prediction mean squared error. The naive minimization of a ranking objective function is typically expensive. This difficulty is usually overcome by a trade-off: sacrificing the accuracy to some extent for computational efficiency by sampling the objective function. In this paper, we present a computationally effective approach for the direct minimization of a ranking objective function, without sampling. We demonstrate by experiments on the Y!Music and Netflix data sets that the proposed method outperforms other implicit feedback recommenders in many cases in terms of the ErrorRate, ARP and Recall evaluation metrics.},
  eventtitle = {The Sixth {{ACM}} Conference},
  isbn = {978-1-4503-1270-7},
  langid = {english},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\QQVEKJU4\\Takács and Tikk - 2012 - Alternating least squares for personalized ranking.pdf}
}

@article{tibshiraniValeriePatrickHastie,
  title = {Valerie and {{Patrick Hastie}}},
  author = {Tibshirani, Sami and Friedman, Harry},
  pages = {764},
  langid = {english},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\7WXXRNPM\\Tibshirani and Friedman - Valerie and Patrick Hastie.pdf}
}

@inproceedings{vargasRankRelevanceNovelty2011,
  title = {Rank and Relevance in Novelty and Diversity Metrics for Recommender Systems},
  booktitle = {Proceedings of the Fifth {{ACM}} Conference on {{Recommender}} Systems - {{RecSys}} '11},
  author = {Vargas, Saúl and Castells, Pablo},
  date = {2011},
  pages = {109},
  publisher = {{ACM Press}},
  location = {{Chicago, Illinois, USA}},
  doi = {10.1145/2043932.2043955},
  url = {http://dl.acm.org/citation.cfm?doid=2043932.2043955},
  urldate = {2021-07-28},
  abstract = {The Recommender Systems community is paying increasing attention to novelty and diversity as key qualities beyond accuracy in real recommendation scenarios. Despite the raise of interest and work on the topic in recent years, we find that a clear common methodological and conceptual ground for the evaluation of these dimensions is still to be consolidated. Different evaluation metrics have been reported in the literature but the precise relation, distinction or equivalence between them has not been explicitly studied. Furthermore, the metrics reported so far miss important properties such as taking into consideration the ranking of recommended items, or whether items are relevant or not, when assessing the novelty and diversity of recommendations.},
  eventtitle = {The Fifth {{ACM}} Conference},
  isbn = {978-1-4503-0683-6},
  langid = {english},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\D3BT2GL3\\Vargas and Castells - 2011 - Rank and relevance in novelty and diversity metric.pdf}
}

@online{vermaFacetsFairnessSearch2020,
  title = {Facets of {{Fairness}} in {{Search}} and {{Recommendation}}},
  author = {Verma, Sahil and Gao, Ruoyuan and Shah, Chirag},
  date = {2020-07-16},
  eprint = {2008.01194},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2008.01194},
  urldate = {2021-07-28},
  abstract = {Several recent works have highlighted how search and recommender systems exhibit bias along different dimensions. Counteracting this bias and bringing a certain amount of fairness in search is crucial to not only creating a more balanced environment that considers relevance and diversity but also providing a more sustainable way forward for both content consumers and content producers. This short paper examines some of the recent works to define relevance, diversity, and related concepts. Then, it focuses on explaining the emerging concept of fairness in various recommendation settings. In doing so, this paper presents comparisons and highlights contracts among various measures, and gaps in our conceptual and evaluative frameworks.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computers and Society,Computer Science - Information Retrieval},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\A48WI6DS\\Verma et al. - 2020 - Facets of Fairness in Search and Recommendation.pdf}
}

@inproceedings{wangCombatingSelectionBiases2021,
  title = {Combating {{Selection Biases}} in {{Recommender Systems}} with a {{Few Unbiased Ratings}}},
  booktitle = {Proceedings of the 14th {{ACM International Conference}} on {{Web Search}} and {{Data Mining}}},
  author = {Wang, Xiaojie and Zhang, Rui and Sun, Yu and Qi, Jianzhong},
  date = {2021-03-08},
  pages = {427--435},
  publisher = {{ACM}},
  location = {{Virtual Event Israel}},
  doi = {10.1145/3437963.3441799},
  url = {https://dl.acm.org/doi/10.1145/3437963.3441799},
  urldate = {2021-07-28},
  abstract = {Recommendation datasets are prone to selection biases due to selfselection behavior of users and item selection process of systems. This makes explicitly combating selection biases an essential problem in training recommender systems. Most previous studies assume no unbiased data available for training. We relax this assumption and assume that a small subset of training data is unbiased. Then, we propose a novel objective that utilizes the unbiased data to adaptively assign propensity weights to biased training ratings. This objective, combined with unbiased performance estimators, alleviates the effects of selection biases on the training of recommender systems. To optimize the objective, we propose an efficient algorithm that minimizes the variance of propensity estimates for better generalized recommender systems. Extensive experiments on two real-world datasets confirm the advantages of our approach in significantly reducing both the error of rating prediction and the variance of propensity estimation.},
  eventtitle = {{{WSDM}} '21: {{The Fourteenth ACM International Conference}} on {{Web Search}} and {{Data Mining}}},
  isbn = {978-1-4503-8297-7},
  langid = {english},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\JC4UW3JC\\Wang et al. - 2021 - Combating Selection Biases in Recommender Systems .pdf}
}

@inproceedings{wangNeuralGraphCollaborative2019,
  title = {Neural {{Graph Collaborative Filtering}}},
  booktitle = {Proceedings of the 42nd {{International ACM SIGIR Conference}} on {{Research}} and {{Development}} in {{Information Retrieval}}},
  author = {Wang, Xiang and He, Xiangnan and Wang, Meng and Feng, Fuli and Chua, Tat-Seng},
  date = {2019-07-18},
  pages = {165--174},
  publisher = {{ACM}},
  location = {{Paris France}},
  doi = {10.1145/3331184.3331267},
  url = {https://dl.acm.org/doi/10.1145/3331184.3331267},
  urldate = {2021-11-05},
  abstract = {Learning vector representations (aka. embeddings) of users and items lies at the core of modern recommender systems. Ranging from early matrix factorization to recently emerged deep learning based methods, existing efforts typically obtain a user’s (or an item’s) embedding by mapping from pre-existing features that describe the user (or the item), such as ID and attributes. We argue that an inherent drawback of such methods is that, the collaborative signal, which is latent in user-item interactions, is not encoded in the embedding process. As such, the resultant embeddings may not be sufficient to capture the collaborative filtering effect.},
  eventtitle = {{{SIGIR}} '19: {{The}} 42nd {{International ACM SIGIR Conference}} on {{Research}} and {{Development}} in {{Information Retrieval}}},
  isbn = {978-1-4503-6172-9},
  langid = {english},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\YTUBCTS4\\Wang et al. - 2019 - Neural Graph Collaborative Filtering.pdf}
}

@inproceedings{xiaoFairnessAwareGroupRecommendation2017,
  title = {Fairness-{{Aware Group Recommendation}} with {{Pareto-Efficiency}}},
  booktitle = {Proceedings of the {{Eleventh ACM Conference}} on {{Recommender Systems}}},
  author = {Xiao, Lin and Min, Zhang and Yongfeng, Zhang and Zhaoquan, Gu and Yiqun, Liu and Shaoping, Ma},
  date = {2017-08-27},
  pages = {107--115},
  publisher = {{ACM}},
  location = {{Como Italy}},
  doi = {10.1145/3109859.3109887},
  url = {https://dl.acm.org/doi/10.1145/3109859.3109887},
  urldate = {2021-07-28},
  abstract = {Group recommendation has attracted significant research efforts for its importance in benefiting a group of users. This paper investigates the Group Recommendation problem from a novel aspect, which tries to maximize the satisfaction of each group member while minimizing the unfairness between them. In this work, we present several semantics of the individual utility and propose two concepts of social welfare and fairness for modeling the overall utilities and the balance between group members. We formulate the problem as a multiple objective optimization problem and show that it is NPHard in different semantics. Given the multiple-objective nature of fairness-aware group recommendation problem, we provide an optimization framework for fairness-aware group recommendation from the perspective of Pareto Efficiency. We conduct extensive experiments on real-world datasets and evaluate our algorithm in terms of standard accuracy metrics. The results indicate that our algorithm achieves superior performances and considering fairness in group recommendation can enhance the recommendation accuracy.},
  eventtitle = {{{RecSys}} '17: {{Eleventh ACM Conference}} on {{Recommender Systems}}},
  isbn = {978-1-4503-4652-8},
  langid = {english},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\ZUI46TAP\\Xiao et al. - 2017 - Fairness-Aware Group Recommendation with Pareto-Ef.pdf}
}

@article{yalcinInvestigatingCounteractingPopularity2021,
  title = {Investigating and Counteracting Popularity Bias in Group Recommendations},
  author = {Yalcin, Emre and Bilge, Alper},
  date = {2021-09},
  journaltitle = {Information Processing \& Management},
  volume = {58},
  number = {5},
  pages = {102608},
  issn = {03064573},
  doi = {10.1016/j.ipm.2021.102608},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0306457321001047},
  urldate = {2021-07-29},
  abstract = {Popularity bias is an undesirable phenomenon associated with recommendation algorithms where popular items tend to be suggested over long-tail ones, even if the latter would be of reasonable interest for individuals. Such intrinsic tendencies of the recommenders may lead to producing ranked lists, in which items are not equally covered along the popularity tail. Although some recent studies aim to detect such biases of traditional algorithms and treat their effects on recommendations, the concept of popularity bias remains elusive for group recommender systems. Therefore, in this study, we focus on investigating popularity bias from the view of group recommender systems, which aggregate individual preferences to achieve recommendations for groups of users. We analyze various state-of-the-art aggregation techniques utilized in group recommender systems regarding their bias towards popular items. To counteract possible popularity issues in group recommendations, we adapt a traditional re-ranking approach that weighs items inversely proportional to their popularity within a group. Also, we propose a novel popularity bias mitigation procedure that re-ranks items by incorporating their popularity level and estimated group ratings in two distinct strategies. The first one aims to penalize popular items during the aggregation process highly and avoids bias better, while the second one puts more emphasis on group ratings than popularity and achieves a more balanced performance regarding conflicting goals of mitigating bias and boosting accuracy. Experiments performed on four real-world benchmark datasets demonstrate that both strategies are more efficient than the adapted approach, and empowering aggregation techniques with one of these strategies significantly decreases their bias towards popular items while maintaining reasonable ranking accuracy.},
  langid = {english},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\BU7J4IMY\\Yalcin and Bilge - 2021 - Investigating and counteracting popularity bias in.pdf}
}

@online{yaoNewFairnessMetrics2017,
  title = {New {{Fairness Metrics}} for {{Recommendation}} That {{Embrace Differences}}},
  author = {Yao, Sirui and Huang, Bert},
  date = {2017-12-13},
  eprint = {1706.09838},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1706.09838},
  urldate = {2021-07-28},
  abstract = {We study fairness in collaborative-filtering recommender systems, which are sensitive to discrimination that exists in historical data. Biased data can lead collaborative filtering methods to make unfair predictions against minority groups of users. We identify the insufficiency of existing fairness metrics and propose four new metrics that address different forms of unfairness. These fairness metrics can be optimized by adding fairness terms to the learning objective. Experiments on synthetic and real data show that our new metrics can better measure fairness than the baseline, and that the fairness objectives effectively help reduce unfairness.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computers and Society},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\WASZ6GLE\\Yao and Huang - 2017 - New Fairness Metrics for Recommendation that Embra.pdf}
}

@online{yaoParityFairnessObjectives2017,
  title = {Beyond {{Parity}}: {{Fairness Objectives}} for {{Collaborative Filtering}}},
  shorttitle = {Beyond {{Parity}}},
  author = {Yao, Sirui and Huang, Bert},
  date = {2017-11-30},
  eprint = {1705.08804},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1705.08804},
  urldate = {2021-07-28},
  abstract = {We study fairness in collaborative-filtering recommender systems, which are sensitive to discrimination that exists in historical data. Biased data can lead collaborative-filtering methods to make unfair predictions for users from minority groups. We identify the insufficiency of existing fairness metrics and propose four new metrics that address different forms of unfairness. These fairness metrics can be optimized by adding fairness terms to the learning objective. Experiments on synthetic and real data show that our new metrics can better measure fairness than the baseline, and that the fairness objectives effectively help reduce unfairness.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Information Retrieval,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\8QNC5AVT\\Yao and Huang - 2017 - Beyond Parity Fairness Objectives for Collaborati.pdf}
}

@online{yinChallengingLongTail2012,
  title = {Challenging the {{Long Tail Recommendation}}},
  author = {Yin, Hongzhi and Cui, Bin and Li, Jing and Yao, Junjie and Chen, Chen},
  date = {2012-05-30},
  eprint = {1205.6700},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1205.6700},
  urldate = {2021-11-09},
  abstract = {The success of "infinite-inventory" retailers such as Amazon.com and Netflix has been largely attributed to a "long tail" phenomenon. Although the majority of their inventory is not in high demand, these niche products, unavailable at limited-inventory competitors, generate a significant fraction of total revenue in aggregate. In addition, tail product availability can boost head sales by offering consumers the convenience of "one-stop shopping" for both their mainstream and niche tastes. However, most of existing recommender systems, especially collaborative filter based methods, can not recommend tail products due to the data sparsity issue. It has been widely acknowledged that to recommend popular products is easier yet more trivial while to recommend long tail products adds more novelty yet it is also a more challenging task. In this paper, we propose a novel suite of graph-based algorithms for the long tail recommendation. We first represent user-item information with undirected edge-weighted graph and investigate the theoretical foundation of applying Hitting Time algorithm for long tail item recommendation. To improve recommendation diversity and accuracy, we extend Hitting Time and propose efficient Absorbing Time algorithm to help users find their favorite long tail items. Finally, we refine the Absorbing Time algorithm and propose two entropy-biased Absorbing Cost algorithms to distinguish the variation on different user-item rating pairs, which further enhances the effectiveness of long tail recommendation. Empirical experiments on two real life datasets show that our proposed algorithms are effective to recommend long tail items and outperform state-of-the-art recommendation techniques.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Databases},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\4EBS5NTM\\Yin et al. - 2012 - Challenging the Long Tail Recommendation.pdf;C\:\\Users\\Romanos\\Zotero\\storage\\456B3UJ8\\1205.html}
}

@article{yinChallengingLongTail2012a,
  title = {Challenging the Long Tail Recommendation},
  author = {Yin, Hongzhi and Cui, Bin and Li, Jing and Yao, Junjie and Chen, Chen},
  date = {2012-05-01},
  journaltitle = {Proc. VLDB Endow.},
  volume = {5},
  number = {9},
  pages = {896--907},
  issn = {2150-8097},
  doi = {10.14778/2311906.2311916},
  url = {https://doi.org/10.14778/2311906.2311916},
  urldate = {2022-02-22},
  abstract = {The success of "infinite-inventory" retailers such as Amazon.com and Netflix has been largely attributed to a "long tail" phenomenon. Although the majority of their inventory is not in high demand, these niche products, unavailable at limited-inventory competitors, generate a significant fraction of total revenue in aggregate. In addition, tail product availability can boost head sales by offering consumers the convenience of "one-stop shopping" for both their mainstream and niche tastes. However, most of existing recommender systems, especially collaborative filter based methods, can not recommend tail products due to the data sparsity issue. It has been widely acknowledged that to recommend popular products is easier yet more trivial while to recommend long tail products adds more novelty yet it is also a more challenging task. In this paper, we propose a novel suite of graph-based algorithms for the long tail recommendation. We first represent user-item information with undirected edge-weighted graph and investigate the theoretical foundation of applying Hitting Time algorithm for long tail item recommendation. To improve recommendation diversity and accuracy, we extend Hitting Time and propose efficient Absorbing Time algorithm to help users find their favorite long tail items. Finally, we refine the Absorbing Time algorithm and propose two entropy-biased Absorbing Cost algorithms to distinguish the variation on different user-item rating pairs, which further enhances the effectiveness of long tail recommendation. Empirical experiments on two real life datasets show that our proposed algorithms are effective to recommend long tail items and outperform state-of-the-art recommendation techniques.},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\AH3N8F2S\\Yin et al. - 2012 - Challenging the long tail recommendation.pdf}
}

@article{zehlikeFAIRFair2017,
  title = {{{FA}}*{{IR}}: {{A Fair Top-k Ranking Algorithm}}},
  shorttitle = {{{FA}}*{{IR}}},
  author = {Zehlike, Meike and Bonchi, Francesco and Castillo, Carlos and Hajian, Sara and Megahed, Mohamed and Baeza-Yates, Ricardo},
  date = {2017-11-06},
  journaltitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
  eprint = {1706.06368},
  eprinttype = {arxiv},
  pages = {1569--1578},
  doi = {10.1145/3132847.3132938},
  url = {http://arxiv.org/abs/1706.06368},
  urldate = {2021-07-28},
  abstract = {In this work, we de ne and solve the Fair Top-k Ranking problem, in which we want to determine a subset of k candidates from a large pool of n k candidates, maximizing utility (i.e., select the “best” candidates) subject to group fairness criteria.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computers and Society,Computer Science - Information Retrieval,H.3.3,J.1},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\XJI3PW22\\Zehlike et al. - 2017 - FAIR A Fair Top-k Ranking Algorithm.pdf}
}

@online{zehlikeFairnessRankingSurvey2021,
  title = {Fairness in {{Ranking}}: {{A Survey}}},
  shorttitle = {Fairness in {{Ranking}}},
  author = {Zehlike, Meike and Yang, Ke and Stoyanovich, Julia},
  date = {2021-05-12},
  eprint = {2103.14000},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2103.14000},
  urldate = {2021-07-29},
  abstract = {In the past few years, there has been much work on incorporating fairness requirements into algorithmic rankers, with contributions coming from the data management, algorithms, information retrieval, and recommender systems communities. In this survey we give a systematic overview of this work, offering a broad perspective that connects formalizations and algorithmic approaches across subfields. An important contribution of our work is in developing a common narrative around the value frameworks that motivate specific fairness-enhancing interventions in ranking. This allows us to unify the presentation of mitigation objectives and of algorithmic techniques to help meet those objectives or identify trade-offs.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Databases,Computer Science - Information Retrieval},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\RZFNL4KZ\\Zehlike et al. - 2021 - Fairness in Ranking A Survey.pdf}
}

@article{zhangCausalInterventionLeveraging2021,
  title = {Causal {{Intervention}} for {{Leveraging Popularity Bias}} in {{Recommendation}}},
  author = {Zhang, Yang and Feng, Fuli and He, Xiangnan and Wei, Tianxin and Song, Chonggang and Ling, Guohui and Zhang, Yongdong},
  date = {2021-07-11},
  journaltitle = {Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  eprint = {2105.06067},
  eprinttype = {arxiv},
  pages = {11--20},
  doi = {10.1145/3404835.3462875},
  url = {http://arxiv.org/abs/2105.06067},
  urldate = {2021-07-28},
  abstract = {Recommender system usually faces popularity bias issues: from the data perspective, items exhibit uneven (usually long-tail) distribution on the interaction frequency; from the method perspective, collaborative filtering methods are prone to amplify the bias by over-recommending popular items. It is undoubtedly critical to consider popularity bias in recommender systems, and existing work mainly eliminates the bias effect with propensity-based unbiased learning or causal embeddings. However, we argue that not all biases in the data are bad, i.e., some items demonstrate higher popularity because of their better intrinsic quality. Blindly pursuing unbiased learning may remove the beneficial patterns in the data, degrading the recommendation accuracy and user satisfaction.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Information Retrieval},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\XGTTFM6S\\Zhang et al. - 2021 - Causal Intervention for Leveraging Popularity Bias.pdf}
}

@article{zhangDeepLearningBased2019,
  title = {Deep {{Learning Based Recommender System}}: {{A Survey}} and {{New Perspectives}}},
  shorttitle = {Deep {{Learning Based Recommender System}}},
  author = {Zhang, Shuai and Yao, Lina and Sun, Aixin and Tay, Yi},
  date = {2019-02-28},
  journaltitle = {ACM Comput. Surv.},
  volume = {52},
  number = {1},
  pages = {1--38},
  issn = {0360-0300, 1557-7341},
  doi = {10.1145/3285029},
  url = {https://dl.acm.org/doi/10.1145/3285029},
  urldate = {2021-11-12},
  abstract = {With the growing volume of online information, recommender systems have been an effective strategy to overcome information overload. The utility of recommender systems cannot be overstated, given their widespread adoption in many web applications, along with their potential impact to ameliorate many problems related to over-choice. In recent years, deep learning has garnered considerable interest in many research fields such as computer vision and natural language processing, owing not only to stellar performance but also to the attractive property of learning feature representations from scratch. The influence of deep learning is also pervasive, recently demonstrating its effectiveness when applied to information retrieval and recommender systems research. The field of deep learning in recommender system is flourishing. This article aims to provide a comprehensive review of recent research efforts on deep learning-based recommender systems. More concretely, we provide and devise a taxonomy of deep learning-based recommendation models, along with a comprehensive summary of the state of the art. Finally, we expand on current trends and provide new perspectives pertaining to this new and exciting development of the field.},
  langid = {english},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\95WVF6GV\\Zhang et al. - 2019 - Deep Learning Based Recommender System A Survey a.pdf}
}

@inproceedings{zhangDemographicsShouldNot2020,
  title = {Demographics {{Should Not Be}} the {{Reason}} of {{Toxicity}}: {{Mitigating Discrimination}} in {{Text Classifications}} with {{Instance Weighting}}},
  shorttitle = {Demographics {{Should Not Be}} the {{Reason}} of {{Toxicity}}},
  booktitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Zhang, Guanhua and Bai, Bing and Zhang, Junqi and Bai, Kun and Zhu, Conghui and Zhao, Tiejun},
  date = {2020},
  pages = {4134--4145},
  publisher = {{Association for Computational Linguistics}},
  location = {{Online}},
  doi = {10.18653/v1/2020.acl-main.380},
  url = {https://www.aclweb.org/anthology/2020.acl-main.380},
  urldate = {2021-07-29},
  abstract = {With the recent proliferation of the use of text classifications, researchers have found that there are certain unintended biases in text classification datasets. For example, texts containing some demographic identity-terms (e.g., “gay”, “black”) are more likely to be abusive in existing abusive language detection datasets. As a result, models trained with these datasets may consider sentences like “She makes me happy to be gay” as abusive simply because of the word “gay.” In this paper, we formalize the unintended biases in text classification datasets as a kind of selection bias from the non-discrimination distribution to the discrimination distribution. Based on this formalization, we further propose a model-agnostic debiasing training framework by recovering the non-discrimination distribution using instance weighting, which does not require any extra resources or annotations apart from a pre-defined set of demographic identity-terms. Experiments demonstrate that our method can effectively alleviate the impacts of the unintended biases without significantly hurting models’ generalization ability.},
  eventtitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  langid = {english},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\NQXLLC5C\\Zhang et al. - 2020 - Demographics Should Not Be the Reason of Toxicity.pdf}
}

@article{zhangExplainableRecommendationSurvey2020,
  title = {Explainable {{Recommendation}}: {{A Survey}} and {{New Perspectives}}},
  shorttitle = {Explainable {{Recommendation}}},
  author = {Zhang, Yongfeng and Chen, Xu},
  date = {2020},
  journaltitle = {FNT in Information Retrieval},
  volume = {14},
  number = {1},
  pages = {1--101},
  issn = {1554-0669, 1554-0677},
  doi = {10.1561/1500000066},
  url = {http://www.nowpublishers.com/article/Details/INR-066},
  urldate = {2021-07-28},
  langid = {english},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\QL6GA554\\Zhang and Chen - 2020 - Explainable Recommendation A Survey and New Persp.pdf}
}

@online{zhangMetricFactorizationRecommendation2018,
  title = {Metric {{Factorization}}: {{Recommendation}} beyond {{Matrix Factorization}}},
  shorttitle = {Metric {{Factorization}}},
  author = {Zhang, Shuai and Yao, Lina and Tay, Yi and Xu, Xiwei and Zhang, Xiang and Zhu, Liming},
  date = {2018-06-04},
  eprint = {1802.04606},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1802.04606},
  urldate = {2021-07-28},
  abstract = {In the past decade, matrix factorization has been extensively researched and has become one of the most popular techniques for personalized recommendations. Nevertheless, the dot product adopted in matrix factorization based recommender models does not satisfy the inequality property, which may limit their expressiveness and lead to sub-optimal solutions. To overcome this problem, we propose a novel recommender technique dubbed as Metric Factorization. We assume that users and items can be placed in a low dimensional space and their explicit closeness can be measured using Euclidean distance which satisfies the inequality property. To demonstrate its effectiveness, we further designed two variants of metric factorization with one for rating estimation and the other for personalized item ranking. Extensive experiments on a number of real-world datasets show that our approach outperforms existing state-of-the-art by a large margin on both rating prediction and item ranking tasks.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Information Retrieval},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\KGI9WSVE\\Zhang et al. - 2018 - Metric Factorization Recommendation beyond Matrix.pdf}
}

@online{zhengDisentanglingUserInterest2021,
  title = {Disentangling {{User Interest}} and {{Conformity}} for {{Recommendation}} with {{Causal Embedding}}},
  author = {Zheng, Yu and Gao, Chen and Li, Xiang and He, Xiangnan and Jin, Depeng and Li, Yong},
  date = {2021-02-19},
  eprint = {2006.11011},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2006.11011},
  urldate = {2021-07-28},
  abstract = {Recommendation models are usually trained on observational interaction data. However, observational interaction data could result from users’ conformity towards popular items, which entangles users’ real interest. Existing methods tracks this problem as eliminating popularity bias, e.g., by re-weighting training samples or leveraging a small fraction of unbiased data. However, the variety of user conformity is ignored by these approaches, and different causes of an interaction are bundled together as unified representations, hence robustness and interpretability are not guaranteed when underlying causes are changing. In this paper, we present DICE, a general framework that learns representations where interest and conformity are structurally disentangled, and various backbone recommendation models could be smoothly integrated. We assign users and items with separate embeddings for interest and conformity, and make each embedding capture only one cause by training with cause-specific data which is obtained according to the colliding effect of causal inference. Our proposed methodology outperforms state-of-the-art baselines with remarkable improvements on two real-world datasets on top of various backbone models. We further demonstrate that the learned embeddings successfully capture the desired causes, and show that DICE guarantees the robustness and interpretability of recommendation.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Information Retrieval},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\YZGTD9HV\\Zheng et al. - 2021 - Disentangling User Interest and Conformity for Rec.pdf}
}

@inproceedings{zhengDisentanglingUserInterest2021a,
  title = {Disentangling {{User Interest}} and {{Conformity}} for {{Recommendation}} with {{Causal Embedding}}},
  booktitle = {Proceedings of the {{Web Conference}} 2021},
  author = {Zheng, Yu and Gao, Chen and Li, Xiang and He, Xiangnan and Li, Yong and Jin, Depeng},
  date = {2021-04-19},
  series = {{{WWW}} '21},
  pages = {2980--2991},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3442381.3449788},
  url = {https://doi.org/10.1145/3442381.3449788},
  urldate = {2022-02-22},
  abstract = {Recommendation models are usually trained on observational interaction data. However, observational interaction data could result from users’ conformity towards popular items, which entangles users’ real interest. Existing methods tracks this problem as eliminating popularity bias, e.g., by re-weighting training samples or leveraging a small fraction of unbiased data. However, the variety of user conformity is ignored by these approaches, and different causes of an interaction are bundled together as unified representations, hence robustness and interpretability are not guaranteed when underlying causes are changing. In this paper, we present DICE, a general framework that learns representations where interest and conformity are structurally disentangled, and various backbone recommendation models could be smoothly integrated. We assign users and items with separate embeddings for interest and conformity, and make each embedding capture only one cause by training with cause-specific data which is obtained according to the colliding effect of causal inference. Our proposed methodology outperforms state-of-the-art baselines with remarkable improvements on two real-world datasets on top of various backbone models. We further demonstrate that the learned embeddings successfully capture the desired causes, and show that DICE guarantees the robustness and interpretability of recommendation.},
  isbn = {978-1-4503-8312-7},
  keywords = {causal embedding,popularity bias,Recommender systems},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\V3FVV4EC\\Zheng et al. - 2021 - Disentangling User Interest and Conformity for Rec.pdf}
}

@inproceedings{zhuFairnessAwareTensorBasedRecommendation2018,
  title = {Fairness-{{Aware Tensor-Based Recommendation}}},
  booktitle = {Proceedings of the 27th {{ACM International Conference}} on {{Information}} and {{Knowledge Management}}},
  author = {Zhu, Ziwei and Hu, Xia and Caverlee, James},
  date = {2018-10-17},
  pages = {1153--1162},
  publisher = {{ACM}},
  location = {{Torino Italy}},
  doi = {10.1145/3269206.3271795},
  url = {https://dl.acm.org/doi/10.1145/3269206.3271795},
  urldate = {2021-07-28},
  abstract = {Tensor-based methods have shown promise in improving upon traditional matrix factorization methods for recommender systems. But tensors may achieve improved recommendation quality while worsening the fairness of the recommendations. Hence, we propose a novel fairness-aware tensor recommendation framework that is designed to maintain quality while dramatically improving fairness. Four key aspects of the proposed framework are: (i) a new sensitive latent factor matrix for isolating sensitive features; (ii) a sensitive information regularizer that extracts sensitive information which can taint other latent factors; (iii) an effective algorithm to solve the proposed optimization model; and (iv) extension to multi-feature and multi-category cases which previous efforts have not addressed. Extensive experiments on real-world and synthetic datasets show that the framework enhances recommendation fairness while preserving recommendation quality in comparison with state-of-the-art alternatives.},
  eventtitle = {{{CIKM}} '18: {{The}} 27th {{ACM International Conference}} on {{Information}} and {{Knowledge Management}}},
  isbn = {978-1-4503-6014-2},
  langid = {english},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\XPTYY33S\\Zhu et al. - 2018 - Fairness-Aware Tensor-Based Recommendation.pdf}
}

@inproceedings{zhuMeasuringMitigatingItem2020,
  title = {Measuring and {{Mitigating Item Under-Recommendation Bias}} in {{Personalized Ranking Systems}}},
  booktitle = {Proceedings of the 43rd {{International ACM SIGIR Conference}} on {{Research}} and {{Development}} in {{Information Retrieval}}},
  author = {Zhu, Ziwei and Wang, Jianling and Caverlee, James},
  date = {2020-07-25},
  pages = {449--458},
  publisher = {{ACM}},
  location = {{Virtual Event China}},
  doi = {10.1145/3397271.3401177},
  url = {https://dl.acm.org/doi/10.1145/3397271.3401177},
  urldate = {2021-07-28},
  abstract = {Recommendation algorithms typically build models based on useritem interactions (e.g., clicks, likes, or ratings) to provide a personalized ranked list of items. These interactions are often distributed unevenly over different groups of items due to varying user preferences. However, we show that recommendation algorithms can inherit or even amplify this imbalanced distribution, leading to item under-recommendation bias. Concretely, we formalize the concepts of ranking-based statistical parity and equal opportunity as two measures of item under-recommendation bias. Then, we empirically show that one of the most widely adopted algorithms – Bayesian Personalized Ranking – produces biased recommendations, which motivates our effort to propose the novel debiased personalized ranking model. The debiased model is able to improve the two proposed bias metrics while preserving recommendation performance. Experiments on three public datasets show strong bias reduction of the proposed model versus state-of-the-art alternatives.},
  eventtitle = {{{SIGIR}} '20: {{The}} 43rd {{International ACM SIGIR}} Conference on Research and Development in {{Information Retrieval}}},
  isbn = {978-1-4503-8016-4},
  langid = {english},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\SC5NTXDW\\Zhu et al. - 2020 - Measuring and Mitigating Item Under-Recommendation.pdf}
}

@inproceedings{zhuPopularityOpportunityBiasCollaborative2021,
  title = {Popularity-{{Opportunity Bias}} in {{Collaborative Filtering}}},
  booktitle = {Proceedings of the 14th {{ACM International Conference}} on {{Web Search}} and {{Data Mining}}},
  author = {Zhu, Ziwei and He, Yun and Zhao, Xing and Zhang, Yin and Wang, Jianling and Caverlee, James},
  date = {2021-03-08},
  pages = {85--93},
  publisher = {{ACM}},
  location = {{Virtual Event Israel}},
  doi = {10.1145/3437963.3441820},
  url = {https://dl.acm.org/doi/10.1145/3437963.3441820},
  urldate = {2021-07-29},
  abstract = {This paper connects equal opportunity to popularity bias in implicit recommenders to introduce the problem of popularity-opportunity bias. That is, conditioned on user preferences that a user likes both items, the more popular item is more likely to be recommended (or ranked higher) to the user than the less popular one. This type of bias is harmful, exerting negative effects on the engagement of both users and item providers. Thus, we conduct a three-part study: (i) By a comprehensive empirical study, we identify the existence of the popularity-opportunity bias in fundamental matrix factorization models on four datasets; (ii) coupled with this empirical study, our theoretical study shows that matrix factorization models inherently produce the bias; and (iii) we demonstrate the potential of alleviating this bias by both in-processing and post-processing algorithms. Extensive experiments on four datasets show the effective debiasing performance of these proposed methods compared with baselines designed for conventional popularity bias.},
  eventtitle = {{{WSDM}} '21: {{The Fourteenth ACM International Conference}} on {{Web Search}} and {{Data Mining}}},
  isbn = {978-1-4503-8297-7},
  langid = {english},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\HBN2GALY\\Zhu et al. - 2021 - Popularity-Opportunity Bias in Collaborative Filte.pdf}
}

@online{ZoteroConnectors,
  title = {Zotero | {{Connectors}}},
  url = {https://www.zotero.org/download/connectors},
  urldate = {2021-07-29}
}

@article{zouRegularizationVariableSelection2005,
  title = {Regularization and Variable Selection via the Elastic Net},
  author = {Zou, Hui and Hastie, Trevor},
  date = {2005-04},
  journaltitle = {J Royal Statistical Soc B},
  volume = {67},
  number = {2},
  pages = {301--320},
  issn = {1369-7412, 1467-9868},
  doi = {10.1111/j.1467-9868.2005.00503.x},
  url = {https://onlinelibrary.wiley.com/doi/10.1111/j.1467-9868.2005.00503.x},
  urldate = {2022-01-03},
  abstract = {We propose the elastic net, a new regularization and variable selection method. Real world data and a simulation study show that the elastic net often outperforms the lasso, while enjoying a similar sparsity of representation. In addition, the elastic net encourages a grouping effect, where strongly correlated predictors tend to be in or out of the model together. The elastic net is particularly useful when the number of predictors (p) is much bigger than the number of observations (n). By contrast, the lasso is not a very satisfactory variable selection method in the p n case. An algorithm called LARS-EN is proposed for computing elastic net regularization paths efficiently, much like algorithm LARS does for the lasso.},
  langid = {english},
  file = {C\:\\Users\\Romanos\\Zotero\\storage\\W98CYQ3C\\Zou and Hastie - 2005 - Regularization and variable selection via the elas.pdf}
}

@preamble{ "\ifdefined\DeclarePrefChars\DeclarePrefChars{'’-}\else\fi " }

